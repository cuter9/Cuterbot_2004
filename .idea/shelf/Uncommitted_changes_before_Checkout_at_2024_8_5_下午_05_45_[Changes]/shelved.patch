Index: jetbot/apps/stats.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Copyright (c) 2017 Adafruit Industries\r\n# Author: Tony DiCola & James DeVito\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the \"Software\"), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included in\r\n# all copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\r\n# THE SOFTWARE.\r\nimport time\r\n\r\n# For Jetson Hardware\r\nfrom jetbot.utils.utils import get_ip_address\r\n\r\n# use ian3221 power monitor i2c device to get the jetson nano power status, the modules are ian3221.py and ian3221_jetbot.py in /jetbot/apps, \r\nfrom jetbot.apps.jetbot_states import jetbot_states, nano_states\r\n\r\nimport subprocess\r\n\r\n# For scanning I2C bus and SparkFun Hardware\r\nimport qwiic\r\n\r\n# For Adafruit Hardware\r\nimport Adafruit_SSD1306\r\nfrom PIL import Image\r\nfrom PIL import ImageDraw\r\nfrom PIL import ImageFont\r\n\r\n# js = jetbot_states()\t# use INA3221\r\njs = nano_states()  # use jtop app\r\n\r\n# Scan for devices on I2C bus\r\naddresses = qwiic.scan()\r\n\r\n# Initialize Display-----------------------------------------------------------\r\n# Try to connect to the OLED display module via I2C.\r\n\r\n# 128x32 display (default)---------------------------------------------\r\nif 60 in addresses:\r\n    disp1 = Adafruit_SSD1306.SSD1306_128_32(rst=None, i2c_bus=1,\r\n                                            gpio=1)  # setting gpio to 1 is hack to avoid platform detection\r\n    try:\r\n        # Initiallize Display\r\n        disp1.begin()\r\n\r\n        # Clear display.\r\n        disp1.clear()\r\n        disp1.display()\r\n\r\n        # Create blank image for drawing.\r\n        # Make sure to create image with mode '1' for 1-bit color.\r\n        width = disp1.width\r\n        height = disp1.height\r\n        image = Image.new('1', (width, height))\r\n\r\n        # Get drawing object to draw on image.\r\n        draw = ImageDraw.Draw(image)\r\n\r\n        # Draw a black filled box to clear the image.\r\n        draw.rectangle((0, 0, width, height), outline=0, fill=0)\r\n\r\n        # Draw some shapes.\r\n        # First define some constants to allow easy resizing of shapes.\r\n        padding = -2\r\n        top = padding\r\n        bottom = height - padding\r\n        # Move left to right keeping track of the current x position for drawing shapes.\r\n        x = 0\r\n\r\n        # Load default font.\r\n        # font = ImageFont.load_default()\r\n        font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", 9)\r\n\r\n        # Draw a black filled box to clear the image.\r\n        draw.rectangle((0, 0, width, height), outline=0, fill=0)\r\n    except OSError as err:\r\n        print(\"OS error: {0}\".format(err))\r\n        time.sleep(5)\r\n# 48 x 64 display------------------------------------------------------\r\nelif 61 in addresses:\r\n    disp2 = qwiic.QwiicMicroOled()\r\n    try:\r\n        # Initiallize Display\r\n        disp2.begin()\r\n\r\n        # Display Flame (set to buffer in begin function)\r\n        disp2.display()\r\n        time.sleep(5)  # Pause 5 sec\r\n\r\n        # Clear Display\r\n        disp2.clear(disp2.PAGE)\r\n        disp2.clear(disp2.ALL)\r\n\r\n        # Set Font\r\n        disp2.set_font_type(0)\r\n        # Could replace line spacing with disp2.getFontHeight, but doesn't scale properly\r\n\r\n        # Screen Width\r\n        import qwiic_micro_oled\r\n\r\n        LCDWIDTH = qwiic_micro_oled._LCDWIDTH\r\n    except OSError as err:\r\n        print(\"OS error: {0}\".format(err))\r\n        time.sleep(5)\r\n\r\nwhile True:\r\n    # Check Eth0, Wlan0, and Wlan1 Connections---------------------------------\r\n    a = 0  # Indexing of Connections\r\n\r\n    # Checks for Ethernet Connection\r\n    try:\r\n        eth = get_ip_address('eth0')\r\n        if eth != None:\r\n            a = a + 1\r\n    except Exception as e:\r\n        print(e)\r\n\r\n    # Checks for WiFi Connection on wlan0\r\n    try:\r\n        wlan0 = get_ip_address('wlan0')\r\n        if wlan0 != None:\r\n            a = a + 2\r\n    except Exception as e:\r\n        print(e)\r\n\r\n    # Checks for WiFi Connection on wlan1\r\n    try:\r\n        wlan1 = get_ip_address('wlan1')\r\n        if wlan1 != None:\r\n            a = a + 4\r\n    except Exception as e:\r\n        print(e)\r\n\r\n    # Check Resource Usage-----------------------------------------------------\r\n    # Shell scripts for system monitoring from here : https://unix.stackexchange.com/questions/119126/command-to-display-memory-usage-disk-$\r\n\r\n    # CPU Load\r\n    cmd = \"top -bn1 | grep load | awk '{printf \\\"%.1f%%\\\", $(NF-2)}'\"\r\n    CPU = subprocess.check_output(cmd, shell=True)\r\n\r\n    # Memory Use\r\n    cmd = \"free -m | awk 'NR==2{printf \\\"%.1f%%\\\", $3*100/$2}'\"\r\n    Mem_percent = subprocess.check_output(cmd, shell=True)\r\n    cmd = \"free -m | awk 'NR==2{printf \\\"%.2f/%.1f\\\", $3/1024,$2/1024}'\"\r\n    MemUsage = subprocess.check_output(cmd, shell=True)\r\n\r\n    # Disk Storage\r\n    cmd = \"df -h | awk '$NF==\\\"/\\\"{printf \\\"%s\\\", $5}'\"\r\n    Disk_percent = subprocess.check_output(cmd, shell=True)\r\n    cmd = \"df -h | awk '$NF==\\\"/\\\"{printf \\\"%d/%d\\\", $3,$2}'\"\r\n    DiskUsage = subprocess.check_output(cmd, shell=True)\r\n\r\n    # Power Status\r\n    # channel : 1 (default) : board level; 2: GPU level; 3: CPU level\r\n    # POM_5V_IN = js.pwr_states(channel=1)\t\t# use INA3221\r\n    POM_5V_IN = js.pwr_states  # jtop app\r\n    print(POM_5V_IN)\r\n    IN_VOLT = POM_5V_IN['in_volt']\r\n    IN_CURR = POM_5V_IN['in_current']\r\n    IN_PWR = POM_5V_IN['in_pwr']\r\n    # POM_5V_CPU = js.pwr_states(channel=3)\r\n    # POM_5V_GPU = js.pwr_states(channel=2)\r\n    # print(\"Load Voltage:  %3.2f V\" % IN_VOLT)\r\n    # print(\"Current:  %3.2f mA\" % IN_CURR)\r\n\r\n    try:\r\n        # 128x32 display (default)-------------------------------------------------\r\n        if 60 in addresses:\r\n            # IP address\r\n            if a == 1:\r\n                draw.text((x, top), \"eth0: \" + str(eth), font=font, fill=255)\r\n                draw.text((x, top + 8), \"Vin: %.2f V\" % IN_VOLT + \"  Pin: %.2f W\" % IN_PWR, font=font, fill=255)\r\n            elif a == 2:\r\n                draw.text((x, top), \"Vin: %.2f V\" % IN_VOLT + \"  Pin: %.2f W\" % IN_PWR, font=font, fill=255)\r\n                draw.text((x, top + 8), \"wlan0: \" + str(wlan0), font=font, fill=255)\r\n            elif a == 3:\r\n                draw.text((x, top), \"eth0: \" + str(eth), font=font, fill=255)\r\n                draw.text((x, top + 8), \"wlan0: \" + str(wlan0), font=font, fill=255)\r\n            elif a == 4:\r\n                draw.text((x, top), \"Vin: %.2f V\" % IN_VOLT + \"  Pin: %.2f W\" % IN_PWR, font=font, fill=255)\r\n                draw.text((x, top + 8), \"wlan1: \" + str(wlan1), font=font, fill=255)\r\n            elif a == 5:\r\n                draw.text((x, top), \"eth0: \" + str(eth), font=font, fill=255)\r\n                draw.text((x, top + 8), \"wlan1: \" + str(wlan1), font=font, fill=255)\r\n            else:\r\n                draw.text((x, top), \"No Connection!\", font=font, fill=255)\r\n\r\n            # Resource Usage\r\n            draw.text((x, top + 16), \"Mem: \" + str(MemUsage.decode('utf-8')) + \"GB\", font=font, fill=255)\r\n            draw.text((x, top + 25), \"Disk: \" + str(DiskUsage.decode('utf-8')) + \"GB\", font=font, fill=255)\r\n\r\n            # Display image.\r\n            disp1.image(image)\r\n            disp1.display()\r\n            time.sleep(2)\r\n\r\n            # Draw a black filled box to clear the image.\r\n            draw.rectangle((0, 0, width, height), outline=0, fill=0)\r\n        #\t\t\ttime.sleep(5) # Pause 5 sec\r\n\r\n        # 48 x 64 display----------------------------------------------------------\r\n        elif 61 in addresses:\r\n            # Text Spacing (places text on right edge of display)----------------------\r\n            b = 0\r\n            c = 0\r\n\r\n            if eth != None:\r\n                #Check String Length\r\n                if len(eth) > 10:\r\n                    # Find '.' to loop numerals\r\n                    while b != -1:\r\n                        x1 = LCDWIDTH - disp2._font.width * (len(eth) - b)\r\n                        i = b + 1\r\n                        b = eth.find('.', i)\r\n\r\n            if wlan0 != None:\r\n                #Check String Length\r\n                if len(wlan0) > 10:\r\n                    # Find '.' to loop numerals\r\n                    while c != -1:\r\n                        x2 = LCDWIDTH - disp2._font.width * (len(wlan0) - c)\r\n                        j = c + 1\r\n                        c = wlan0.find('.', j)\r\n\r\n            if wlan1 != None:\r\n                #Check String Length\r\n                if len(wlan1) > 10:\r\n                    # Find '.' to loop numerals\r\n                    while c != -1:\r\n                        x2 = LCDWIDTH - disp2._font.width * (len(wlan1) - c)\r\n                        j = c + 1\r\n                        c = wlan1.find('.', j)\r\n\r\n            x3 = LCDWIDTH - (disp2._font.width + 1) * (len(str(CPU.decode('utf-8'))))\r\n            x4 = LCDWIDTH - (disp2._font.width + 1) * (len(str(Mem_percent.decode('utf-8'))))\r\n            x5 = LCDWIDTH - (disp2._font.width + 1) * (len(str(Disk_percent.decode('utf-8'))))\r\n            x6 = LCDWIDTH - (disp2._font.width + 1) * (len(str(MemUsage.decode('utf-8')) + \"GB\"))\r\n            x7 = LCDWIDTH - (disp2._font.width + 1) * (len(str(DiskUsage.decode('utf-8')) + \"GB\"))\r\n\r\n            # Displays IP Address (if available)---------------------------------------\r\n\r\n            # Clear Display\r\n            disp2.clear(disp2.PAGE)\r\n            disp2.clear(disp2.ALL)\r\n\r\n            #Set Cursor at Origin\r\n            disp2.set_cursor(0, 0)\r\n\r\n            # Prints IP Address on OLED Display\r\n            if a == 1:\r\n                disp2.print(\"eth0:\")\r\n                disp2.set_cursor(0, 8)\r\n                if b != 0:\r\n                    disp2.print(str(eth[0:i]))\r\n                    disp2.set_cursor(x1, 16)\r\n                    disp2.print(str(eth[i::]))\r\n                else:\r\n                    disp2.print(str(eth))\r\n\r\n            elif a == 2:\r\n                disp2.print(\"wlan0: \")\r\n                disp2.set_cursor(0, 8)\r\n                if c != 0:\r\n                    disp2.print(str(wlan0[0:j]))\r\n                    disp2.set_cursor(x2, 16)\r\n                    disp2.print(str(wlan0[j::]))\r\n                else:\r\n                    disp2.print(str(wlan0))\r\n\r\n            elif a == 3:\r\n                disp2.print(\"eth0:\")\r\n                disp2.set_cursor(0, 8)\r\n                if b != 0:\r\n                    disp2.print(str(eth[0:i]))\r\n                    disp2.set_cursor(x1, 16)\r\n                    disp2.print(str(eth[i::]))\r\n                else:\r\n                    disp2.print(str(eth))\r\n\r\n                disp2.set_cursor(0, 24)\r\n                disp2.print(\"wlan0: \")\r\n                disp2.set_cursor(0, 32)\r\n                if c != 0:\r\n                    disp2.print(str(wlan0[0:j]))\r\n                    disp2.set_cursor(x2, 40)\r\n                    disp2.print(str(wlan0[j::]))\r\n                else:\r\n                    disp2.print(str(wlan0))\r\n\r\n            elif a == 4:\r\n                disp2.print(\"wlan1: \")\r\n                disp2.set_cursor(0, 8)\r\n                if c != 0:\r\n                    disp2.print(str(wlan1[0:j]))\r\n                    disp2.set_cursor(x2, 16)\r\n                    disp2.print(str(wlan1[j::]))\r\n                else:\r\n                    disp2.print(str(wlan1))\r\n\r\n            elif a == 5:\r\n                disp2.print(\"eth0:\")\r\n                disp2.set_cursor(0, 8)\r\n                if b != 0:\r\n                    disp2.print(str(eth[0:i]))\r\n                    disp2.set_cursor(x1, 16)\r\n                    disp2.print(str(eth[i::]))\r\n                else:\r\n                    disp2.print(str(eth))\r\n\r\n                disp2.set_cursor(0, 24)\r\n                disp2.print(\"wlan1: \")\r\n                disp2.set_cursor(0, 32)\r\n                if c != 0:\r\n                    disp2.print(str(wlan1[0:j]))\r\n                    disp2.set_cursor(x2, 40)\r\n                    disp2.print(str(wlan1[j::]))\r\n                else:\r\n                    disp2.print(str(wlan1))\r\n\r\n            else:\r\n                disp2.print(\"No Connection!\")\r\n\r\n            disp2.display()\r\n            time.sleep(10)  # Pause 10 sec\r\n\r\n            # Displays Resource Usage-------------------------------------------\r\n            # ------------------------------------------------------------------\r\n\r\n            # Percentage--------------------------------------------------------\r\n            # Clear Display\r\n            disp2.clear(disp2.PAGE)\r\n            disp2.clear(disp2.ALL)\r\n\r\n            #Set Cursor at Origin\r\n            disp2.set_cursor(0, 0)\r\n\r\n            # Prints Percentage Use on OLED Display\r\n            disp2.set_cursor(0, 0)  # Set Cursor at Origin\r\n            disp2.print(\"CPU:\")\r\n            disp2.set_cursor(0, 10)\r\n            disp2.print(\"Mem:\")\r\n            disp2.set_cursor(0, 20)\r\n            disp2.print(\"Disk:\")\r\n\r\n            disp2.set_cursor(x3, 0)\r\n            disp2.print(str(CPU.decode('utf-8')))\r\n            disp2.set_cursor(x4, 10)\r\n            disp2.print(str(Mem_percent.decode('utf-8')))\r\n            disp2.set_cursor(x5, 20)\r\n            disp2.print(str(Disk_percent.decode('utf-8')))\r\n\r\n            disp2.display()\r\n            time.sleep(7.5)  # Pause 7.5 sec\r\n\r\n            # Size--------------------------------------------------------------\r\n            # Clear Display\r\n            disp2.clear(disp2.PAGE)\r\n            disp2.clear(disp2.ALL)\r\n\r\n            #Set Cursor at Origin\r\n            disp2.set_cursor(0, 0)\r\n\r\n            # Prints Capacity Use on OLED Display\r\n            disp2.set_cursor(0, 0)  # Set Cursor at Origin\r\n            disp2.print(\"Mem:\")\r\n            disp2.set_cursor(x6, 10)\r\n            disp2.print(str(MemUsage.decode('utf-8')) + \"GB\")\r\n            disp2.set_cursor(0, 20)\r\n            disp2.print(\"Disk:\")\r\n            disp2.set_cursor(x7, 30)\r\n            disp2.print(str(DiskUsage.decode('utf-8')) + \"GB\")\r\n\r\n            disp2.display()\r\n            time.sleep(7.5)  # Pause 7.5 sec\r\n        else:\r\n            break\r\n\r\n\r\n    except OSError as err:\r\n        print(\"OS error: {0}\".format(err))\r\n        time.sleep(5)\r\n        break\r\n\r\n    except:\r\n        break\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/jetbot/apps/stats.py b/jetbot/apps/stats.py
--- a/jetbot/apps/stats.py	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/jetbot/apps/stats.py	(date 1722767250338)
@@ -51,7 +51,7 @@
     disp1 = Adafruit_SSD1306.SSD1306_128_32(rst=None, i2c_bus=1,
                                             gpio=1)  # setting gpio to 1 is hack to avoid platform detection
     try:
-        # Initiallize Display
+        # Initialize Display
         disp1.begin()
 
         # Clear display.
@@ -80,7 +80,7 @@
 
         # Load default font.
         # font = ImageFont.load_default()
-        font = ImageFont.truetype("DejaVuSans-Bold.ttf", 9)
+        font = ImageFont.truetype("DejaVuSansMono.ttf", 9)
 
         # Draw a black filled box to clear the image.
         draw.rectangle((0, 0, width, height), outline=0, fill=0)
@@ -91,7 +91,7 @@
 elif 61 in addresses:
     disp2 = qwiic.QwiicMicroOled()
     try:
-        # Initiallize Display
+        # Initialize Display
         disp2.begin()
 
         # Display Flame (set to buffer in begin function)
@@ -121,7 +121,7 @@
     # Checks for Ethernet Connection
     try:
         eth = get_ip_address('eth0')
-        if eth != None:
+        if eth is not None:
             a = a + 1
     except Exception as e:
         print(e)
@@ -129,7 +129,7 @@
     # Checks for WiFi Connection on wlan0
     try:
         wlan0 = get_ip_address('wlan0')
-        if wlan0 != None:
+        if wlan0 is not None:
             a = a + 2
     except Exception as e:
         print(e)
@@ -137,7 +137,7 @@
     # Checks for WiFi Connection on wlan1
     try:
         wlan1 = get_ip_address('wlan1')
-        if wlan1 != None:
+        if wlan1 is not None:
             a = a + 4
     except Exception as e:
         print(e)
@@ -180,15 +180,15 @@
             # IP address
             if a == 1:
                 draw.text((x, top), "eth0: " + str(eth), font=font, fill=255)
-                draw.text((x, top + 8), "Vin: %.2f V" % IN_VOLT + "  Pin: %.2f W" % IN_PWR, font=font, fill=255)
+                draw.text((x, top + 8), "Vin: %.2fV" % IN_VOLT + " Pin:%.2fW" % IN_PWR, font=font, fill=255)
             elif a == 2:
-                draw.text((x, top), "Vin: %.2f V" % IN_VOLT + "  Pin: %.2f W" % IN_PWR, font=font, fill=255)
+                draw.text((x, top), "Vin: %.2fV" % IN_VOLT + " Pin: %.2fW" % IN_PWR, font=font, fill=255)
                 draw.text((x, top + 8), "wlan0: " + str(wlan0), font=font, fill=255)
             elif a == 3:
                 draw.text((x, top), "eth0: " + str(eth), font=font, fill=255)
                 draw.text((x, top + 8), "wlan0: " + str(wlan0), font=font, fill=255)
             elif a == 4:
-                draw.text((x, top), "Vin: %.2f V" % IN_VOLT + "  Pin: %.2f W" % IN_PWR, font=font, fill=255)
+                draw.text((x, top), "Vin: %.2fV" % IN_VOLT + " Pin: %.2fW" % IN_PWR, font=font, fill=255)
                 draw.text((x, top + 8), "wlan1: " + str(wlan1), font=font, fill=255)
             elif a == 5:
                 draw.text((x, top), "eth0: " + str(eth), font=font, fill=255)
Index: jetbot/utils/model_selection.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pandas as pd\r\nimport os\r\nfrom traitlets import HasTraits, Unicode, List, Bool\r\nimport numpy as np\r\n\r\nHEAD_LIST = ['model_function', 'model_type', 'model_path']\r\nMODEL_REPO_DIR = os.path.join(os.environ[\"HOME\"], \"model_repo\")\r\nMODEL_REPO_DIR_DOCKER = os.path.join(\"/workspace\", \"model_repo\")\r\nos.environ['MODEL_REPO_DIR_DOCKER'] = MODEL_REPO_DIR_DOCKER\r\nos.environ['MODEL_REPO_DIR'] = MODEL_REPO_DIR\r\n\r\n\r\nclass model_selection(HasTraits):\r\n    model_function = Unicode(default_value='object detection').tag(config=True)\r\n    model_function_list = List(default_value=[]).tag(config=True)\r\n    model_type = Unicode(default_value='SSD').tag(config=True)\r\n    model_type_list = List(default_value=[]).tag(config=True)\r\n    model_path = Unicode(default_value='').tag(config=True)\r\n    model_path_list = List(default_value=[]).tag(config=True)\r\n    selected_model_path = Unicode(default_value='').tag(config=True)\r\n    is_selected = Bool(default_value=False).tag(config=True)\r\n\r\n    def __init__(self, core_library='TensorRT'):\r\n        super().__init__()\r\n\r\n        self.core_library = core_library\r\n        if self.core_library == 'TensorRT':\r\n            self.df = pd.read_csv(os.path.join(MODEL_REPO_DIR_DOCKER, \"trt_model_tbl.csv\"),\r\n                                  header=None, names=HEAD_LIST)\r\n        elif self.core_library == 'Pytorch':\r\n            self.df = pd.read_csv(os.path.join(MODEL_REPO_DIR_DOCKER, \"torch_model_tbl.csv\"),\r\n                                  header=None, names=HEAD_LIST)\r\n\r\n        for p in self.df.values:\r\n            p[2] = os.path.join(MODEL_REPO_DIR_DOCKER, p[2].split(\"/\", 1)[1])\r\n\r\n        self.model_function_list = list(self.df[\"model_function\"].astype(\"category\").cat.categories)\r\n        self.update_model_type_list()\r\n        # d_mf = self.df[self.df.model_function == self.model_function]   # data frame of given function\r\n        # self.model_type_list = list(d_mf[\"model_type\"].astype(\"category\").cat.categories)\r\n        self.update_model_list()\r\n        # mpl = d_mf[d_mf.model_type == self.model_type].loc[:, ['model_path']].values.tolist()\r\n        # self.model_path_list = np.squeeze(mpl).tolist()\r\n        self.observe(self.update_model, names=['model_function', 'model_type', 'model_path'])\r\n        # self.is_selected = False\r\n        # self.observe(self.selected, names=['is_selected'])\r\n\r\n    def update_model_type_list(self):\r\n        mf = self.df[self.df.model_function == self.model_function]  # select the models based on given model function\r\n        # mt = mf[mf.model_type == self.model_type]\r\n        self.model_type_list = list(\r\n            mf[\"model_type\"].astype(\"category\").cat.categories)  # the model types of the given model function\r\n        return self.model_type_list\r\n\r\n    def update_model_list(self):\r\n        mf = self.df[self.df.model_function == self.model_function]  # select the models based on given model function\r\n        mt = mf[mf.model_type == self.model_type]  # select the models from the given model type\r\n        mpl = mt.loc[:, ['model_path']].values\r\n        self.model_path_list = np.squeeze(mpl).tolist()\r\n        return self.model_path_list\r\n\r\n    def update_model(self, change):\r\n        # print(change)\r\n        if change['name'] == 'model_function':\r\n            self.model_function = change['new']\r\n            self.update_model_type_list()\r\n        if change['name'] == 'model_type':\r\n            self.model_type = change['new']\r\n            self.update_model_list()\r\n        if change['name'] == 'model_path':\r\n            self.model_path = change['new']\r\n            # self.selected_model_path = os.path.join(MODEL_REPO_DIR_DOCKER, self.model_path.split(\"/\", 1)[1])\r\n        # print(self.selected_model_path)\r\n\r\n    # def selected(self, change):\r\n    #     self.is_selected = change['new']\r\n\r\n\r\n'''\r\nms = trt_model_selection()\r\n# ms.model_function = 'object detection'\r\n# ms.model_type = 'SSD_FPN'\r\n# model_type_list = ms.update_model_type_list()\r\nmodel_path_list = ms.update_model_list()\r\nprint(ms.model_function_list, ms.model_type_list)\r\nprint(model_path_list)\r\n'''\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/jetbot/utils/model_selection.py b/jetbot/utils/model_selection.py
--- a/jetbot/utils/model_selection.py	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/jetbot/utils/model_selection.py	(date 1722782485895)
@@ -3,6 +3,9 @@
 from traitlets import HasTraits, Unicode, List, Bool
 import numpy as np
 
+import torch
+import torchvision.models as pth_models
+
 HEAD_LIST = ['model_function', 'model_type', 'model_path']
 MODEL_REPO_DIR = os.path.join(os.environ["HOME"], "model_repo")
 MODEL_REPO_DIR_DOCKER = os.path.join("/workspace", "model_repo")
@@ -10,6 +13,64 @@
 os.environ['MODEL_REPO_DIR'] = MODEL_REPO_DIR
 
 
+def load_tune_pth_model(pth_model_name="resnet18", pretrained=True):
+    if pretrained:
+        model = getattr(pth_models, pth_model_name)()  # for fine tuning
+    else:
+        model = getattr(pth_models, pth_model_name)(pretrained=False)  # for inferencig
+    # ----- modify last layer for classification, and the model used in notebook should be modified too.
+
+    if pth_model_name == 'mobilenet_v3_large':  # MobileNet
+        model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features,
+                                              2)  # for mobilenet_v3 model. must add the block expansion factor 4
+        model_type = "MobileNet"
+
+    elif pth_model_name == 'mobilenet_v2':
+        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features,
+                                              2)  # for mobilenet_v2 model. must add the block expansion factor 4
+        model_type = "MobileNet"
+
+    elif pth_model_name == 'vgg11':  # VGGNet
+        model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features,
+                                              2)  # for VGG model. must add the block expansion factor 4
+        model_type = "VggNet"
+
+    elif 'resnet' in pth_model_name:  # ResNet
+        model.fc = torch.nn.Linear(model.fc.in_features,
+                                   2)  # for resnet model must add the block expansion factor 4
+        # model.fc = torch.nn.Linear(512, 2)
+        model_type = "ResNet"
+
+    elif 'efficientnet' in pth_model_name:  # ResNet
+        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # for efficientnet model
+        # model.classifier[0].dropout = torch.nn.Dropout(p=dropout)
+        model_type = "EfficientNet"
+
+    elif pth_model_name == 'inception_v3':  # Inception_v3
+        model.fc = torch.nn.Linear(model.fc.in_features, 2)
+        # model.dropout = torch.nn.Dropout(p=dropout)
+        if model.aux_logits:
+            model.AuxLogits.fc = torch.nn.Linear(model.AuxLogits.fc.in_features, 2)
+        model_type = "InceptionNet"
+
+    elif pth_model_name == 'googlenet':  # Inception_v3
+        model.fc = torch.nn.Linear(model.fc.in_features, 2)
+        # model.dropout = torch.nn.Dropout(p=dropout)
+        if model.aux_logits:
+            model.aux1.fc2 = torch.nn.Linear(model.aux1.fc2.in_features, 2)
+            model.aux2.fc2 = torch.nn.Linear(model.aux2.fc2.in_features, 2)
+        #   model.aux1.dropout = torch.nn.Dropout(p=dropout)
+        #   model.aux2.dropout = torch.nn.Dropout(p=dropout)
+        model_type = "GoogleNet"
+
+    elif "densenet" in pth_model_name:  # densenet121, densenet161, densenet169, densenet201
+        model.classifier = torch.nn.Linear(model.classifier.in_features, 2)
+        model_type = "DenseNet"
+    else:
+        model_type = None
+    return model, model_type
+
+
 class model_selection(HasTraits):
     model_function = Unicode(default_value='object detection').tag(config=True)
     model_function_list = List(default_value=[]).tag(config=True)
@@ -20,19 +81,19 @@
     selected_model_path = Unicode(default_value='').tag(config=True)
     is_selected = Bool(default_value=False).tag(config=True)
 
-    def __init__(self, core_library='TensorRT'):
+    def __init__(self, core_library='TensorRT', dir_model_repo=MODEL_REPO_DIR_DOCKER):
         super().__init__()
 
         self.core_library = core_library
         if self.core_library == 'TensorRT':
-            self.df = pd.read_csv(os.path.join(MODEL_REPO_DIR_DOCKER, "trt_model_tbl.csv"),
+            self.df = pd.read_csv(os.path.join(dir_model_repo, "trt_model_tbl.csv"),
                                   header=None, names=HEAD_LIST)
         elif self.core_library == 'Pytorch':
-            self.df = pd.read_csv(os.path.join(MODEL_REPO_DIR_DOCKER, "torch_model_tbl.csv"),
+            self.df = pd.read_csv(os.path.join(dir_model_repo, "torch_model_tbl.csv"),
                                   header=None, names=HEAD_LIST)
 
         for p in self.df.values:
-            p[2] = os.path.join(MODEL_REPO_DIR_DOCKER, p[2].split("/", 1)[1])
+            p[2] = os.path.join(dir_model_repo, p[2].split("/", 1)[1])
 
         self.model_function_list = list(self.df["model_function"].astype("category").cat.categories)
         self.update_model_type_list()
@@ -56,7 +117,8 @@
         mf = self.df[self.df.model_function == self.model_function]  # select the models based on given model function
         mt = mf[mf.model_type == self.model_type]  # select the models from the given model type
         mpl = mt.loc[:, ['model_path']].values
-        self.model_path_list = np.squeeze(mpl).tolist()
+        # self.model_path_list = np.squeeze(mpl).tolist()
+        self.model_path_list = mpl[:, 0].tolist()
         return self.model_path_list
 
     def update_model(self, change):
Index: docker/display/build.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>sudo pip3 install jetson-stats -U\r\nsudo docker build \\\r\n    --build-arg BASE_IMAGE=$CUTERBOT_DOCKER_REMOTE/cuterbot:base-$CUTERBOT_IMAGES_TAG \\\r\n    -t $CUTERBOT_DOCKER_REMOTE/cuterbot:display-$CUTERBOT_IMAGES_TAG \\\r\n    -f Dockerfile .\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/docker/display/build.sh b/docker/display/build.sh
--- a/docker/display/build.sh	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/docker/display/build.sh	(date 1722849994098)
@@ -2,5 +2,4 @@
 sudo docker build \
     --build-arg BASE_IMAGE=$CUTERBOT_DOCKER_REMOTE/cuterbot:base-$CUTERBOT_IMAGES_TAG \
     -t $CUTERBOT_DOCKER_REMOTE/cuterbot:display-$CUTERBOT_IMAGES_TAG \
-    -f Dockerfile .
-
+    -f Dockerfile .
\ No newline at end of file
Index: docker/jupyter/enable.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>WORKSPACE=$1\r\nJETBOT_CAMERA=${2:-opencv_gst_camera}\r\n\r\n# set default swap limit as unlimited\r\nif [[ -z \"$JETBOT_JUPYTER_MEMORY_SWAP\" ]]\r\nthen\r\n\texport JETBOT_JUPYTER_MEMORY_SWAP=-1\r\nfi\r\n\r\nif [[ -z \"$JETBOT_JUPYTER_MEMORY\" ]]\r\nthen\r\n\r\n\tsudo docker run -it -d \\\r\n\t    --restart always \\\r\n\t    --runtime nvidia \\\r\n\t    --network host \\\r\n\t    --privileged \\\r\n\t    --device /dev/video* \\\r\n\t    --volume /dev/bus/usb:/dev/bus/usb \\\r\n\t    --volume /tmp/argus_socket:/tmp/argus_socket \\\r\n\t    -p 8888:8888 \\\r\n\t    -v $WORKSPACE:/workspace \\\r\n      -v /tmp/.X11-unix:/tmp/.X11-unix \\\r\n      -v $HOME/.Xauthority:/root/.Xauthority \\\r\n\t    --workdir /workspace \\\r\n\t    --name=jetbot_jupyter \\\r\n\t    --memory-swap=$JETBOT_JUPYTER_MEMORY_SWAP \\\r\n\t    --env JETBOT_DEFAULT_CAMERA=$JETBOT_CAMERA \\\r\n      --env DISPLAY=$DISPLAY \\\r\n      --env LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libGLdispatch.so.0:/usr/lib/aarch64-linux-gnu/libgomp.so.1\\\r\n\t    $CUTERBOT_DOCKER_REMOTE/cuterbot:jupyter-$CUTERBOT_IMAGES_TAG\r\n\r\nelse\r\n\r\n\tsudo docker run -it -d \\\r\n\t    --restart always \\\r\n\t    --runtime nvidia \\\r\n\t    --network host \\\r\n\t    --privileged \\\r\n\t    --device /dev/video* \\\r\n\t    --volume /dev/bus/usb:/dev/bus/usb \\\r\n\t    --volume /tmp/argus_socket:/tmp/argus_socket \\\r\n\t    -p 8888:8888 \\\r\n\t    -v $WORKSPACE:/workspace \\\r\n      -v /tmp/.X11-unix:/tmp/.X11-unix \\\r\n      -v $HOME/.Xauthority:/root/.Xauthority \\\r\n\t    --workdir /workspace \\\r\n\t    --name=jetbot_jupyter \\\r\n\t    --memory=$JETBOT_JUPYTER_MEMORY \\\r\n\t    --memory-swap=$JETBOT_JUPYTER_MEMORY_SWAP \\\r\n\t    --env JETBOT_DEFAULT_CAMERA=$JETBOT_CAMERA \\\r\n      --env DISPLAY=$DISPLAY \\\r\n\t    $CUTERBOT_DOCKER_REMOTE/cuterbot:jupyter-$CUTERBOT_IMAGES_TAG\r\n\r\nfi\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/docker/jupyter/enable.sh b/docker/jupyter/enable.sh
--- a/docker/jupyter/enable.sh	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/docker/jupyter/enable.sh	(date 1722849617531)
@@ -22,6 +22,7 @@
 	    -v $WORKSPACE:/workspace \
       -v /tmp/.X11-unix:/tmp/.X11-unix \
       -v $HOME/.Xauthority:/root/.Xauthority \
+      -v /run/jtop.sock:/run/jtop.sock \
 	    --workdir /workspace \
 	    --name=jetbot_jupyter \
 	    --memory-swap=$JETBOT_JUPYTER_MEMORY_SWAP \
Index: notebooks/road_following/live_demo_build_trt.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# Road Following - Build TensorRT model for live demo\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"In this notebook, we will optimize the model we trained using TensorRT.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"## Load the trained model\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"We will assume that you have already downloaded ``best_steering_model_xy.pth`` to work station as instructed in \\\"train_model.ipynb\\\" notebook. Now, you should upload model file to JetBot in to this notebook's directory. Once that's finished there should be a file named ``best_steering_model_xy.pth`` in this notebook's directory.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"> Please make sure the file has uploaded fully before calling the next cell\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Execute the code below to initialize the PyTorch model. This should look very familiar from the training notebook.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 1,\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"name\": \"stderr\",\r\n     \"output_type\": \"stream\",\r\n     \"text\": [\r\n      \"/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\\n\",\r\n      \"  warnings.warn(\\n\",\r\n      \"/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\\n\",\r\n      \"  warnings.warn(msg)\\n\"\r\n     ]\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"import torchvision\\n\",\r\n    \"import torch\\n\",\r\n    \"\\n\",\r\n    \"#--- ResNet\\n\",\r\n    \"model = torchvision.models.resnet18(pretrained=False)\\n\",\r\n    \"# model = torchvision.models.resnet34(pretrained=False)\\n\",\r\n    \"model.fc = torch.nn.Linear(model.fc.in_features, 2)\\n\",\r\n    \"\\n\",\r\n    \"#--- MobileNet\\n\",\r\n    \"# model = torchvision.models.mobilenet_v3_large(pretrained=False)\\n\",\r\n    \"# model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, 2)\\n\",\r\n    \"\\n\",\r\n    \"#--- Inception Net\\n\",\r\n    \"# model = torchvision.models.inception_v3(pretrained=False)\\n\",\r\n    \"# model.fc = torch.nn.Linear(model.fc.in_features, 2)\\n\",\r\n    \"# if model.aux_logits:\\n\",\r\n    \"#     model.AuxLogits.fc = torch.nn.Linear(model.AuxLogits.fc.in_features, 2)\\n\",\r\n    \"\\n\",\r\n    \"model = model.cuda().eval().half()\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Next, load the trained weights from the ``best_steering_model_xy.pth`` file that you uploaded.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 2,\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"<All keys matched successfully>\"\r\n      ]\r\n     },\r\n     \"execution_count\": 2,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"model.load_state_dict(torch.load('best_steering_model_xy_resnet18.pth'))\\n\",\r\n    \"# model.load_state_dict(torch.load('best_steering_model_xy_resnet34.pth'))\\n\",\r\n    \"# model.load_state_dict(torch.load('best_steering_model_xy_mobilenet_v3_large.pth'))\\n\",\r\n    \"# model.load_state_dict(torch.load('best_steering_model_xy_inception_v3.pth'))\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 3,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"device = torch.device('cuda')\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"## TensorRT\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"> If your setup does not have `torch2trt` installed, you need to first install `torch2trt` by executing the following in the console.\\n\",\r\n    \"```bash\\n\",\r\n    \"cd $HOME\\n\",\r\n    \"git clone https://github.com/NVIDIA-AI-IOT/torch2trt\\n\",\r\n    \"cd torch2trt\\n\",\r\n    \"sudo python3 setup.py install\\n\",\r\n    \"```\\n\",\r\n    \"\\n\",\r\n    \"Convert and optimize the model using torch2trt for faster inference with TensorRT. Please see the [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) readme for more details.\\n\",\r\n    \"\\n\",\r\n    \"> This optimization process can take a couple minutes to complete.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 4,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from torch2trt import torch2trt\\n\",\r\n    \"\\n\",\r\n    \"data = torch.zeros((1, 3, 224, 224)).cuda().half()  # resnet\\n\",\r\n    \"# data = torch.zeros((1, 3, 299, 299)).cuda().half()   # inception_v3\\n\",\r\n    \"\\n\",\r\n    \"model_trt = torch2trt(model, [data], fp16_mode=True)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Save the optimized model using the cell below\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 5,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_resnet18.pth')\\n\",\r\n    \"# torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_resnet34.pth')\\n\",\r\n    \"# torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_mobilenet_v3_large.pth')\\n\",\r\n    \"# torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_inception_v3.pth')\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"## Next\\n\",\r\n    \"Open live_demo_trt.ipynb to move JetBot with the TensorRT optimized model.\"\r\n   ]\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3 (ipykernel)\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.8.10\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 4\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/road_following/live_demo_build_trt.ipynb b/notebooks/road_following/live_demo_build_trt.ipynb
--- a/notebooks/road_following/live_demo_build_trt.ipynb	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/notebooks/road_following/live_demo_build_trt.ipynb	(date 1722836106000)
@@ -44,124 +44,113 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
-      "  warnings.warn(\n",
-      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
-      "  warnings.warn(msg)\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "import torchvision\n",
     "import torch\n",
-    "\n",
-    "#--- ResNet\n",
-    "model = torchvision.models.resnet18(pretrained=False)\n",
-    "# model = torchvision.models.resnet34(pretrained=False)\n",
-    "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
+    "from jetbot.utils import model_selection\n",
+    "import os \n",
+    "import ipywidgets.widgets as widgets\n",
+    "from ipywidgets.widgets import Box, HBox, VBox, Layout, Label\n",
+    "import traitlets\n",
     "\n",
-    "#--- MobileNet\n",
-    "# model = torchvision.models.mobilenet_v3_large(pretrained=False)\n",
-    "# model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, 2)\n",
+    "# The path of trt models is 'MODEL_REPO_DIR_DOCKER' which is set in /jetbot/utils/model_selection.py,\n",
+    "# which may be modified if you change the file path of trt models, 'MODEL_REPO_DIR_DOCKER' or dir_model_repo.\n",
     "\n",
-    "#--- Inception Net\n",
-    "# model = torchvision.models.inception_v3(pretrained=False)\n",
-    "# model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
-    "# if model.aux_logits:\n",
-    "#     model.AuxLogits.fc = torch.nn.Linear(model.AuxLogits.fc.in_features, 2)\n",
+    "dir_model_repo = os.environ['MODEL_REPO_DIR_DOCKER']\n",
+    "print(dir_model_repo)\n",
     "\n",
-    "model = model.cuda().eval().half()"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "Next, load the trained weights from the ``best_steering_model_xy.pth`` file that you uploaded."
+    "# pth_ms = model_selection(core_library = \"Pytorch\")  # if 'MODEL_REPO_DIR_DOCKER' is used.\n",
+    "pth_ms = model_selection(core_library = \"Pytorch\", dir_model_repo=dir_model_repo)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "<All keys matched successfully>"
-      ]
-     },
-     "execution_count": 2,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "outputs": [],
    "source": [
-    "model.load_state_dict(torch.load('best_steering_model_xy_resnet18.pth'))\n",
-    "# model.load_state_dict(torch.load('best_steering_model_xy_resnet34.pth'))\n",
-    "# model.load_state_dict(torch.load('best_steering_model_xy_mobilenet_v3_large.pth'))\n",
-    "# model.load_state_dict(torch.load('best_steering_model_xy_inception_v3.pth'))"
+    "pth_ms.model_function = \"classifier\"\n",
+    "\n",
+    "model_type_widget = widgets.Select(options=pth_ms.model_type_list, value=pth_ms.model_type_list[0],\n",
+    "                                      description='Model Type:')\n",
+    "traitlets.dlink((pth_ms, 'model_type_list'), (model_type_widget, 'options'))\n",
+    "traitlets.dlink((model_type_widget, 'value'), (pth_ms, 'model_type'))\n",
+    "\n",
+    "model_path_widget = widgets.Select(options=pth_ms.model_path_list, description='Model Path:',\n",
+    "                                      layout=Layout(width='65%'))\n",
+    "traitlets.dlink((pth_ms, 'model_path_list'), (model_path_widget, 'options'))\n",
+    "traitlets.dlink((model_path_widget, 'value'), (pth_ms, 'model_path'))\n"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device."
+    "Next, load the pytoch model and the trained weights from the model_path (e.g.``best_steering_model_xy_<<pth_model_name>>.pth``) file that you uploaded."
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
-    "device = torch.device('cuda')"
+    "def load_trained_model():\n",
+    "    from jetbot.utils.model_selection import load_tune_pth_model\n",
+    "    model_path = model_path_widget.value\n",
+    "    pth_model_name = model_path_widget.value.split('/')[-1].split('.')[0].split('_', 4)[-1]\n",
+    "    print(\"start load trained model -- \\n model name : \", pth_model_name, '\\n model path: ', model_path)\n",
+    "\n",
+    "    model, model_type = load_tune_pth_model(pth_model_name=pth_model_name, pretrained=False)\n",
+    "    model = model.cuda().eval().half()\n",
+    "    \n",
+    "    model.load_state_dict(torch.load(model_path))\n",
+    "    \n",
+    "    # Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device. \n",
+    "    device = torch.device('cuda')\n",
+    "    \n",
+    "    return pth_model_name, model_type, model"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "## TensorRT"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
+    "## TensorRT conversion:\n",
+    "\n",
+    "> Note: if you are running with docker container, you may not need to do the following installation. \n",
     "> If your setup does not have `torch2trt` installed, you need to first install `torch2trt` by executing the following in the console.\n",
     "```bash\n",
-    "cd $HOME\n",
-    "git clone https://github.com/NVIDIA-AI-IOT/torch2trt\n",
-    "cd torch2trt\n",
-    "sudo python3 setup.py install\n",
+    "    cd $HOME\n",
+    "    git clone https://github.com/NVIDIA-AI-IOT/torch2trt\n",
+    "    cd torch2trt\n",
+    "    sudo python3 setup.py install\n",
     "```\n",
-    "\n",
-    "Convert and optimize the model using torch2trt for faster inference with TensorRT. Please see the [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) readme for more details.\n",
-    "\n",
-    "> This optimization process can take a couple minutes to complete."
+    "> Convert and optimize the model using torch2trt for faster inference with TensorRT. Please see the [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) readme for more details.\n",
+    "> This optimization process can take a couple minutes to complete.\n"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
-    "from torch2trt import torch2trt\n",
+    "def trt_conversion(pth_model_name, model):\n",
+    "    from torch2trt import torch2trt\n",
+    "    print(\"start building TRT model -- \")\n",
     "\n",
-    "data = torch.zeros((1, 3, 224, 224)).cuda().half()  # resnet\n",
-    "# data = torch.zeros((1, 3, 299, 299)).cuda().half()   # inception_v3\n",
-    "\n",
-    "model_trt = torch2trt(model, [data], fp16_mode=True)"
+    "    if pth_model_name == 'inception_v3':\n",
+    "        data = torch.zeros((1, 3, 299, 299)).cuda().half()   # inception_v3\n",
+    "    else:\n",
+    "        data = torch.zeros((1, 3, 224, 224)).cuda().half()  # resnet\n",
+    "        \n",
+    "    model_trt = torch2trt(model, [data], fp16_mode=True)\n",
+    "    \n",
+    "    return model_trt"
    ]
   },
   {
@@ -173,14 +162,53 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
-    "torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_resnet18.pth')\n",
-    "# torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_resnet34.pth')\n",
-    "# torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_mobilenet_v3_large.pth')\n",
-    "# torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_inception_v3.pth')"
+    "# Save the optimized model using the cell below\n",
+    "def save_trt_model(pth_model_name, model_type, model_trt):\n",
+    "    import pandas as pd\n",
+    "    print(\"saving built trt model  --\")\n",
+    "    \n",
+    "    path_trt_model = os.path.join(dir_model_repo, 'road_following', \"best_steering_model_xy_trt_\"+pth_model_name+'.pth')\n",
+    "    torch.save(model_trt.state_dict(), path_trt_model)\n",
+    "\n",
+    "    df_file = os.path.join(dir_model_repo, 'trt_model_tbl.csv')\n",
+    "    if os.path.isfile(df_file):\n",
+    "        df = pd.read_csv(df_file, header=None)\n",
+    "    else:\n",
+    "        df = pd.DataFrame()\n",
+    "\n",
+    "    trt_model_path_tbl = os.path.join('.', 'road_following', \"best_steering_model_xy_trt_\"+pth_model_name+'.pth')\n",
+    "    df = df.append([[\"classifier\", model_type, trt_model_path_tbl]], ignore_index = False)\n",
+    "    df = df.drop_duplicates()\n",
+    "    df.to_csv(df_file, header=False, index=False)\n",
+    "    return trt_model_path_tbl"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def start_trt_conversion(change):\n",
+    "    button_OK.disabled=True\n",
+    "    \n",
+    "    pth_model_name, model_type, model = load_trained_model()\n",
+    "    model_trt = trt_conversion(pth_model_name, model)\n",
+    "    trt_model_path_tbl = save_trt_model(pth_model_name, model_type, model_trt)\n",
+    "    \n",
+    "    print(\"Building finished, and TRT model is saved in -- \", trt_model_path_tbl)\n",
+    "    button_OK.disabled=False\n",
+    "    \n",
+    "display(HBox([model_type_widget, model_path_widget]))\n",
+    "\n",
+    "button_OK = widgets.Button(description='OK', tooltip='Click to start', icon=\"start\")\n",
+    "button_OK.style.button_color='lightblue'\n",
+    "button_OK.on_click(start_trt_conversion)\n",
+    "display(button_OK)"
    ]
   },
   {
@@ -194,7 +222,7 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3 (ipykernel)",
+   "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
@@ -208,7 +236,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.10"
+   "version": "3.6.9"
   }
  },
  "nbformat": 4,
Index: notebooks/road_following/live_demo_light.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# Road Following - Live demo\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"In this notebook, we will use model we trained to move jetBot smoothly on track. \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Load Trained Model\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"We will assume that you have already downloaded ``best_steering_model_xy.pth`` to work station as instructed in \\\"train_model.ipynb\\\" notebook. Now, you should upload model file to JetBot in to this notebook's directory. Once that's finished there should be a file named ``best_steering_model_xy_model.pth`` in this notebook's directory.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"> Please make sure the file has uploaded fully before calling the next cell\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Execute the code below to initialize the PyTorch model. This should look very familiar from the training notebook.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# %matplotlib notebook\\n\",\r\n    \"from jetbot import RoadCruiser\\n\",\r\n    \"from jetbot.utils import model_selection\\n\",\r\n    \"import ipywidgets.widgets as widgets\\n\",\r\n    \"from ipywidgets.widgets import Box, HBox, VBox, Layout, Label\\n\",\r\n    \"import traitlets\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"RC = RoadCruiser(init_sensor_rc=True)\\n\",\r\n    \"\\n\",\r\n    \"pth_ms = model_selection(core_library = \\\"Pytorch\\\")\\n\",\r\n    \"pth_ms.model_function = \\\"classifier\\\"\\n\",\r\n    \"\\n\",\r\n    \"model_type_widget = widgets.Select(options=pth_ms.model_type_list, value=pth_ms.model_type_list[0],\\n\",\r\n    \"                                      description='Model Type:')\\n\",\r\n    \"traitlets.dlink((pth_ms, 'model_type_list'), (model_type_widget, 'options'))\\n\",\r\n    \"traitlets.dlink((model_type_widget, 'value'), (pth_ms, 'model_type'))\\n\",\r\n    \"traitlets.dlink((pth_ms, 'model_type'), (RC, 'type_cruiser_model'))\\n\",\r\n    \"\\n\",\r\n    \"model_path_widget = widgets.Select(options=pth_ms.model_path_list, description='Model Path:',\\n\",\r\n    \"                                      layout=Layout(width='60%'))\\n\",\r\n    \"traitlets.dlink((pth_ms, 'model_path_list'), (model_path_widget, 'options'))\\n\",\r\n    \"traitlets.dlink((model_path_widget, 'value'), (pth_ms, 'model_path'))\\n\",\r\n    \"traitlets.dlink((pth_ms, 'model_path'), (RC, 'cruiser_model'))\\n\",\r\n    \"\\n\",\r\n    \"use_gpu_widget = widgets.RadioButtons(options=['gpu', 'cpu'], value='gpu', description='choose processor:', layout=Layout(width='10%'), disabled=False)\\n\",\r\n    \"traitlets.dlink((use_gpu_widget, 'value'), (RC, 'use_gpu'))\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Next, load the trained weights from the ``best_steering_model_xy_model.pth`` file that you uploaded.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"### Creating the Pre-Processing Function\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"We have now loaded our model, but there's a slight issue. The format that we trained our model doesn't exactly match the format of the camera. To do that, we need to do some preprocessing. This involves the following steps:\\n\",\r\n    \"\\n\",\r\n    \"1. Convert from HWC layout to CHW layout\\n\",\r\n    \"2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\\n\",\r\n    \"3. Transfer the data from CPU memory to GPU memory\\n\",\r\n    \"4. Add a batch dimension\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Awesome! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\\n\",\r\n    \"\\n\",\r\n    \"Now, let's start and display our camera. You should be pretty familiar with this by now. \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from IPython.display import display\\n\",\r\n    \"import ipywidgets\\n\",\r\n    \"from jetbot import bgr8_to_jpeg\\n\",\r\n    \"\\n\",\r\n    \"image_widget = ipywidgets.Image(width=300, height=300)\\n\",\r\n    \"# fps_widget = ipywidgets.FloatText(description='Capture rate')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((RC.capturer, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\\n\",\r\n    \"# traitlets.dlink((RC.camera, 'cap_time'), (fps_widget, 'value'))\\n\",\r\n    \"                \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Now, we will define sliders to control JetBot\\n\",\r\n    \"> Note: We have initialize the slider values for best known configurations, however these might not work for your dataset, therefore please increase or decrease the sliders according to your setup and environment\\n\",\r\n    \"\\n\",\r\n    \"1. Speed Control (speed_gain_slider): To start your JetBot increase ``speed_gain_slider`` \\n\",\r\n    \"2. Steering Gain Control (steering_gain_slider): If you see JetBot is wobbling, you need to reduce ``steering_gain_slider`` till it is smooth\\n\",\r\n    \"3. Steering Bias control (steering_bias_slider): If you see JetBot is biased towards extreme right or extreme left side of the track, you should control this slider till JetBot start following line or track in the center.  This accounts for motor biases as well as camera offsets\\n\",\r\n    \"\\n\",\r\n    \"> Note: You should play around above mentioned sliders with lower speed to get smooth JetBot road following behavior.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"speed_gain_slider = ipywidgets.FloatSlider(min=0, max=1, step=0.001, value=0.2, description='speed gain', readout_format='.3f')\\n\",\r\n    \"steering_gain_slider = ipywidgets.FloatSlider(min=0, max=0.5, step=0.001, value=0.08, description='steering gain', readout_format='.3f')\\n\",\r\n    \"steering_dgain_slider = ipywidgets.FloatSlider(min=0, max=2.0, step=0.001, value=0.82, description='steering kd', readout_format='.3f')\\n\",\r\n    \"steering_bias_slider = ipywidgets.FloatSlider(min=-0.1, max=0.1, step=0.001, value=-0.01, description='steering bias', readout_format='.3f')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((speed_gain_slider, 'value'), (RC, 'speed_gain'))\\n\",\r\n    \"traitlets.dlink((steering_gain_slider, 'value'), (RC, 'steering_gain'))\\n\",\r\n    \"traitlets.dlink((steering_dgain_slider, 'value'), (RC, 'steering_dgain'))\\n\",\r\n    \"traitlets.dlink((steering_bias_slider, 'value'), (RC, 'steering_bias'))\\n\",\r\n    \"\\n\",\r\n    \"# VBox_image = ipywidgets.VBox([image_widget, fps_widget], layout=ipywidgets.Layout(align_self='center'))\\n\",\r\n    \"VBox_image = ipywidgets.VBox([image_widget], layout=ipywidgets.Layout(align_self='center'))\\n\",\r\n    \"VBox_control = ipywidgets.VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider], layout=ipywidgets.Layout(align_self='center'))\\n\",\r\n    \"\\n\",\r\n    \"# display(ipywidgets.HBox([VBox_image, VBox_control]))\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Next, let's display some sliders that will let us see what JetBot is thinking.  The x and y sliders will display the predicted x, y values.\\n\",\r\n    \"\\n\",\r\n    \"The steering slider will display our estimated steering value.  Please remember, this value isn't the actual angle of the target, but simply a value that is\\n\",\r\n    \"nearly proportional.  When the actual angle is ``0``, this will be zero, and it will increase / decrease with the actual angle.  \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\\n\",\r\n    \"y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\\n\",\r\n    \"steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\\n\",\r\n    \"speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((RC, 'x_slider'), (x_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((RC, 'y_slider'), (y_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((RC, 'steering'), (steering_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((RC, 'speed'), (speed_slider, 'value'))\\n\",\r\n    \"\\n\",\r\n    \"Box_y_state = ipywidgets.HBox([y_slider, speed_slider])\\n\",\r\n    \"Box_x_state = ipywidgets.VBox([x_slider, steering_slider])\\n\",\r\n    \"Box_state = ipywidgets.VBox([Box_y_state, Box_x_state])\\n\",\r\n    \"\\n\",\r\n    \"# display(ipywidgets.HBox([y_slider, speed_slider]))\\n\",\r\n    \"# display(x_slider, steering_slider)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Next, we'll create a function that will get called whenever the camera's value changes. This function will do the following steps\\n\",\r\n    \"\\n\",\r\n    \"1. Pre-process the camera image\\n\",\r\n    \"2. Execute the neural network\\n\",\r\n    \"3. Compute the approximate steering value\\n\",\r\n    \"4. Control the motors using proportional / derivative control (PD)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing.\\n\",\r\n    \"\\n\",\r\n    \"We accomplish that with the observe function.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \">WARNING: This code will move the robot!! Please make sure your robot has clearance and it is on Lego or Track you have collected data on. The road follower should work, but the neural network is only as good as the data it's trained on!\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# display(VBox_image)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame. \\n\",\r\n    \"\\n\",\r\n    \"You can now place JetBot on  Lego or Track you have collected data on and see whether it can follow track.\\n\",\r\n    \"\\n\",\r\n    \"If you want to stop this behavior, you can unattach this callback by executing the code below.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {\r\n    \"scrolled\": true,\r\n    \"tags\": []\r\n   },\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"display(HBox([use_gpu_widget, model_type_widget, model_path_widget]))\\n\",\r\n    \"\\n\",\r\n    \"display(ipywidgets.HBox([Box_state, VBox_image, VBox_control]))\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"button_start = ipywidgets.Button(description='Start', tooltip='Click to start running', icon='solid play')\\n\",\r\n    \"button_start.style.button_color='lightBlue'\\n\",\r\n    \"button_start.on_click(RC.start_rc)\\n\",\r\n    \"\\n\",\r\n    \"button_stop = ipywidgets.Button(description='Stop', tooltip='Click to stop running', icon=\\\"solid stop\\\")\\n\",\r\n    \"button_stop.style.button_color='Red'\\n\",\r\n    \"button_stop.on_click(RC.stop_rc)\\n\",\r\n    \"display(HBox([button_start, button_stop]))\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Again, let's close the camera conneciton properly so that we can use the camera in other notebooks.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"### Conclusion\\n\",\r\n    \"That's it for this live demo! Hopefully you had some fun seeing your JetBot moving smoothly on track following the road!!!\\n\",\r\n    \"\\n\",\r\n    \"If your JetBot wasn't following road very well, try to spot where it fails. The beauty is that we can collect more data for these failure scenarios and the JetBot should get even better :)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": []\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3 (ipykernel)\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.8.10\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 4\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/road_following/live_demo_light.ipynb b/notebooks/road_following/live_demo_light.ipynb
--- a/notebooks/road_following/live_demo_light.ipynb	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/notebooks/road_following/live_demo_light.ipynb	(date 1722846610028)
@@ -79,7 +79,7 @@
     "traitlets.dlink((model_path_widget, 'value'), (pth_ms, 'model_path'))\n",
     "traitlets.dlink((pth_ms, 'model_path'), (RC, 'cruiser_model'))\n",
     "\n",
-    "use_gpu_widget = widgets.RadioButtons(options=['gpu', 'cpu'], value='gpu', description='choose processor:', layout=Layout(width='10%'), disabled=False)\n",
+    "use_gpu_widget = widgets.RadioButtons(options=['gpu', 'cpu'], value='gpu', description='processor:', layout=Layout(width='15%'))\n",
     "traitlets.dlink((use_gpu_widget, 'value'), (RC, 'use_gpu'))\n"
    ]
   },
@@ -170,10 +170,10 @@
     "steering_dgain_slider = ipywidgets.FloatSlider(min=0, max=2.0, step=0.001, value=0.82, description='steering kd', readout_format='.3f')\n",
     "steering_bias_slider = ipywidgets.FloatSlider(min=-0.1, max=0.1, step=0.001, value=-0.01, description='steering bias', readout_format='.3f')\n",
     "\n",
-    "traitlets.dlink((speed_gain_slider, 'value'), (RC, 'speed_gain'))\n",
-    "traitlets.dlink((steering_gain_slider, 'value'), (RC, 'steering_gain'))\n",
-    "traitlets.dlink((steering_dgain_slider, 'value'), (RC, 'steering_dgain'))\n",
-    "traitlets.dlink((steering_bias_slider, 'value'), (RC, 'steering_bias'))\n",
+    "traitlets.dlink((speed_gain_slider, 'value'), (RC, 'speed_gain_rc'))\n",
+    "traitlets.dlink((steering_gain_slider, 'value'), (RC, 'steering_gain_rc'))\n",
+    "traitlets.dlink((steering_dgain_slider, 'value'), (RC, 'steering_dgain_rc'))\n",
+    "traitlets.dlink((steering_bias_slider, 'value'), (RC, 'steering_bias_rc'))\n",
     "\n",
     "# VBox_image = ipywidgets.VBox([image_widget, fps_widget], layout=ipywidgets.Layout(align_self='center'))\n",
     "VBox_image = ipywidgets.VBox([image_widget], layout=ipywidgets.Layout(align_self='center'))\n",
@@ -205,8 +205,8 @@
     "\n",
     "traitlets.dlink((RC, 'x_slider'), (x_slider, 'value'))\n",
     "traitlets.dlink((RC, 'y_slider'), (y_slider, 'value'))\n",
-    "traitlets.dlink((RC, 'steering'), (steering_slider, 'value'))\n",
-    "traitlets.dlink((RC, 'speed'), (speed_slider, 'value'))\n",
+    "traitlets.dlink((RC, 'steering_rc'), (steering_slider, 'value'))\n",
+    "traitlets.dlink((RC, 'speed_rc'), (speed_slider, 'value'))\n",
     "\n",
     "Box_y_state = ipywidgets.HBox([y_slider, speed_slider])\n",
     "Box_x_state = ipywidgets.VBox([x_slider, steering_slider])\n",
@@ -245,15 +245,6 @@
    ]
   },
   {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# display(VBox_image)"
-   ]
-  },
-  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
@@ -268,29 +259,19 @@
    "cell_type": "code",
    "execution_count": null,
    "metadata": {
-    "scrolled": true,
     "tags": []
    },
    "outputs": [],
    "source": [
     "display(HBox([use_gpu_widget, model_type_widget, model_path_widget]))\n",
     "\n",
-    "display(ipywidgets.HBox([Box_state, VBox_image, VBox_control]))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "tags": []
-   },
-   "outputs": [],
-   "source": [
-    "button_start = ipywidgets.Button(description='Start', tooltip='Click to start running', icon='solid play')\n",
+    "display(ipywidgets.HBox([Box_state, VBox_image, VBox_control]))\n",
+    "\n",
+    "button_start = ipywidgets.Button(description='Start', tooltip='Click to start running', icon='play')\n",
     "button_start.style.button_color='lightBlue'\n",
     "button_start.on_click(RC.start_rc)\n",
     "\n",
-    "button_stop = ipywidgets.Button(description='Stop', tooltip='Click to stop running', icon=\"solid stop\")\n",
+    "button_stop = ipywidgets.Button(description='Stop', tooltip='Click to stop running', icon=\"stop\")\n",
     "button_stop.style.button_color='Red'\n",
     "button_stop.on_click(RC.stop_rc)\n",
     "display(HBox([button_start, button_stop]))"
@@ -325,7 +306,7 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3 (ipykernel)",
+   "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
@@ -339,7 +320,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.10"
+   "version": "3.6.9"
   }
  },
  "nbformat": 4,
Index: notebooks/road_following/data_collection_gamepad.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# Road Following - Data Collection (using Gamepad)\\n\",\r\n    \"\\n\",\r\n    \"If you've run through the collision avoidance sample, your should be familiar following three steps\\n\",\r\n    \"\\n\",\r\n    \"1.  Data collection\\n\",\r\n    \"2.  Training\\n\",\r\n    \"3.  Deployment\\n\",\r\n    \"\\n\",\r\n    \"In this notebook, we'll do the same exact thing!  Except, instead of classification, you'll learn a different fundamental technique, **regression**, that we'll use to\\n\",\r\n    \"enable JetBot to follow a road (or really, any path or target point).  \\n\",\r\n    \"\\n\",\r\n    \"1. Place the JetBot in different positions on a path (offset from center, different angles, etc)\\n\",\r\n    \"\\n\",\r\n    \">  Remember from collision avoidance, data variation is key!\\n\",\r\n    \"\\n\",\r\n    \"2. Display the live camera feed from the robot\\n\",\r\n    \"3. Using a gamepad controller, place a 'green dot', which corresponds to the target direction we want the robot to travel, on the image.\\n\",\r\n    \"4. Store the X, Y values of this green dot along with the image from the robot's camera\\n\",\r\n    \"\\n\",\r\n    \"Then, in the training notebook, we'll train a neural network to predict the X, Y values of our label.  In the live demo, we'll use\\n\",\r\n    \"the predicted X, Y values to compute an approximate steering value (it's not 'exactly' an angle, as\\n\",\r\n    \"that would require image calibration, but it's roughly proportional to the angle so our controller will work fine).\\n\",\r\n    \"\\n\",\r\n    \"So how do you decide exactly where to place the target for this example?  Here is a guide we think may help\\n\",\r\n    \"\\n\",\r\n    \"1.  Look at the live video feed from the camera\\n\",\r\n    \"2.  Imagine the path that the robot should follow (try to approximate the distance it needs to avoid running off road etc.)\\n\",\r\n    \"3.  Place the target as far along this path as it can go so that the robot could head straight to the target without 'running off' the road.\\n\",\r\n    \"\\n\",\r\n    \"> For example, if we're on a very straight road, we could place it at the horizon.  If we're on a sharp turn, it may need to be placed closer to the robot so it doesn't run out of boundaries.\\n\",\r\n    \"\\n\",\r\n    \"Assuming our deep learning model works as intended, these labeling guidelines should ensure the following:\\n\",\r\n    \"\\n\",\r\n    \"1.  The robot can safely travel directly towards the target (without going out of bounds etc.)\\n\",\r\n    \"2.  The target will continuously progress along our imagined path\\n\",\r\n    \"\\n\",\r\n    \"What we get, is a 'carrot on a stick' that moves along our desired trajectory.  Deep learning decides where to place the carrot, and JetBot just follows it :)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Labeling example video\\n\",\r\n    \"\\n\",\r\n    \"Execute the block of code to see an example of how to we labeled the images.  This model worked after only 123 images :)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 1,\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/html\": [\r\n       \"\\n\",\r\n       \"        <iframe\\n\",\r\n       \"            width=\\\"920\\\"\\n\",\r\n       \"            height=\\\"560\\\"\\n\",\r\n       \"            src=\\\"https://www.youtube.com/embed/FW4En6LejhI\\\"\\n\",\r\n       \"            frameborder=\\\"0\\\"\\n\",\r\n       \"            allowfullscreen\\n\",\r\n       \"        ></iframe>\\n\",\r\n       \"        \"\r\n      ],\r\n      \"text/plain\": [\r\n       \"<IPython.lib.display.IFrame at 0x7f9d0d0128>\"\r\n      ]\r\n     },\r\n     \"execution_count\": 1,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"# from IPython.display import HTML\\n\",\r\n    \"from IPython.display import IFrame\\n\",\r\n    \"# HTML('<iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/FW4En6LejhI\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>')\\n\",\r\n    \"IFrame(src=\\\"https://www.youtube.com/embed/FW4En6LejhI\\\", width=\\\"920\\\", height=\\\"560\\\")\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Import Libraries\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"So lets get started by importing all the required libraries for \\\"data collection\\\" purpose. We will mainly use OpenCV to visualize and save image with labels. Libraries such as uuid, datetime are used for image naming. \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 2,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# IPython Libraries for display and widgets\\n\",\r\n    \"import traitlets\\n\",\r\n    \"import ipywidgets.widgets as widgets\\n\",\r\n    \"from IPython.display import display\\n\",\r\n    \"\\n\",\r\n    \"# Camera and Motor Interface for JetBot\\n\",\r\n    \"from jetbot import Robot, Camera, bgr8_to_jpeg\\n\",\r\n    \"\\n\",\r\n    \"# Basic Python packages for image annotation\\n\",\r\n    \"from uuid import uuid1\\n\",\r\n    \"import os\\n\",\r\n    \"import json\\n\",\r\n    \"import glob\\n\",\r\n    \"import datetime\\n\",\r\n    \"import numpy as np\\n\",\r\n    \"import cv2\\n\",\r\n    \"import time\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Display Live Camera Feed\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"First, let's initialize and display our camera like we did in the teleoperation notebook. \\n\",\r\n    \"\\n\",\r\n    \"We use Camera Class from JetBot to enable CSI MIPI camera. Our neural network takes a 224x224 pixel image as input. We'll set our camera to that size to minimize the filesize of our dataset (we've tested that it works for this task). In some scenarios it may be better to collect data in a larger image size and downscale to the desired size later.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 3,\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"application/vnd.jupyter.widget-view+json\": {\r\n       \"model_id\": \"578e1e8fb14a459099957315076f29ce\",\r\n       \"version_major\": 2,\r\n       \"version_minor\": 0\r\n      },\r\n      \"text/plain\": [\r\n       \"HBox(children=(Image(value=b'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x00\\\\x00\\\\x01\\\\x00\\\\x01\\\\x00\\\\x00\\\\xff\\\\xdb\\\\x00C…\"\r\n      ]\r\n     },\r\n     \"metadata\": {},\r\n     \"output_type\": \"display_data\"\r\n    },\r\n    {\r\n     \"data\": {\r\n      \"application/vnd.jupyter.widget-view+json\": {\r\n       \"model_id\": \"390a9781d1934d1895c52ed9769c2fb0\",\r\n       \"version_major\": 2,\r\n       \"version_minor\": 0\r\n      },\r\n      \"text/plain\": [\r\n       \"FloatSlider(value=0.0, description='x', max=1.0, min=-1.0, step=0.001)\"\r\n      ]\r\n     },\r\n     \"metadata\": {},\r\n     \"output_type\": \"display_data\"\r\n    },\r\n    {\r\n     \"data\": {\r\n      \"application/vnd.jupyter.widget-view+json\": {\r\n       \"model_id\": \"d4936209776a4e1f92d0c0d30759a2c5\",\r\n       \"version_major\": 2,\r\n       \"version_minor\": 0\r\n      },\r\n      \"text/plain\": [\r\n       \"FloatSlider(value=0.0, description='y', max=1.0, min=-1.0, step=0.001)\"\r\n      ]\r\n     },\r\n     \"metadata\": {},\r\n     \"output_type\": \"display_data\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"camera = Camera()\\n\",\r\n    \"\\n\",\r\n    \"widget_width = camera.width\\n\",\r\n    \"widget_height = camera.height\\n\",\r\n    \"\\n\",\r\n    \"image_widget = widgets.Image(format='jpeg', width=widget_width, height=widget_height)\\n\",\r\n    \"target_widget = widgets.Image(format='jpeg', width=widget_width, height=widget_height)\\n\",\r\n    \"\\n\",\r\n    \"x_slider = widgets.FloatSlider(min=-1.0, max=1.0, step=0.001, description='x')\\n\",\r\n    \"y_slider = widgets.FloatSlider(min=-1.0, max=1.0, step=0.001, description='y')\\n\",\r\n    \"\\n\",\r\n    \"def display_xy(camera_image):\\n\",\r\n    \"    image = np.copy(camera_image)\\n\",\r\n    \"    x = x_slider.value\\n\",\r\n    \"    y = y_slider.value\\n\",\r\n    \"    x = int(x * widget_width / 2 + widget_width / 2)\\n\",\r\n    \"    y = int(y * widget_height / 2 + widget_height / 2)\\n\",\r\n    \"    image = cv2.circle(image, (x, y), 8, (0, 255, 0), 3)\\n\",\r\n    \"    image = cv2.circle(image, (int(widget_width / 2), int(widget_height)), 8, (0, 0,255), 3)\\n\",\r\n    \"    image = cv2.line(image, (x,y), (int(widget_width / 2), int(widget_height)), (255,0,0), 3)\\n\",\r\n    \"    image = cv2.line(image, (0, int(widget_height/2)), (int(widget_width), int(widget_height/2)), (0,255,255), 3)\\n\",\r\n    \"    jpeg_image = bgr8_to_jpeg(image)\\n\",\r\n    \"    return jpeg_image\\n\",\r\n    \"\\n\",\r\n    \"time.sleep(1)\\n\",\r\n    \"traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\\n\",\r\n    \"traitlets.dlink((camera, 'value'), (target_widget, 'value'), transform=display_xy)\\n\",\r\n    \"\\n\",\r\n    \"display(widgets.HBox([image_widget, target_widget]), x_slider, y_slider)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"### Create Gamepad Controller\\n\",\r\n    \"\\n\",\r\n    \"This step is similar to \\\"Teleoperation\\\" task. In this task, we will use gamepad controller to label images.\\n\",\r\n    \"\\n\",\r\n    \"The first thing we want to do is create an instance of the Controller widget, which we'll use to label images with \\\"x\\\" and \\\"y\\\" values as mentioned in introduction. The Controller widget takes a index parameter, which specifies the number of the controller. This is useful in case you have multiple controllers attached, or some gamepads appear as multiple controllers. To determine the index of the controller you're using,\\n\",\r\n    \"\\n\",\r\n    \"Visit http://html5gamepad.com.\\n\",\r\n    \"Press buttons on the gamepad you're using\\n\",\r\n    \"Remember the index of the gamepad that is responding to the button presses\\n\",\r\n    \"Next, we'll create and display our controller using that index.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 4,\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"application/vnd.jupyter.widget-view+json\": {\r\n       \"model_id\": \"7836aff2b0b84883b40cce01560df687\",\r\n       \"version_major\": 2,\r\n       \"version_minor\": 0\r\n      },\r\n      \"text/plain\": [\r\n       \"Controller()\"\r\n      ]\r\n     },\r\n     \"metadata\": {},\r\n     \"output_type\": \"display_data\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"controller = widgets.Controller(index=0)\\n\",\r\n    \"\\n\",\r\n    \"display(controller)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"### Connect Gamepad Controller to Label Images\\n\",\r\n    \"\\n\",\r\n    \"Now, even though we've connected our gamepad, we haven't yet attached the controller to label images! We'll connect that to the left and right vertical axes using the dlink function. The dlink function, unlike the link function, allows us to attach a transform between the source and target. \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 6,\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"DirectionalLink(source=(Axis(value=0.003921627998352051), 'value'), target=(FloatSlider(value=0.0, description…\"\r\n      ]\r\n     },\r\n     \"metadata\": {},\r\n     \"output_type\": \"display_data\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"widgets.jsdlink((controller.axes[0], 'value'), (x_slider, 'value'))\\n\",\r\n    \"widgets.jsdlink((controller.axes[1], 'value'), (y_slider, 'value'))\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"### Collect data\\n\",\r\n    \"\\n\",\r\n    \"The following block of code will display the live image feed, as well as the number of images we've saved.  We store\\n\",\r\n    \"the target X, Y values by\\n\",\r\n    \"\\n\",\r\n    \"1. Place the green dot on the target\\n\",\r\n    \"2. Press 'down' on the DPAD to save\\n\",\r\n    \"\\n\",\r\n    \"This will store a file in the ``dataset_xy`` folder with files named\\n\",\r\n    \"\\n\",\r\n    \"``xy_<x value>_<y value>_<uuid>.jpg``\\n\",\r\n    \"\\n\",\r\n    \"where `<x value>` and `<y value>` are the coordinates **in pixel (not in percentage)** (count from the top left corner).\\n\",\r\n    \"\\n\",\r\n    \"When we train, we load the images and parse the x, y values from the filename\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 7,\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"application/vnd.jupyter.widget-view+json\": {\r\n       \"model_id\": \"f4cdb14346a44aa787554617601d76be\",\r\n       \"version_major\": 2,\r\n       \"version_minor\": 0\r\n      },\r\n      \"text/plain\": [\r\n       \"VBox(children=(Image(value=b'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x00\\\\x00\\\\x01\\\\x00\\\\x01\\\\x00\\\\x00\\\\xff\\\\xdb\\\\x00C…\"\r\n      ]\r\n     },\r\n     \"metadata\": {},\r\n     \"output_type\": \"display_data\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"DATASET_DIR = 'dataset_xy'\\n\",\r\n    \"\\n\",\r\n    \"# we have this \\\"try/except\\\" statement because these next functions can throw an error if the directories exist already\\n\",\r\n    \"try:\\n\",\r\n    \"    os.makedirs(DATASET_DIR)\\n\",\r\n    \"except FileExistsError:\\n\",\r\n    \"    print('Directories not created because they already exist')\\n\",\r\n    \"\\n\",\r\n    \"for b in controller.buttons:\\n\",\r\n    \"    b.unobserve_all()\\n\",\r\n    \"\\n\",\r\n    \"count_widget = widgets.IntText(description='count', value=len(glob.glob(os.path.join(DATASET_DIR, '*.jpg'))))\\n\",\r\n    \"\\n\",\r\n    \"def xy_uuid(x, y):\\n\",\r\n    \"    return 'xy_%03d_%03d_%s' % (x * widget_width / 2 + widget_width / 2, y * widget_height / 2 + widget_height / 2, uuid1())\\n\",\r\n    \"\\n\",\r\n    \"def save_snapshot(change):\\n\",\r\n    \"    if change['new']:\\n\",\r\n    \"        uuid = xy_uuid(x_slider.value, y_slider.value)\\n\",\r\n    \"        image_path = os.path.join(DATASET_DIR, uuid + '.jpg')\\n\",\r\n    \"        with open(image_path, 'wb') as f:\\n\",\r\n    \"            f.write(image_widget.value)\\n\",\r\n    \"        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\\n\",\r\n    \"\\n\",\r\n    \"controller.buttons[11].observe(save_snapshot, names='value')\\n\",\r\n    \"\\n\",\r\n    \"display(widgets.VBox([\\n\",\r\n    \"    target_widget,\\n\",\r\n    \"    count_widget\\n\",\r\n    \"]))\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Again, let's close the camera conneciton properly so that we can use the camera in other notebooks.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 8,\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"name\": \"stdout\",\r\n     \"output_type\": \"stream\",\r\n     \"text\": [\r\n      \"Capture thread is stopped\\n\",\r\n      \"Camera operation is released ! \\n\",\r\n      \"service nvargus-daemon is restarted ! \\n\"\r\n     ]\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"camera.stop()\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Next\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Once you've collected enough data, we'll need to copy that data to our GPU desktop or cloud machine for training. First, we can call the following terminal command to compress our dataset folder into a single zip file.  \\n\",\r\n    \"\\n\",\r\n    \"> If you're training on the JetBot itself, you can skip this step!\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"The ! prefix indicates that we want to run the cell as a shell (or terminal) command.\\n\",\r\n    \"\\n\",\r\n    \"The -r flag in the zip command below indicates recursive so that we include all nested files, the -q flag indicates quiet so that the zip command doesn't print any output\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 9,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"def timestr():\\n\",\r\n    \"    return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\\n\",\r\n    \"\\n\",\r\n    \"!zip -r -q road_following_{DATASET_DIR}_{timestr()}.zip {DATASET_DIR}\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"You should see a file named road_following_<Date&Time>.zip in the Jupyter Lab file browser. You should download the zip file using the Jupyter Lab file browser by right clicking and selecting Download.\"\r\n   ]\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.6.9\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 4\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/road_following/data_collection_gamepad.ipynb b/notebooks/road_following/data_collection_gamepad.ipynb
--- a/notebooks/road_following/data_collection_gamepad.ipynb	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/notebooks/road_following/data_collection_gamepad.ipynb	(date 1722836200136)
@@ -54,37 +54,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/html": [
-       "\n",
-       "        <iframe\n",
-       "            width=\"920\"\n",
-       "            height=\"560\"\n",
-       "            src=\"https://www.youtube.com/embed/FW4En6LejhI\"\n",
-       "            frameborder=\"0\"\n",
-       "            allowfullscreen\n",
-       "        ></iframe>\n",
-       "        "
-      ],
-      "text/plain": [
-       "<IPython.lib.display.IFrame at 0x7f9d0d0128>"
-      ]
-     },
-     "execution_count": 1,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
    "source": [
     "# from IPython.display import HTML\n",
     "from IPython.display import IFrame\n",
     "# HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FW4En6LejhI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n",
     "IFrame(src=\"https://www.youtube.com/embed/FW4En6LejhI\", width=\"920\", height=\"560\")"
-   ]
+   ],
+   "outputs": [],
+   "execution_count": null
   },
   {
    "cell_type": "markdown",
@@ -102,9 +80,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
    "metadata": {},
-   "outputs": [],
    "source": [
     "# IPython Libraries for display and widgets\n",
     "import traitlets\n",
@@ -123,7 +99,9 @@
     "import numpy as np\n",
     "import cv2\n",
     "import time"
-   ]
+   ],
+   "outputs": [],
+   "execution_count": null
   },
   {
    "cell_type": "markdown",
@@ -143,52 +121,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "578e1e8fb14a459099957315076f29ce",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "390a9781d1934d1895c52ed9769c2fb0",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "FloatSlider(value=0.0, description='x', max=1.0, min=-1.0, step=0.001)"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "d4936209776a4e1f92d0c0d30759a2c5",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "FloatSlider(value=0.0, description='y', max=1.0, min=-1.0, step=0.001)"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
    "source": [
     "camera = Camera()\n",
     "\n",
@@ -219,7 +152,9 @@
     "traitlets.dlink((camera, 'value'), (target_widget, 'value'), transform=display_xy)\n",
     "\n",
     "display(widgets.HBox([image_widget, target_widget]), x_slider, y_slider)"
-   ]
+   ],
+   "outputs": [],
+   "execution_count": null
   },
   {
    "cell_type": "markdown",
@@ -241,29 +176,14 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "7836aff2b0b84883b40cce01560df687",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "Controller()"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
    "source": [
     "controller = widgets.Controller(index=0)\n",
     "\n",
     "display(controller)"
-   ]
+   ],
+   "outputs": [],
+   "execution_count": null
   },
   {
    "cell_type": "markdown",
@@ -278,23 +198,13 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "DirectionalLink(source=(Axis(value=0.003921627998352051), 'value'), target=(FloatSlider(value=0.0, description…"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
    "source": [
     "widgets.jsdlink((controller.axes[0], 'value'), (x_slider, 'value'))\n",
     "widgets.jsdlink((controller.axes[1], 'value'), (y_slider, 'value'))"
-   ]
+   ],
+   "outputs": [],
+   "execution_count": null
   },
   {
    "cell_type": "markdown",
@@ -321,24 +231,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "f4cdb14346a44aa787554617601d76be",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "VBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
    "source": [
     "DATASET_DIR = 'dataset_xy'\n",
     "\n",
@@ -370,7 +263,9 @@
     "    target_widget,\n",
     "    count_widget\n",
     "]))"
-   ]
+   ],
+   "outputs": [],
+   "execution_count": null
   },
   {
    "cell_type": "markdown",
@@ -381,22 +276,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Capture thread is stopped\n",
-      "Camera operation is released ! \n",
-      "service nvargus-daemon is restarted ! \n"
-     ]
-    }
-   ],
    "source": [
     "camera.stop()"
-   ]
+   ],
+   "outputs": [],
+   "execution_count": null
   },
   {
    "cell_type": "markdown",
@@ -425,15 +310,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
    "metadata": {},
-   "outputs": [],
    "source": [
     "def timestr():\n",
     "    return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
     "\n",
     "!zip -r -q road_following_{DATASET_DIR}_{timestr()}.zip {DATASET_DIR}"
-   ]
+   ],
+   "outputs": [],
+   "execution_count": null
   },
   {
    "cell_type": "markdown",
Index: notebooks/road_following/live_demo_build_trt_v1.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/road_following/live_demo_build_trt_v1.ipynb b/notebooks/road_following/live_demo_build_trt_v1.ipynb
new file mode 100644
--- /dev/null	(date 1722835095000)
+++ b/notebooks/road_following/live_demo_build_trt_v1.ipynb	(date 1722835095000)
@@ -0,0 +1,240 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Road Following - Build TensorRT model for live demo"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "In this notebook, we will optimize the model we trained using TensorRT."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Load the trained model"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "We will assume that you have already downloaded ``best_steering_model_xy.pth`` to work station as instructed in \"train_model.ipynb\" notebook. Now, you should upload model file to JetBot in to this notebook's directory. Once that's finished there should be a file named ``best_steering_model_xy.pth`` in this notebook's directory."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "> Please make sure the file has uploaded fully before calling the next cell"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Execute the code below to initialize the PyTorch model. This should look very familiar from the training notebook."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torchvision\n",
+    "import torch\n",
+    "from jetbot.utils import model_selection\n",
+    "import os \n",
+    "import ipywidgets.widgets as widgets\n",
+    "from ipywidgets.widgets import Box, HBox, VBox, Layout, Label\n",
+    "import traitlets\n",
+    "\n",
+    "# The path of trt models is 'MODEL_REPO_DIR_DOCKER' which is set in /jetbot/utils/model_selection.py,\n",
+    "# which may be modified if you change the file path of trt models, 'MODEL_REPO_DIR_DOCKER' or dir_model_repo.\n",
+    "\n",
+    "dir_model_repo = os.environ['MODEL_REPO_DIR_DOCKER']\n",
+    "print(dir_model_repo)\n",
+    "\n",
+    "# pth_ms = model_selection(core_library = \"Pytorch\")  # if 'MODEL_REPO_DIR_DOCKER' is used.\n",
+    "pth_ms = model_selection(core_library = \"Pytorch\", dir_model_repo=dir_model_repo)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "pth_ms.model_function = \"classifier\"\n",
+    "\n",
+    "model_type_widget = widgets.Select(options=pth_ms.model_type_list, value=pth_ms.model_type_list[0],\n",
+    "                                      description='Model Type:')\n",
+    "traitlets.dlink((pth_ms, 'model_type_list'), (model_type_widget, 'options'))\n",
+    "traitlets.dlink((model_type_widget, 'value'), (pth_ms, 'model_type'))\n",
+    "\n",
+    "model_path_widget = widgets.Select(options=pth_ms.model_path_list, description='Model Path:',\n",
+    "                                      layout=Layout(width='65%'))\n",
+    "traitlets.dlink((pth_ms, 'model_path_list'), (model_path_widget, 'options'))\n",
+    "traitlets.dlink((model_path_widget, 'value'), (pth_ms, 'model_path'))\n",
+    "\n",
+    "display(HBox([model_type_widget, model_path_widget]))\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# load pytorch model with no pretrained parameters\n",
+    "from jetbot.utils.model_selection import load_tune_pth_model\n",
+    "model_path = model_path_widget.value\n",
+    "pth_model_name = model_path_widget.value.split('/')[-1].split('.')[0].split('_', 4)[-1]\n",
+    "print(pth_model_name, '\\n', model_path)\n",
+    "\n",
+    "model, model_type = load_tune_pth_model(pth_model_name=pth_model_name, pretrained=False)\n",
+    "model = model.cuda().eval().half()\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Next, load the trained weights from the model_path (e.g.``best_steering_model_xy_<<pth_model_name>>.pth``) file that you uploaded."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model.load_state_dict(torch.load(model_path))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "device = torch.device('cuda')"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## TensorRT"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "> Note: if you are running with docker container, you may not need to do the following installation. \n",
+    "\n",
+    "> If your setup does not have `torch2trt` installed, you need to first install `torch2trt` by executing the following in the console.\n",
+    "```bash\n",
+    "cd $HOME\n",
+    "git clone https://github.com/NVIDIA-AI-IOT/torch2trt\n",
+    "cd torch2trt\n",
+    "sudo python3 setup.py install\n",
+    "```\n",
+    "\n",
+    "Convert and optimize the model using torch2trt for faster inference with TensorRT. Please see the [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) readme for more details.\n",
+    "\n",
+    "> This optimization process can take a couple minutes to complete."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from torch2trt import torch2trt\n",
+    "\n",
+    "if pth_model_name == 'inception_v3':\n",
+    "    data = torch.zeros((1, 3, 299, 299)).cuda().half()   # inception_v3\n",
+    "else:\n",
+    "    data = torch.zeros((1, 3, 224, 224)).cuda().half()  # resnet\n",
+    "\n",
+    "model_trt = torch2trt(model, [data], fp16_mode=True)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Save the optimized model using the cell below"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import pandas as pd\n",
+    "\n",
+    "path_trt_model = os.path.join(dir_model_repo, 'road_following', \"best_steering_model_xy_trt_\"+pth_model_name+'.pth')\n",
+    "torch.save(model_trt.state_dict(), path_trt_model)\n",
+    "\n",
+    "df_file = os.path.join(dir_model_repo, 'trt_model_tbl.csv')\n",
+    "if os.path.isfile(df_file):\n",
+    "    df = pd.read_csv(df_file, header=None)\n",
+    "else:\n",
+    "    df = pd.DataFrame()\n",
+    "\n",
+    "trt_model_path_tbl = os.path.join('.', 'road_following', \"best_steering_model_xy_trt_\"+pth_model_name+'.pth')\n",
+    "df = df.append([[\"classifier\", model_type, trt_model_path_tbl]], ignore_index = False)\n",
+    "df = df.drop_duplicates()\n",
+    "df.to_csv(df_file, header=False, index=False)\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Next\n",
+    "Open live_demo_trt.ipynb to move JetBot with the TensorRT optimized model."
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.6.9"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
Index: notebooks/road_following/train_model_PC_o.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/road_following/train_model_PC_o.ipynb b/notebooks/road_following/train_model_PC_o.ipynb
new file mode 100644
--- /dev/null	(date 1722782630000)
+++ b/notebooks/road_following/train_model_PC_o.ipynb	(date 1722782630000)
@@ -0,0 +1,439 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "tags": []
+   },
+   "source": [
+    "# Road Follower - Train Model\n",
+    "\n",
+    "In this notebook we will train a neural network to take an input image, and output a set of x, y values corresponding to a target.\n",
+    "\n",
+    "We will be using PyTorch deep learning framework to train ResNet18 neural network architecture model for road follower application."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# execute this script as followings:\n",
+    "# 1. set project interpreter to PC python of version 3.8 pr above, and then\n",
+    "# 2. run jupyter lab in command window \n",
+    "\n",
+    "import torch\n",
+    "import torch.optim as optim\n",
+    "import torch.nn.functional as F\n",
+    "import torchvision\n",
+    "import torchvision.datasets as datasets\n",
+    "import torchvision.models as models\n",
+    "import torchvision.transforms as transforms\n",
+    "import glob\n",
+    "import PIL.Image\n",
+    "import os\n",
+    "import numpy as np\n",
+    "import ipywidgets\n",
+    "\n",
+    "\n",
+    "TRAIN_MODEL = \"inception_v3\"  # resnet18, resnet34, resnet50, resnet101, mobilenet_v2, vgg11, mobilenet_v3_large, inception_v3, efficientnet_b4, googlenet\n",
+    "\n",
+    "# *** refererence : https://pytorch.org/docs/stable/optim.html#algorithms\n",
+    "# use the following learning algorithms for evaluation\n",
+    "TRAIN_METHOD = \"Adam\"  # \"Adam\", \"SGD\", \"ASGD\", \"Adadelta\", \"RAdam\"; the parameters lr=0.01, momentum=0.92 is needed for SGD"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Download and extract data\n",
+    "\n",
+    "Before you start, you should upload the zip file that you created in the ``data_collection.ipynb`` notebook on the robot. \n",
+    "\n",
+    "> If you're training on the JetBot you collected data on, you can skip this!\n",
+    "\n",
+    "You should then extract this dataset by calling the command below:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# !unzip -q road_following.zip\n",
+    "from zipfile import ZipFile\n",
+    "dir_depo = 'D:\\\\AI_Lecture_Demos\\\\Data_Repo\\\\Cuterbot_2004_Repo'\n",
+    "os.makedirs(dir_depo, exist_ok=True)\n",
+    "# dir_depo = os.getcwd()\n",
+    "training_datafile = 'dataset_xy_0916_1.zip'  # check the data file is loaded to dir_depo\n",
+    "\n",
+    "with ZipFile(os.path.join(dir_depo, training_datafile), 'r') as zObject:\n",
+    "    zObject.extractall(path=dir_depo)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "You should see a folder named ``dataset_xy`` appear in the file directory dir_depo."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Create Dataset Instance\n",
+    "\n",
+    "Here we create a custom ``torch.utils.data.Dataset`` implementation, which implements the ``__len__`` and ``__getitem__`` functions.  This class\n",
+    "is responsible for loading images and parsing the x, y values from the image filenames.  Because we implement the ``torch.utils.data.Dataset`` class,\n",
+    "we can use all of the torch data utilities :)\n",
+    "\n",
+    "We hard coded some transformations (like color jitter) into our dataset.  We made random horizontal flips optional (in case you want to follow a non-symmetric path, like a road\n",
+    "where we need to 'stay right').  If it doesn't matter whether your robot follows some convention, you could enable flips to augment the dataset."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def get_x(path, width):\n",
+    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
+    "    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n",
+    "\n",
+    "def get_y(path, height):\n",
+    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
+    "    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n",
+    "\n",
+    "class XYDataset(torch.utils.data.Dataset):\n",
+    "    \n",
+    "    def __init__(self, directory, random_hflips=False):\n",
+    "        self.directory = directory\n",
+    "        self.random_hflips = random_hflips\n",
+    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
+    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
+    "    \n",
+    "    def __len__(self):\n",
+    "        return len(self.image_paths)\n",
+    "    \n",
+    "    def __getitem__(self, idx):\n",
+    "        image_path = self.image_paths[idx]\n",
+    "        \n",
+    "        image = PIL.Image.open(image_path)\n",
+    "        width, height = image.size\n",
+    "        x = float(get_x(os.path.basename(image_path), width))\n",
+    "        y = float(get_y(os.path.basename(image_path), height))\n",
+    "      \n",
+    "        if float(np.random.rand(1)) > 0.5:\n",
+    "            image = transforms.functional.hflip(image)\n",
+    "            x = -x\n",
+    "        \n",
+    "        image = self.color_jitter(image)\n",
+    "        if TRAIN_MODEL == 'inception_v3':\n",
+    "            image = transforms.functional.resize(image, (299, 299))\n",
+    "        else:\n",
+    "            image = transforms.functional.resize(image, (224, 224))\n",
+    "        image = transforms.functional.to_tensor(image)\n",
+    "        image = image.numpy()[::-1].copy()\n",
+    "        image = torch.from_numpy(image)\n",
+    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
+    "        \n",
+    "        return image, torch.tensor([x, y]).float()\n",
+    "    \n",
+    "dataset = XYDataset(os.path.join(dir_depo, 'dataset_xy'), random_hflips=False)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Split dataset into train and test sets\n",
+    "Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 90%-10%. The test set will be used to verify the accuracy of the model we train."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "test_percent = 0.1\n",
+    "num_test = int(test_percent * len(dataset))\n",
+    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Create data loaders to load data in batches\n",
+    "\n",
+    "We use ``DataLoader`` class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "train_loader = torch.utils.data.DataLoader(\n",
+    "    train_dataset,\n",
+    "    batch_size=8,\n",
+    "    shuffle=True,\n",
+    "    num_workers=0\n",
+    ")\n",
+    "\n",
+    "test_loader = torch.utils.data.DataLoader(\n",
+    "    test_dataset,\n",
+    "    batch_size=8,\n",
+    "    shuffle=True,\n",
+    "    num_workers=0\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Define Neural Network Model \n",
+    "We use the models (the parameter setting TRAIN_MODEL above) available on PyTorch TorchVision, such as resnet18, resnet34, resnet50, resnet101, mobilenet_v2, vgg11, mobilenet_v3_large, inception_v3, efficientnet_b4, googlenet.\n",
+    "\n",
+    "Referring the following information pytorch model web site:\n",
+    "1. classifier : https://pytorch.org/vision/0.10/models.html#classification\n",
+    "2. github : https://github.com/pytorch/vision/tree/release/0.11/torchvision/models\n",
+    "More details on ResNet-18 and other variants: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
+    "\n",
+    "### Define Neural Network Model Learning Algoritmom\n",
+    "We can use the following learning algorithms (the parameter setting TRAIN_METHOD above) for training a model\n",
+    "\"Adam\", \"SGD\", \"ASGD\", \"Adadelta\", \"RAdam\"; the parameters lr=0.01, momentum=0.92 is needed for SGD\n",
+    "1. reference web site : https://pytorch.org/docs/stable/optim.html#algorithms\n",
+    "\n",
+    "In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n",
+    "More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE \n",
+    "\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# model = models.resnet18(pretrained=True)\n",
+    "model = getattr(models, TRAIN_MODEL)()\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "ResNet model has fully connect (fc) final layer with 512 as ``in_features`` and we will be training for regression thus ``out_features`` as 2\n",
+    "\n",
+    "Finally, we transfer our model for execution on the GPU"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "\n",
+    "# ----- modify last layer for classification, and the model used in notebook should be modified as well.\n",
+    "\n",
+    "if TRAIN_MODEL == 'mobilenet_v3_large':  # MobileNet\n",
+    "    model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, 2)  # for mobilenet_v3 model. must add the block expansion factor 4\n",
+    "\n",
+    "elif TRAIN_MODEL == 'mobilenet_v2':\n",
+    "    model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # for mobilenet_v2 model. must add the block expansion factor 4\n",
+    "\n",
+    "elif TRAIN_MODEL == 'vgg11': # VGGNet\n",
+    "    model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)  # for VGG model. must add the block expansion factor 4\n",
+    "\n",
+    "elif 'resnet' in TRAIN_MODEL: # ResNet\n",
+    "    model.fc = torch.nn.Linear(model.fc.in_features, 2)  # for resnet model must add the block expansion factor 4\n",
+    "    # model.fc = torch.nn.Linear(512, 2)\n",
+    "\n",
+    "elif TRAIN_MODEL == 'inception_v3':   # Inception_v3\n",
+    "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
+    "    if model.aux_logits:\n",
+    "        model.AuxLogits.fc = torch.nn.Linear(model.AuxLogits.fc.in_features, 2)\n",
+    "\n",
+    "elif 'efficientnet' in TRAIN_MODEL:   # efficientnet\n",
+    "    model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # for efficientnet_b1 \n",
+    "    # model.classifier[0] = torch.nn.Dropout(p=0.4, inplace=True)\n",
+    "    model.classifier\n",
+    "\n",
+    "\n",
+    "# ** you may use CPU or GPU for training\n",
+    "processor = 'GPU'\n",
+    "if processor == 'GPU':\n",
+    "    device = torch.device('cuda')\n",
+    "    print(\"torch cuda version : \", torch.version.cuda)\n",
+    "    print(\"cuda is available for pytorch: \", torch.cuda.is_available())    \n",
+    "elif processor == 'CPU':\n",
+    "    device = torch.device('cpu')\n",
+    "model = model.float()\n",
+    "model = model.to(device, dtype=torch.float)\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Train Regression:\n",
+    "\n",
+    "We train for 70 epochs and save best model if the loss is reduced. "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "%cd \"../../jetbot/utils\"\n",
+    "import time\n",
+    "from tqdm.notebook import tqdm\n",
+    "from training_profile import *\n",
+    "\n",
+    "dir_training_records = os.path.join(dir_depo, 'training records', processor, TRAIN_MODEL)\n",
+    "os.makedirs(dir_training_records, exist_ok=True)\n",
+    "\n",
+    "DIR_MODEL_REPO = os.path.join(dir_depo, 'model_repo', processor)\n",
+    "os.makedirs(DIR_MODEL_REPO, exist_ok=True)    \n",
+    "# BEST_MODEL_PATH = 'best_steering_model_xy.pth'\n",
+    "BEST_MODEL_PATH = os.path.join(DIR_MODEL_REPO, \"best_steering_model_xy_\" + TRAIN_MODEL + \".pth\")\n",
+    "\n",
+    "NUM_EPOCHS = 70\n",
+    "best_loss = 1e9\n",
+    "\n",
+    "optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), weight_decay=0)\n",
+    "# optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), lr=0.01, momentum=0.95)\n",
+    "\n",
+    "loss_data = []\n",
+    "lt_epoch = []  # learning time per epoch\n",
+    "lt_sample = []  # learning time per epoch\n",
+    "\n",
+    "batch_size = len(train_loader)\n",
+    "pbar_overall_format = \"{desc} {percentage:.2f}% | {bar} | elapsed: {elapsed}; estimated to finish: {remaining}\"\n",
+    "pbar_overall = tqdm(total=100, bar_format = pbar_overall_format)\n",
+    "show_batch_progress = False   # Set True if need to show the batch learning progress in an epoch\n",
+    "show_training_plot = False  # Set True if need to show the converbent profile during training\n",
+    "\n",
+    "best_loss = None\n",
+    "\n",
+    "for epoch in range(NUM_EPOCHS):\n",
+    "    start_epoch = time.time()\n",
+    "    \n",
+    "    model.train()\n",
+    "    train_loss = 0.0\n",
+    "    \n",
+    "    if show_batch_progress:\n",
+    "        pbar_batch = tqdm(train_loader, total = batch_size)\n",
+    "    else:\n",
+    "        pbar_batch = iter(train_loader)\n",
+    "    for index, (images, labels) in enumerate(pbar_batch):\n",
+    "        start_sample = time.time()\n",
+    "        \n",
+    "        images = images.to(device)\n",
+    "        labels = labels.to(device)\n",
+    "        optimizer.zero_grad()\n",
+    "        if TRAIN_MODEL == 'inception_v3':\n",
+    "            outputs = model(images)\n",
+    "            loss_main = F.mse_loss(outputs.logits, labels, reduction='mean')\n",
+    "            loss_aux = F.mse_loss(outputs.aux_logits, labels, reduction='mean')\n",
+    "            loss = loss_main + 0.3 * loss_aux\n",
+    "        else:\n",
+    "            outputs = model(images)\n",
+    "            loss = F.mse_loss(outputs, labels, reduction='mean')\n",
+    "        train_loss += float(loss)\n",
+    "        loss.backward()\n",
+    "        optimizer.step()\n",
+    "        \n",
+    "        end_sample = time.time()\n",
+    "        lt_sample.append(end_sample - start_sample)\n",
+    "        \n",
+    "        pbar_overall.update(round(100/(NUM_EPOCHS*batch_size), 2))\n",
+    "        pbar_overall.set_description(desc = f'Overall progress - Epoch [{epoch+1}/{NUM_EPOCHS}]')\n",
+    "        pbar_overall.set_postfix(best_loss = best_loss, train_loss = train_loss/(index+1))\n",
+    "               \n",
+    "        if show_batch_progress:\n",
+    "            pbar_batch.set_description(desc = f'Progress in the epoch {epoch+1} ')\n",
+    "            pbar_batch.set_postfix(mean_batch_loss = train_loss/(index+1))\n",
+    "    \n",
+    "    train_loss /= len(train_loader)\n",
+    "    \n",
+    "    model.eval()\n",
+    "    test_loss = 0.0\n",
+    "    for images, labels in iter(test_loader):\n",
+    "        images = images.to(device)\n",
+    "        labels = labels.to(device)\n",
+    "        outputs = model(images)\n",
+    "        loss = F.mse_loss(outputs, labels)\n",
+    "        test_loss += float(loss)\n",
+    "    test_loss /= len(test_loader)\n",
+    "\n",
+    "    end_epoch = time.time()\n",
+    "    lt_epoch.append(end_epoch - start_epoch)\n",
+    "      \n",
+    "    if best_loss == None or test_loss < best_loss :\n",
+    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
+    "        best_loss = test_loss\n",
+    "    \n",
+    "    loss_data.append([train_loss, test_loss])\n",
+    "    \n",
+    "# function plot_loss(loss_data, best_loss, no_epoch, dir_training_records, train_model, train_method) is in jetbot.utils\n",
+    "    if epoch == NUM_EPOCHS:\n",
+    "        show_training_plot=True    \n",
+    "    plot_loss(loss_data=loss_data, best_loss=best_loss, no_epoch=NUM_EPOCHS,\n",
+    "              dir_training_records=dir_training_records, # the directory stored training records\n",
+    "              train_model=TRAIN_MODEL, train_method=TRAIN_METHOD, processor=processor, # this 2 parameters are for plot title only\n",
+    "              show_training_plot=show_training_plot)\n",
+    "\n",
+    "overall_time = pbar_overall.format_dict[\"elapsed\"]\n",
+    "# function lt_plot(lt_epoch, lt_sample, dir_training_records, train_model, train_method) is in jetbot.utils\n",
+    "lt_plot(lt_epoch=lt_epoch, lt_sample=lt_sample, overall_time=overall_time,\n",
+    "        dir_training_records=dir_training_records, # the directory stored training records\n",
+    "        train_model=TRAIN_MODEL, train_method=TRAIN_METHOD, processor=processor) # this 2 parameters are for plot title only\n",
+    "\n",
+    "print('Training is completed! \\n you can close the figures by restart the kernel!')"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Once the model is trained, it will generate ``best_steering_model_xy_<TRAIN_MODEL>.pth`` file which you can use for inferencing in the live demo notebook.\n",
+    "\n",
+    "If you trained on a different machine other than JetBot, you'll need to upload this to the JetBot to the ``road_following`` example folder."
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.6.8"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
Index: notebooks/road_following/live_demo_build_trt_v0.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/road_following/live_demo_build_trt_v0.ipynb b/notebooks/road_following/live_demo_build_trt_v0.ipynb
new file mode 100644
--- /dev/null	(date 1722795975000)
+++ b/notebooks/road_following/live_demo_build_trt_v0.ipynb	(date 1722795975000)
@@ -0,0 +1,262 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Road Following - Build TensorRT model for live demo"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "In this notebook, we will optimize the model we trained using TensorRT."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Load the trained model"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "We will assume that you have already downloaded ``best_steering_model_xy.pth`` to work station as instructed in \"train_model.ipynb\" notebook. Now, you should upload model file to JetBot in to this notebook's directory. Once that's finished there should be a file named ``best_steering_model_xy.pth`` in this notebook's directory."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "> Please make sure the file has uploaded fully before calling the next cell"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Execute the code below to initialize the PyTorch model. This should look very familiar from the training notebook."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torchvision\n",
+    "import torch\n",
+    "from jetbot.utils import model_selection\n",
+    "import os \n",
+    "import ipywidgets.widgets as widgets\n",
+    "from ipywidgets.widgets import Box, HBox, VBox, Layout, Label\n",
+    "import traitlets\n",
+    "\n",
+    "model_repo_dir = os.environ['MODEL_REPO_DIR_DOCKER']\n",
+    "print(model_repo_dir)\n",
+    "\n",
+    "# pth_ms = model_selection(core_library = \"Pytorch\", model_repo_dir=model_repo_dir)\n",
+    "pth_ms = model_selection(core_library = \"Pytorch\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "pth_ms.model_function = \"classifier\"\n",
+    "\n",
+    "model_type_widget = widgets.Select(options=pth_ms.model_type_list, value=pth_ms.model_type_list[0],\n",
+    "                                      description='Model Type:')\n",
+    "traitlets.dlink((pth_ms, 'model_type_list'), (model_type_widget, 'options'))\n",
+    "traitlets.dlink((model_type_widget, 'value'), (pth_ms, 'model_type'))\n",
+    "# traitlets.dlink((pth_ms, 'model_type'), (RC, 'type_cruiser_model'))\n",
+    "\n",
+    "model_path_widget = widgets.Select(options=pth_ms.model_path_list, description='Model Path:',\n",
+    "                                      layout=Layout(width='60%'))\n",
+    "traitlets.dlink((pth_ms, 'model_path_list'), (model_path_widget, 'options'))\n",
+    "traitlets.dlink((model_path_widget, 'value'), (pth_ms, 'model_path'))\n",
+    "# traitlets.dlink((pth_ms, 'model_path'), (RC, 'cruiser_model'))\n",
+    "\n",
+    "display(HBox([model_type_widget, model_path_widget]))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model_path = model_path_widget.value\n",
+    "pth_model_name = model_path_widget.value.split('/')[-1].split('.')[0].split('_', 4)[-1]\n",
+    "print(pth_model_name, '\\n', model_path)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torchvision.models as pth_models\n",
+    "model = getattr(pth_models, pth_model_name)()\n",
+    "\n",
+    "# ----- modify last layer for classification, and the model used in notebook should be modified as well.\n",
+    "\n",
+    "if pth_model_name == 'mobilenet_v3_large':  # MobileNet\n",
+    "    model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, 2)  # for mobilenet_v3 model. must add the block expansion factor 4\n",
+    "\n",
+    "elif pth_model_name == 'mobilenet_v2':\n",
+    "    model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # for mobilenet_v2 model. must add the block expansion factor 4\n",
+    "\n",
+    "elif pth_model_name == 'vgg11': # VGGNet\n",
+    "    model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)  # for VGG model. must add the block expansion factor 4\n",
+    "\n",
+    "elif 'resnet' in pth_model_name: # ResNet\n",
+    "    model.fc = torch.nn.Linear(model.fc.in_features, 2)  # for resnet model must add the block expansion factor 4\n",
+    "    # model.fc = torch.nn.Linear(512, 2)\n",
+    "\n",
+    "elif pth_model_name == 'inception_v3':   # Inception_v3\n",
+    "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
+    "    if model.aux_logits:\n",
+    "        model.AuxLogits.fc = torch.nn.Linear(model.AuxLogits.fc.in_features, 2)\n",
+    "\n",
+    "elif 'efficientnet' in pth_model_name:   # efficientnet\n",
+    "    model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # for efficientnet_b1 \n",
+    "\n",
+    "model = model.cuda().eval().half()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Next, load the trained weights from the ``best_steering_model_xy.pth`` file that you uploaded."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model.load_state_dict(torch.load(model_path))\n",
+    "# model.load_state_dict(torch.load('best_steering_model_xy_resnet18.pth'))\n",
+    "# model.load_state_dict(torch.load('best_steering_model_xy_resnet34.pth'))\n",
+    "# model.load_state_dict(torch.load('best_steering_model_xy_mobilenet_v3_large.pth'))\n",
+    "# model.load_state_dict(torch.load('best_steering_model_xy_inception_v3.pth'))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "device = torch.device('cuda')"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## TensorRT"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "> If your setup does not have `torch2trt` installed, you need to first install `torch2trt` by executing the following in the console.\n",
+    "```bash\n",
+    "cd $HOME\n",
+    "git clone https://github.com/NVIDIA-AI-IOT/torch2trt\n",
+    "cd torch2trt\n",
+    "sudo python3 setup.py install\n",
+    "```\n",
+    "\n",
+    "Convert and optimize the model using torch2trt for faster inference with TensorRT. Please see the [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) readme for more details.\n",
+    "\n",
+    "> This optimization process can take a couple minutes to complete."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from torch2trt import torch2trt\n",
+    "\n",
+    "if pth_model_name == 'inception_v3':\n",
+    "    data = torch.zeros((1, 3, 299, 299)).cuda().half()   # inception_v3\n",
+    "else:\n",
+    "    data = torch.zeros((1, 3, 224, 224)).cuda().half()  # resnet\n",
+    "\n",
+    "model_trt = torch2trt(model, [data], fp16_mode=True)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Save the optimized model using the cell below"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "path_trt_model = os.path.join(model_repo_dir, 'road_following', \"best_steering_model_xy_trt_\"+pth_model_name+'.pth')\n",
+    "torch.save(model_trt.state_dict(), path_trt_model)\n",
+    "# torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_resnet18.pth')\n",
+    "# torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_resnet34.pth')\n",
+    "# torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_mobilenet_v3_large.pth')\n",
+    "# torch.save(model_trt.state_dict(), 'best_steering_model_xy_trt_inception_v3.pth')"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Next\n",
+    "Open live_demo_trt.ipynb to move JetBot with the TensorRT optimized model."
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.6.9"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
Index: notebooks/road_following/train_model_PC.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"# Road Follower - Train Model\\n\",\r\n    \"\\n\",\r\n    \"In this notebook we will train a neural network to take an input image, and output a set of x, y values corresponding to a target.\\n\",\r\n    \"\\n\",\r\n    \"We will be using PyTorch deep learning framework to train ResNet18 neural network architecture model for road follower application.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# execute this script as followings:\\n\",\r\n    \"# set project interpreter to PC python of version 3.8 pr above, and then\\n\",\r\n    \"# run jupyter lab in command window \\n\",\r\n    \"\\n\",\r\n    \"import torch\\n\",\r\n    \"import torch.optim as optim\\n\",\r\n    \"import torch.nn.functional as F\\n\",\r\n    \"import torchvision\\n\",\r\n    \"import torchvision.datasets as datasets\\n\",\r\n    \"import torchvision.models as models\\n\",\r\n    \"import torchvision.transforms as transforms\\n\",\r\n    \"import glob\\n\",\r\n    \"import PIL.Image\\n\",\r\n    \"import os\\n\",\r\n    \"import numpy as np\\n\",\r\n    \"import ipywidgets\\n\",\r\n    \"\\n\",\r\n    \"TRAIN_MODEL = \\\"resnet101\\\"  # resnet18, resnet34, resnet50, resnet101, mobilenet_v2, vgg11, mobilenet_v3_large, inception_v3\\n\",\r\n    \"\\n\",\r\n    \"# *** refererence : https://pytorch.org/docs/stable/optim.html#algorithms\\n\",\r\n    \"# use the following learning algorithms for evaluation\\n\",\r\n    \"TRAIN_METHOD = \\\"Adam\\\"  # \\\"Adam\\\", \\\"SGD\\\", \\\"ASGD\\\", \\\"Adadelta\\\", \\\"RAdam\\\"; the parameters lr=0.01, momentum=0.92 is needed for SGD\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Download and extract data\\n\",\r\n    \"\\n\",\r\n    \"Before you start, you should upload the zip file that you created in the ``data_collection.ipynb`` notebook on the robot. \\n\",\r\n    \"\\n\",\r\n    \"> If you're training on the JetBot you collected data on, you can skip this!\\n\",\r\n    \"\\n\",\r\n    \"You should then extract this dataset by calling the command below:\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# !unzip -q road_following.zip\\n\",\r\n    \"from zipfile import ZipFile\\n\",\r\n    \"dir_depo = 'D:\\\\\\\\AI_Lecture_Demos\\\\\\\\Data_Repo\\\\\\\\Cuterbot_2004_Repo'\\n\",\r\n    \"os.makedirs(dir_depo, exist_ok=True)\\n\",\r\n    \"# dir_depo = os.getcwd()\\n\",\r\n    \"with ZipFile(os.path.join(dir_depo, 'dataset_xy_0916_1.zip'), 'r') as zObject:\\n\",\r\n    \"    zObject.extractall(path=dir_depo)\\n\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"You should see a folder named ``dataset_xy`` appear in the file directory dir_depo.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Create Dataset Instance\\n\",\r\n    \"\\n\",\r\n    \"Here we create a custom ``torch.utils.data.Dataset`` implementation, which implements the ``__len__`` and ``__getitem__`` functions.  This class\\n\",\r\n    \"is responsible for loading images and parsing the x, y values from the image filenames.  Because we implement the ``torch.utils.data.Dataset`` class,\\n\",\r\n    \"we can use all of the torch data utilities :)\\n\",\r\n    \"\\n\",\r\n    \"We hard coded some transformations (like color jitter) into our dataset.  We made random horizontal flips optional (in case you want to follow a non-symmetric path, like a road\\n\",\r\n    \"where we need to 'stay right').  If it doesn't matter whether your robot follows some convention, you could enable flips to augment the dataset.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"def get_x(path, width):\\n\",\r\n    \"    \\\"\\\"\\\"Gets the x value from the image filename\\\"\\\"\\\"\\n\",\r\n    \"    return (float(int(path.split(\\\"_\\\")[1])) - width/2) / (width/2)\\n\",\r\n    \"\\n\",\r\n    \"def get_y(path, height):\\n\",\r\n    \"    \\\"\\\"\\\"Gets the y value from the image filename\\\"\\\"\\\"\\n\",\r\n    \"    return (float(int(path.split(\\\"_\\\")[2])) - height/2) / (height/2)\\n\",\r\n    \"\\n\",\r\n    \"class XYDataset(torch.utils.data.Dataset):\\n\",\r\n    \"    \\n\",\r\n    \"    def __init__(self, directory, random_hflips=False):\\n\",\r\n    \"        self.directory = directory\\n\",\r\n    \"        self.random_hflips = random_hflips\\n\",\r\n    \"        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\\n\",\r\n    \"        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\\n\",\r\n    \"    \\n\",\r\n    \"    def __len__(self):\\n\",\r\n    \"        return len(self.image_paths)\\n\",\r\n    \"    \\n\",\r\n    \"    def __getitem__(self, idx):\\n\",\r\n    \"        image_path = self.image_paths[idx]\\n\",\r\n    \"        \\n\",\r\n    \"        image = PIL.Image.open(image_path)\\n\",\r\n    \"        width, height = image.size\\n\",\r\n    \"        x = float(get_x(os.path.basename(image_path), width))\\n\",\r\n    \"        y = float(get_y(os.path.basename(image_path), height))\\n\",\r\n    \"      \\n\",\r\n    \"        if float(np.random.rand(1)) > 0.5:\\n\",\r\n    \"            image = transforms.functional.hflip(image)\\n\",\r\n    \"            x = -x\\n\",\r\n    \"        \\n\",\r\n    \"        image = self.color_jitter(image)\\n\",\r\n    \"        if TRAIN_MODEL == 'inception_v3':\\n\",\r\n    \"            image = transforms.functional.resize(image, (299, 299))\\n\",\r\n    \"        else:\\n\",\r\n    \"            image = transforms.functional.resize(image, (224, 224))\\n\",\r\n    \"        image = transforms.functional.to_tensor(image)\\n\",\r\n    \"        image = image.numpy()[::-1].copy()\\n\",\r\n    \"        image = torch.from_numpy(image)\\n\",\r\n    \"        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\\n\",\r\n    \"        \\n\",\r\n    \"        return image, torch.tensor([x, y]).float()\\n\",\r\n    \"    \\n\",\r\n    \"dataset = XYDataset(os.path.join(dir_depo, 'dataset_xy'), random_hflips=False)\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Split dataset into train and test sets\\n\",\r\n    \"Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 90%-10%. The test set will be used to verify the accuracy of the model we train.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"test_percent = 0.1\\n\",\r\n    \"num_test = int(test_percent * len(dataset))\\n\",\r\n    \"train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Create data loaders to load data in batches\\n\",\r\n    \"\\n\",\r\n    \"We use ``DataLoader`` class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"train_loader = torch.utils.data.DataLoader(\\n\",\r\n    \"    train_dataset,\\n\",\r\n    \"    batch_size=8,\\n\",\r\n    \"    shuffle=True,\\n\",\r\n    \"    num_workers=0\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"test_loader = torch.utils.data.DataLoader(\\n\",\r\n    \"    test_dataset,\\n\",\r\n    \"    batch_size=8,\\n\",\r\n    \"    shuffle=True,\\n\",\r\n    \"    num_workers=0\\n\",\r\n    \")\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Define Neural Network Model \\n\",\r\n    \"\\n\",\r\n    \"We use ResNet-18 model available on PyTorch TorchVision. \\n\",\r\n    \"\\n\",\r\n    \"In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\\n\",\r\n    \"\\n\",\r\n    \"\\n\",\r\n    \"More details on ResNet-18 and other variants: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\\n\",\r\n    \"\\n\",\r\n    \"More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# model = models.resnet18(pretrained=True)\\n\",\r\n    \"# model = models.resnet101(pretrained=True)\\n\",\r\n    \"model = getattr(models, TRAIN_MODEL)()\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"ResNet model has fully connect (fc) final layer with 512 as ``in_features`` and we will be training for regression thus ``out_features`` as 2\\n\",\r\n    \"\\n\",\r\n    \"Finally, we transfer our model for execution on the GPU\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"print(\\\"torch cuda version : \\\", torch.version.cuda)\\n\",\r\n    \"print(\\\"cuda is available for pytorch: \\\", torch.cuda.is_available())\\n\",\r\n    \"\\n\",\r\n    \"# ----- modify last layer for classification, and the model used in notebook should be modified as well.\\n\",\r\n    \"\\n\",\r\n    \"if TRAIN_MODEL == 'mobilenet_v3_large':  # MobileNet\\n\",\r\n    \"    model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, 2)  # for mobilenet_v3 model. must add the block expansion factor 4\\n\",\r\n    \"\\n\",\r\n    \"elif TRAIN_MODEL == 'mobilenet_v2':\\n\",\r\n    \"    model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # for mobilenet_v2 model. must add the block expansion factor 4\\n\",\r\n    \"\\n\",\r\n    \"elif TRAIN_MODEL == 'vgg11': # VGGNet\\n\",\r\n    \"    model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)  # for VGG model. must add the block expansion factor 4\\n\",\r\n    \"\\n\",\r\n    \"elif 'resnet' in TRAIN_MODEL: # ResNet\\n\",\r\n    \"    model.fc = torch.nn.Linear(model.fc.in_features, 2)  # for resnet model must add the block expansion factor 4\\n\",\r\n    \"    # model.fc = torch.nn.Linear(512, 2)\\n\",\r\n    \"\\n\",\r\n    \"elif TRAIN_MODEL == 'inception_v3':   # Inception_v3\\n\",\r\n    \"    model.fc = torch.nn.Linear(model.fc.in_features, 2)\\n\",\r\n    \"    if model.aux_logits:\\n\",\r\n    \"        model.AuxLogits.fc = torch.nn.Linear(model.AuxLogits.fc.in_features, 2)\\n\",\r\n    \"\\n\",\r\n    \"# ** you may use cpu for training\\n\",\r\n    \"device = torch.device('cuda')\\n\",\r\n    \"# device = torch.device('cpu')\\n\",\r\n    \"model = model.float()\\n\",\r\n    \"model = model.to(device, dtype=torch.float)\\n\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Train Regression:\\n\",\r\n    \"\\n\",\r\n    \"We train for 70 epochs and save best model if the loss is reduced. \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"%cd \\\"../../jetbot/utils\\\"\\n\",\r\n    \"import time\\n\",\r\n    \"from tqdm.notebook import tqdm\\n\",\r\n    \"from training_profile import *\\n\",\r\n    \"\\n\",\r\n    \"dir_training_records = os.path.join(dir_depo, 'training records', TRAIN_MODEL)\\n\",\r\n    \"os.makedirs(dir_training_records, exist_ok=True)\\n\",\r\n    \"\\n\",\r\n    \"DIR_MODEL_REPO = os.path.join(dir_depo, 'model_repo')\\n\",\r\n    \"os.makedirs(DIR_MODEL_REPO, exist_ok=True)\\n\",\r\n    \"# BEST_MODEL_PATH = 'best_steering_model_xy.pth'\\n\",\r\n    \"BEST_MODEL_PATH = os.path.join(DIR_MODEL_REPO, \\\"best_steering_model_xy_\\\" + TRAIN_MODEL + \\\".pth\\\")\\n\",\r\n    \"\\n\",\r\n    \"NUM_EPOCHS = 3\\n\",\r\n    \"best_loss = 1e9\\n\",\r\n    \"\\n\",\r\n    \"max_count = NUM_EPOCHS * len(train_loader)\\n\",\r\n    \"pbar = tqdm(total=100)\\n\",\r\n    \"pbar.set_description(desc=\\\"Training Progressing\\\")\\n\",\r\n    \"\\n\",\r\n    \"# optimizer = optim.Adam(model.parameters())\\n\",\r\n    \"optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), weight_decay=0)\\n\",\r\n    \"# optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), lr=0.01, momentum=0.95)\\n\",\r\n    \"\\n\",\r\n    \"loss_data = []\\n\",\r\n    \"lt_epoch = []  # learning time per epoch\\n\",\r\n    \"lt_sample = []  # learning time per epoch\\n\",\r\n    \"\\n\",\r\n    \"for epoch in range(NUM_EPOCHS):\\n\",\r\n    \"    start_epoch = time.process_time()\\n\",\r\n    \"    \\n\",\r\n    \"    model.train()\\n\",\r\n    \"    train_loss = 0.0\\n\",\r\n    \"    for images, labels in iter(train_loader):\\n\",\r\n    \"        start_sample = time.process_time()\\n\",\r\n    \"        \\n\",\r\n    \"        images = images.to(device)\\n\",\r\n    \"        labels = labels.to(device)\\n\",\r\n    \"        optimizer.zero_grad()\\n\",\r\n    \"        if TRAIN_MODEL == 'inception_v3':\\n\",\r\n    \"            outputs = model(images)\\n\",\r\n    \"            loss_main = F.mse_loss(outputs.logits, labels, reduction='mean')\\n\",\r\n    \"            loss_aux = F.mse_loss(outputs.aux_logits, labels, reduction='mean')\\n\",\r\n    \"            loss = loss_main + 0.3 * loss_aux\\n\",\r\n    \"        else:\\n\",\r\n    \"            outputs = model(images)\\n\",\r\n    \"            loss = F.mse_loss(outputs, labels, reduction='mean')\\n\",\r\n    \"        train_loss += float(loss)\\n\",\r\n    \"        loss.backward()\\n\",\r\n    \"        optimizer.step()\\n\",\r\n    \"        \\n\",\r\n    \"        end_sample = time.process_time()\\n\",\r\n    \"        lt_sample.append(end_sample - start_sample)\\n\",\r\n    \"        \\n\",\r\n    \"        pbar.update(round(100/max_count, 2))\\n\",\r\n    \"                \\n\",\r\n    \"    train_loss /= len(train_loader)\\n\",\r\n    \"    \\n\",\r\n    \"    model.eval()\\n\",\r\n    \"    test_loss = 0.0\\n\",\r\n    \"    for images, labels in iter(test_loader):\\n\",\r\n    \"        images = images.to(device)\\n\",\r\n    \"        labels = labels.to(device)\\n\",\r\n    \"        outputs = model(images)\\n\",\r\n    \"        loss = F.mse_loss(outputs, labels)\\n\",\r\n    \"        test_loss += float(loss)\\n\",\r\n    \"    test_loss /= len(test_loader)\\n\",\r\n    \"\\n\",\r\n    \"    end_epoch = time.process_time()\\n\",\r\n    \"    lt_epoch.append(end_epoch - start_epoch)\\n\",\r\n    \"   \\n\",\r\n    \"    if test_loss < best_loss:\\n\",\r\n    \"        torch.save(model.state_dict(), BEST_MODEL_PATH)\\n\",\r\n    \"        best_loss = test_loss\\n\",\r\n    \"    \\n\",\r\n    \"    loss_data.append([train_loss, test_loss])\\n\",\r\n    \"    \\n\",\r\n    \"# function plot_loss(loss_data, best_loss, no_epoch, dir_training_records, train_model, train_method) is in jetbot.utils\\n\",\r\n    \"    plot_loss(loss_data=loss_data, best_loss=best_loss, no_epoch=NUM_EPOCHS,\\n\",\r\n    \"              dir_training_records=dir_training_records, # the directory stored training records\\n\",\r\n    \"              train_model=TRAIN_MODEL, train_method=TRAIN_METHOD) # this 2 parameters are for plot title only\\n\",\r\n    \"\\n\",\r\n    \"# function lt_plot(lt_epoch, lt_sample, dir_training_records, train_model, train_method) is in jetbot.utils\\n\",\r\n    \"lt_plot(lt_epoch=lt_epoch, lt_sample=lt_sample, \\n\",\r\n    \"        dir_training_records=dir_training_records, # the directory stored training records\\n\",\r\n    \"        train_model=TRAIN_MODEL, train_method=TRAIN_METHOD) # this 2 parameters are for plot title only\\n\",\r\n    \"\\n\",\r\n    \"print('Training is completed! \\\\n you can close the figures by restart the kernel!')\\n\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Once the model is trained, it will generate ``best_steering_model_xy_<TRAIN_MODEL>.pth`` file which you can use for inferencing in the live demo notebook.\\n\",\r\n    \"\\n\",\r\n    \"If you trained on a different machine other than JetBot, you'll need to upload this to the JetBot to the ``road_following`` example folder.\"\r\n   ]\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.6.8\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 4\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/road_following/train_model_PC.ipynb b/notebooks/road_following/train_model_PC.ipynb
--- a/notebooks/road_following/train_model_PC.ipynb	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/notebooks/road_following/train_model_PC.ipynb	(date 1722796070000)
@@ -15,33 +15,38 @@
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
+   "outputs": [],
    "source": [
     "# execute this script as followings:\n",
-    "# set project interpreter to PC python of version 3.8 pr above, and then\n",
-    "# run jupyter lab in command window \n",
+    "# 1. set project interpreter to PC python of version 3.8 pr above, and then\n",
+    "# 2. run jupyter lab in command window \n",
     "\n",
     "import torch\n",
     "import torch.optim as optim\n",
     "import torch.nn.functional as F\n",
-    "import torchvision\n",
-    "import torchvision.datasets as datasets\n",
-    "import torchvision.models as models\n",
+    "\n",
     "import torchvision.transforms as transforms\n",
     "import glob\n",
     "import PIL.Image\n",
     "import os\n",
     "import numpy as np\n",
-    "import ipywidgets\n",
     "\n",
-    "TRAIN_MODEL = \"resnet101\"  # resnet18, resnet34, resnet50, resnet101, mobilenet_v2, vgg11, mobilenet_v3_large, inception_v3\n",
+    "# The pytorch models for classification:\n",
+    "# resnet18, resnet34, resnet50, resnet101, \n",
+    "# mobilenet_v2, vgg11, mobilenet_v3_large, \n",
+    "# inception_v3, \n",
+    "# efficientnet_b0~b7, \n",
+    "# googlenet, \n",
+    "# densenet121, densenet161, densenet169, densenet201\n",
+    "TRAIN_MODEL = \"resnet34\" \n",
     "\n",
     "# *** refererence : https://pytorch.org/docs/stable/optim.html#algorithms\n",
     "# use the following learning algorithms for evaluation\n",
-    "TRAIN_METHOD = \"Adam\"  # \"Adam\", \"SGD\", \"ASGD\", \"Adadelta\", \"RAdam\"; the parameters lr=0.01, momentum=0.92 is needed for SGD"
-   ],
-   "outputs": [],
-   "execution_count": null
+    "# \"Adam\", \"SGD\", \"ASGD\", \"Adadelta\", \"RAdam\"; the parameters lr=0.01, momentum=0.92 is needed for SGD\n",
+    "TRAIN_METHOD = \"Adam\"  "
+   ]
   },
   {
    "cell_type": "markdown",
@@ -58,18 +63,20 @@
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
+   "outputs": [],
    "source": [
     "# !unzip -q road_following.zip\n",
     "from zipfile import ZipFile\n",
     "dir_depo = 'D:\\\\AI_Lecture_Demos\\\\Data_Repo\\\\Cuterbot_2004_Repo'\n",
     "os.makedirs(dir_depo, exist_ok=True)\n",
     "# dir_depo = os.getcwd()\n",
-    "with ZipFile(os.path.join(dir_depo, 'dataset_xy_0916_1.zip'), 'r') as zObject:\n",
+    "training_datafile = 'dataset_xy_0916_1.zip'  # check the data file is loaded to dir_depo\n",
+    "\n",
+    "with ZipFile(os.path.join(dir_depo, training_datafile), 'r') as zObject:\n",
     "    zObject.extractall(path=dir_depo)\n"
-   ],
-   "outputs": [],
-   "execution_count": null
+   ]
   },
   {
    "cell_type": "markdown",
@@ -94,7 +101,9 @@
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
+   "outputs": [],
    "source": [
     "def get_x(path, width):\n",
     "    \"\"\"Gets the x value from the image filename\"\"\"\n",
@@ -140,9 +149,7 @@
     "        return image, torch.tensor([x, y]).float()\n",
     "    \n",
     "dataset = XYDataset(os.path.join(dir_depo, 'dataset_xy'), random_hflips=False)"
-   ],
-   "outputs": [],
-   "execution_count": null
+   ]
   },
   {
    "cell_type": "markdown",
@@ -154,14 +161,14 @@
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
+   "outputs": [],
    "source": [
     "test_percent = 0.1\n",
     "num_test = int(test_percent * len(dataset))\n",
     "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
-   ],
-   "outputs": [],
-   "execution_count": null
+   ]
   },
   {
    "cell_type": "markdown",
@@ -174,7 +181,9 @@
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
+   "outputs": [],
    "source": [
     "train_loader = torch.utils.data.DataLoader(\n",
     "    train_dataset,\n",
@@ -189,36 +198,41 @@
     "    shuffle=True,\n",
     "    num_workers=0\n",
     ")"
-   ],
-   "outputs": [],
-   "execution_count": null
+   ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Define Neural Network Model \n",
-    "\n",
-    "We use ResNet-18 model available on PyTorch TorchVision. \n",
+    "We use the models (the parameter setting TRAIN_MODEL above) available on PyTorch TorchVision, such as resnet18, resnet34, resnet50, resnet101, mobilenet_v2, vgg11, mobilenet_v3_large, inception_v3, efficientnet_b4, googlenet.\n",
     "\n",
-    "In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n",
+    "Referring the following information pytorch model web site:\n",
+    "1. classifier : https://pytorch.org/vision/0.10/models.html#classification\n",
+    "2. github : https://github.com/pytorch/vision/tree/release/0.11/torchvision/models\n",
+    "More details on ResNet-18 and other variants: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
     "\n",
+    "### Define Neural Network Model Learning Algoritmom\n",
+    "We can use the following learning algorithms (the parameter setting TRAIN_METHOD above) for training a model\n",
+    "\"Adam\", \"SGD\", \"ASGD\", \"Adadelta\", \"RAdam\"; the parameters lr=0.01, momentum=0.92 is needed for SGD\n",
+    "1. reference web site : https://pytorch.org/docs/stable/optim.html#algorithms\n",
     "\n",
-    "More details on ResNet-18 and other variants: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
-    "\n",
-    "More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE "
+    "In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n",
+    "More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE \n",
+    "\n"
    ]
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
+   "outputs": [],
    "source": [
-    "# model = models.resnet18(pretrained=True)\n",
-    "# model = models.resnet101(pretrained=True)\n",
-    "model = getattr(models, TRAIN_MODEL)()"
-   ],
-   "outputs": [],
-   "execution_count": null
+    "\n",
+    "%cd \"../../jetbot/utils\"\n",
+    "from model_selection import load_tune_pth_model\n",
+    "model, model_type = load_tune_pth_model(pth_model_name=TRAIN_MODEL, pretrained=True)\n"
+   ]
   },
   {
    "cell_type": "markdown",
@@ -231,39 +245,21 @@
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
+   "outputs": [],
    "source": [
-    "print(\"torch cuda version : \", torch.version.cuda)\n",
-    "print(\"cuda is available for pytorch: \", torch.cuda.is_available())\n",
-    "\n",
-    "# ----- modify last layer for classification, and the model used in notebook should be modified as well.\n",
-    "\n",
-    "if TRAIN_MODEL == 'mobilenet_v3_large':  # MobileNet\n",
-    "    model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, 2)  # for mobilenet_v3 model. must add the block expansion factor 4\n",
-    "\n",
-    "elif TRAIN_MODEL == 'mobilenet_v2':\n",
-    "    model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # for mobilenet_v2 model. must add the block expansion factor 4\n",
-    "\n",
-    "elif TRAIN_MODEL == 'vgg11': # VGGNet\n",
-    "    model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)  # for VGG model. must add the block expansion factor 4\n",
-    "\n",
-    "elif 'resnet' in TRAIN_MODEL: # ResNet\n",
-    "    model.fc = torch.nn.Linear(model.fc.in_features, 2)  # for resnet model must add the block expansion factor 4\n",
-    "    # model.fc = torch.nn.Linear(512, 2)\n",
-    "\n",
-    "elif TRAIN_MODEL == 'inception_v3':   # Inception_v3\n",
-    "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
-    "    if model.aux_logits:\n",
-    "        model.AuxLogits.fc = torch.nn.Linear(model.AuxLogits.fc.in_features, 2)\n",
-    "\n",
-    "# ** you may use cpu for training\n",
-    "device = torch.device('cuda')\n",
-    "# device = torch.device('cpu')\n",
+    "# ** you may use CPU or GPU for training\n",
+    "processor = 'GPU'\n",
+    "if processor == 'GPU':\n",
+    "    device = torch.device('cuda')\n",
+    "    print(\"torch cuda version : \", torch.version.cuda)\n",
+    "    print(\"cuda is available for pytorch: \", torch.cuda.is_available())    \n",
+    "elif processor == 'CPU':\n",
+    "    device = torch.device('cpu')\n",
     "model = model.float()\n",
     "model = model.to(device, dtype=torch.float)\n"
-   ],
-   "outputs": [],
-   "execution_count": null
+   ]
   },
   {
    "cell_type": "markdown",
@@ -276,29 +272,27 @@
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
+   "outputs": [],
    "source": [
     "%cd \"../../jetbot/utils\"\n",
     "import time\n",
     "from tqdm.notebook import tqdm\n",
-    "from training_profile import *\n",
+    "from training_profile import * \n",
     "\n",
-    "dir_training_records = os.path.join(dir_depo, 'training records', TRAIN_MODEL)\n",
+    "\n",
+    "dir_training_records = os.path.join(dir_depo, processor, 'training records', \"road_following\", TRAIN_MODEL)\n",
     "os.makedirs(dir_training_records, exist_ok=True)\n",
     "\n",
-    "DIR_MODEL_REPO = os.path.join(dir_depo, 'model_repo')\n",
-    "os.makedirs(DIR_MODEL_REPO, exist_ok=True)\n",
+    "DIR_MODEL_REPO = os.path.join(dir_depo, processor, 'model_repo')\n",
+    "os.makedirs(DIR_MODEL_REPO, exist_ok=True)    \n",
     "# BEST_MODEL_PATH = 'best_steering_model_xy.pth'\n",
-    "BEST_MODEL_PATH = os.path.join(DIR_MODEL_REPO, \"best_steering_model_xy_\" + TRAIN_MODEL + \".pth\")\n",
+    "BEST_MODEL_PATH = os.path.join(DIR_MODEL_REPO, \"road_following\", \"best_steering_model_xy_\" + TRAIN_MODEL + \".pth\")\n",
     "\n",
-    "NUM_EPOCHS = 3\n",
+    "NUM_EPOCHS = 70\n",
     "best_loss = 1e9\n",
     "\n",
-    "max_count = NUM_EPOCHS * len(train_loader)\n",
-    "pbar = tqdm(total=100)\n",
-    "pbar.set_description(desc=\"Training Progressing\")\n",
-    "\n",
-    "# optimizer = optim.Adam(model.parameters())\n",
     "optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), weight_decay=0)\n",
     "# optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), lr=0.01, momentum=0.95)\n",
     "\n",
@@ -306,13 +300,26 @@
     "lt_epoch = []  # learning time per epoch\n",
     "lt_sample = []  # learning time per epoch\n",
     "\n",
+    "batch_size = len(train_loader)\n",
+    "pbar_overall_format = \"{desc} {percentage:.2f}% | {bar} | elapsed: {elapsed}; estimated to finish: {remaining}\"\n",
+    "pbar_overall = tqdm(total=100, bar_format = pbar_overall_format)\n",
+    "show_batch_progress = False   # Set True if need to show the batch learning progress in an epoch\n",
+    "show_training_plot = False  # Set True if need to show the converbent profile during training\n",
+    "\n",
+    "best_loss = None\n",
+    "\n",
     "for epoch in range(NUM_EPOCHS):\n",
-    "    start_epoch = time.process_time()\n",
+    "    start_epoch = time.time()\n",
     "    \n",
     "    model.train()\n",
     "    train_loss = 0.0\n",
-    "    for images, labels in iter(train_loader):\n",
-    "        start_sample = time.process_time()\n",
+    "    \n",
+    "    if show_batch_progress:\n",
+    "        pbar_batch = tqdm(train_loader, total = batch_size)\n",
+    "    else:\n",
+    "        pbar_batch = iter(train_loader)\n",
+    "    for index, (images, labels) in enumerate(pbar_batch):\n",
+    "        start_sample = time.time()\n",
     "        \n",
     "        images = images.to(device)\n",
     "        labels = labels.to(device)\n",
@@ -322,6 +329,12 @@
     "            loss_main = F.mse_loss(outputs.logits, labels, reduction='mean')\n",
     "            loss_aux = F.mse_loss(outputs.aux_logits, labels, reduction='mean')\n",
     "            loss = loss_main + 0.3 * loss_aux\n",
+    "        if TRAIN_MODEL == 'googlenet':\n",
+    "            outputs = model(images)\n",
+    "            loss_main = F.mse_loss(outputs.logits, labels, reduction='mean')\n",
+    "            loss_aux1 = F.mse_loss(outputs.aux_logits1, labels, reduction='mean')\n",
+    "            loss_aux2 = F.mse_loss(outputs.aux_logits2, labels, reduction='mean')\n",
+    "            loss = loss_main + 0.3 * loss_aux1 + 0.3 * loss_aux2\n",
     "        else:\n",
     "            outputs = model(images)\n",
     "            loss = F.mse_loss(outputs, labels, reduction='mean')\n",
@@ -329,11 +342,17 @@
     "        loss.backward()\n",
     "        optimizer.step()\n",
     "        \n",
-    "        end_sample = time.process_time()\n",
+    "        end_sample = time.time()\n",
     "        lt_sample.append(end_sample - start_sample)\n",
     "        \n",
-    "        pbar.update(round(100/max_count, 2))\n",
-    "                \n",
+    "        pbar_overall.update(round(100/(NUM_EPOCHS*batch_size), 2))\n",
+    "        pbar_overall.set_description(desc = f'Overall progress - Epoch [{epoch+1}/{NUM_EPOCHS}]')\n",
+    "        pbar_overall.set_postfix(best_loss = best_loss, train_loss = train_loss/(index+1))\n",
+    "               \n",
+    "        if show_batch_progress:\n",
+    "            pbar_batch.set_description(desc = f'Progress in the epoch {epoch+1} ')\n",
+    "            pbar_batch.set_postfix(mean_batch_loss = train_loss/(index+1))\n",
+    "    \n",
     "    train_loss /= len(train_loader)\n",
     "    \n",
     "    model.eval()\n",
@@ -346,29 +365,60 @@
     "        test_loss += float(loss)\n",
     "    test_loss /= len(test_loader)\n",
     "\n",
-    "    end_epoch = time.process_time()\n",
+    "    end_epoch = time.time()\n",
     "    lt_epoch.append(end_epoch - start_epoch)\n",
-    "   \n",
-    "    if test_loss < best_loss:\n",
+    "      \n",
+    "    if best_loss == None or test_loss < best_loss :\n",
     "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
     "        best_loss = test_loss\n",
     "    \n",
     "    loss_data.append([train_loss, test_loss])\n",
     "    \n",
     "# function plot_loss(loss_data, best_loss, no_epoch, dir_training_records, train_model, train_method) is in jetbot.utils\n",
+    "    if epoch == NUM_EPOCHS:\n",
+    "        show_training_plot=True    \n",
     "    plot_loss(loss_data=loss_data, best_loss=best_loss, no_epoch=NUM_EPOCHS,\n",
     "              dir_training_records=dir_training_records, # the directory stored training records\n",
-    "              train_model=TRAIN_MODEL, train_method=TRAIN_METHOD) # this 2 parameters are for plot title only\n",
+    "              train_model=TRAIN_MODEL, train_method=TRAIN_METHOD, processor=processor, # this 2 parameters are for plot title only\n",
+    "              show_training_plot=show_training_plot)\n",
     "\n",
+    "overall_time = pbar_overall.format_dict[\"elapsed\"]\n",
     "# function lt_plot(lt_epoch, lt_sample, dir_training_records, train_model, train_method) is in jetbot.utils\n",
-    "lt_plot(lt_epoch=lt_epoch, lt_sample=lt_sample, \n",
+    "lt_plot(lt_epoch=lt_epoch, lt_sample=lt_sample, overall_time=overall_time,\n",
     "        dir_training_records=dir_training_records, # the directory stored training records\n",
-    "        train_model=TRAIN_MODEL, train_method=TRAIN_METHOD) # this 2 parameters are for plot title only\n",
+    "        train_model=TRAIN_MODEL, train_method=TRAIN_METHOD, processor=processor) # this 2 parameters are for plot title only"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [],
+   "source": [
+    "import pandas as pd\n",
     "\n",
-    "print('Training is completed! \\n you can close the figures by restart the kernel!')\n"
-   ],
+    "df_file = os.path.join(DIR_MODEL_REPO, 'torch_model_tbl.csv')\n",
+    "model_path = \"./model_repo/road_following/best_steering_model_xy_\" + TRAIN_MODEL + \".pth\"\n",
+    "\n",
+    "if os.path.isfile(df_file):\n",
+    "    df = pd.read_csv(df_file, header=None)\n",
+    "else:\n",
+    "    df = pd.DataFrame()\n",
+    "df = df.append([[\"classifier\", model_type, model_path]], ignore_index = False)\n",
+    "df = df.drop_duplicates()\n",
+    "df.to_csv(df_file, header=False, index=False)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
    "outputs": [],
-   "execution_count": null
+   "source": [
+    "print('Training is completed! \\n you can close the figures by restart the kernel!')"
+   ]
   },
   {
    "cell_type": "markdown",
@@ -396,7 +446,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.6.8"
+   "version": "3.6.9"
   }
  },
  "nbformat": 4,
Index: notebooks/road_following/live_demo_trt.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# Road Following - Live demo (TensorRT)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"In this notebook, we will use model we trained to move JetBot smoothly on track. \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# TensorRT\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 1,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"import torch\\n\",\r\n    \"device = torch.device('cuda')\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Load the TRT optimized model by executing the cell below\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 2,\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"ename\": \"FileNotFoundError\",\r\n     \"evalue\": \"[Errno 2] No such file or directory: 'best_steering_model_xy_trt_resnet18.pth'\",\r\n     \"output_type\": \"error\",\r\n     \"traceback\": [\r\n      \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\r\n      \"\\u001b[0;31mFileNotFoundError\\u001b[0m                         Traceback (most recent call last)\",\r\n      \"Cell \\u001b[0;32mIn[2], line 8\\u001b[0m\\n\\u001b[1;32m      4\\u001b[0m type_model \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124mresnet\\u001b[39m\\u001b[38;5;124m'\\u001b[39m   \\u001b[38;5;66;03m# resnet, mobilenet\\u001b[39;00m\\n\\u001b[1;32m      6\\u001b[0m model_trt \\u001b[38;5;241m=\\u001b[39m TRTModule()\\n\\u001b[0;32m----> 8\\u001b[0m model_trt\\u001b[38;5;241m.\\u001b[39mload_state_dict(\\u001b[43mtorch\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mload\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;124;43m'\\u001b[39;49m\\u001b[38;5;124;43mbest_steering_model_xy_trt_resnet18.pth\\u001b[39;49m\\u001b[38;5;124;43m'\\u001b[39;49m\\u001b[43m)\\u001b[49m)\\n\\u001b[1;32m      9\\u001b[0m \\u001b[38;5;66;03m# model_trt.load_state_dict(torch.load('best_steering_model_xy_trt_resnet34.pth'))\\u001b[39;00m\\n\\u001b[1;32m     10\\u001b[0m \\u001b[38;5;66;03m# model_trt.load_state_dict(torch.load('best_steering_model_xy_trt_mobilenet_v3_large.pth'))\\u001b[39;00m\\n\\u001b[1;32m     11\\u001b[0m \\u001b[38;5;66;03m# model_trt.load_state_dict(torch.load('best_steering_model_xy_trt_inception_v3.pth'))\\u001b[39;00m\\n\",\r\n      \"File \\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:771\\u001b[0m, in \\u001b[0;36mload\\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\\u001b[0m\\n\\u001b[1;32m    768\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124mencoding\\u001b[39m\\u001b[38;5;124m'\\u001b[39m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;129;01min\\u001b[39;00m pickle_load_args\\u001b[38;5;241m.\\u001b[39mkeys():\\n\\u001b[1;32m    769\\u001b[0m     pickle_load_args[\\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124mencoding\\u001b[39m\\u001b[38;5;124m'\\u001b[39m] \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124mutf-8\\u001b[39m\\u001b[38;5;124m'\\u001b[39m\\n\\u001b[0;32m--> 771\\u001b[0m \\u001b[38;5;28;01mwith\\u001b[39;00m \\u001b[43m_open_file_like\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mf\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;124;43m'\\u001b[39;49m\\u001b[38;5;124;43mrb\\u001b[39;49m\\u001b[38;5;124;43m'\\u001b[39;49m\\u001b[43m)\\u001b[49m \\u001b[38;5;28;01mas\\u001b[39;00m opened_file:\\n\\u001b[1;32m    772\\u001b[0m     \\u001b[38;5;28;01mif\\u001b[39;00m _is_zipfile(opened_file):\\n\\u001b[1;32m    773\\u001b[0m         \\u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\\u001b[39;00m\\n\\u001b[1;32m    774\\u001b[0m         \\u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\\u001b[39;00m\\n\\u001b[1;32m    775\\u001b[0m         \\u001b[38;5;66;03m# reset back to the original position.\\u001b[39;00m\\n\\u001b[1;32m    776\\u001b[0m         orig_position \\u001b[38;5;241m=\\u001b[39m opened_file\\u001b[38;5;241m.\\u001b[39mtell()\\n\",\r\n      \"File \\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:270\\u001b[0m, in \\u001b[0;36m_open_file_like\\u001b[0;34m(name_or_buffer, mode)\\u001b[0m\\n\\u001b[1;32m    268\\u001b[0m \\u001b[38;5;28;01mdef\\u001b[39;00m \\u001b[38;5;21m_open_file_like\\u001b[39m(name_or_buffer, mode):\\n\\u001b[1;32m    269\\u001b[0m     \\u001b[38;5;28;01mif\\u001b[39;00m _is_path(name_or_buffer):\\n\\u001b[0;32m--> 270\\u001b[0m         \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[43m_open_file\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mname_or_buffer\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[43mmode\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m    271\\u001b[0m     \\u001b[38;5;28;01melse\\u001b[39;00m:\\n\\u001b[1;32m    272\\u001b[0m         \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124mw\\u001b[39m\\u001b[38;5;124m'\\u001b[39m \\u001b[38;5;129;01min\\u001b[39;00m mode:\\n\",\r\n      \"File \\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:251\\u001b[0m, in \\u001b[0;36m_open_file.__init__\\u001b[0;34m(self, name, mode)\\u001b[0m\\n\\u001b[1;32m    250\\u001b[0m \\u001b[38;5;28;01mdef\\u001b[39;00m \\u001b[38;5;21m__init__\\u001b[39m(\\u001b[38;5;28mself\\u001b[39m, name, mode):\\n\\u001b[0;32m--> 251\\u001b[0m     \\u001b[38;5;28msuper\\u001b[39m(_open_file, \\u001b[38;5;28mself\\u001b[39m)\\u001b[38;5;241m.\\u001b[39m\\u001b[38;5;21m__init__\\u001b[39m(\\u001b[38;5;28;43mopen\\u001b[39;49m\\u001b[43m(\\u001b[49m\\u001b[43mname\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[43mmode\\u001b[49m\\u001b[43m)\\u001b[49m)\\n\",\r\n      \"\\u001b[0;31mFileNotFoundError\\u001b[0m: [Errno 2] No such file or directory: 'best_steering_model_xy_trt_resnet18.pth'\"\r\n     ]\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"# import torch\\n\",\r\n    \"from torch2trt import TRTModule\\n\",\r\n    \"\\n\",\r\n    \"type_model = 'resnet'   # resnet, mobilenet\\n\",\r\n    \"\\n\",\r\n    \"model_trt = TRTModule()\\n\",\r\n    \"\\n\",\r\n    \"model_trt.load_state_dict(torch.load('best_steering_model_xy_trt_resnet18.pth'))\\n\",\r\n    \"# model_trt.load_state_dict(torch.load('best_steering_model_xy_trt_resnet34.pth'))\\n\",\r\n    \"# model_trt.load_state_dict(torch.load('best_steering_model_xy_trt_mobilenet_v3_large.pth'))\\n\",\r\n    \"# model_trt.load_state_dict(torch.load('best_steering_model_xy_trt_inception_v3.pth'))\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Creating the Pre-Processing Function\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"We have now loaded our model, but there's a slight issue. The format that we trained our model doesn't exactly match the format of the camera. To do that, we need to do some preprocessing. This involves the following steps:\\n\",\r\n    \"\\n\",\r\n    \"1. Convert from HWC layout to CHW layout\\n\",\r\n    \"2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\\n\",\r\n    \"3. Transfer the data from CPU memory to GPU memory\\n\",\r\n    \"4. Add a batch dimension\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"import torchvision.transforms as transforms\\n\",\r\n    \"import torch.nn.functional as F\\n\",\r\n    \"import cv2\\n\",\r\n    \"import PIL.Image\\n\",\r\n    \"import numpy as np\\n\",\r\n    \"\\n\",\r\n    \"mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\\n\",\r\n    \"std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\\n\",\r\n    \"\\n\",\r\n    \"def preprocess(image):\\n\",\r\n    \"    image = PIL.Image.fromarray(image)\\n\",\r\n    \"    if type_model == 'inception':\\n\",\r\n    \"        image = image.resize((299, 299))\\n\",\r\n    \"    else:\\n\",\r\n    \"        image = image.resize((224, 224))  \\n\",\r\n    \"    image = transforms.functional.to_tensor(image).to(device).half()\\n\",\r\n    \"    image.sub_(mean[:, None, None]).div_(std[:, None, None])\\n\",\r\n    \"    return image[None, ...]\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Awesome! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\\n\",\r\n    \"\\n\",\r\n    \"Now, let's start and display our camera. You should be pretty familiar with this by now. \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from IPython.display import display\\n\",\r\n    \"import ipywidgets\\n\",\r\n    \"import traitlets\\n\",\r\n    \"from jetbot import Camera, bgr8_to_jpeg\\n\",\r\n    \"\\n\",\r\n    \"camera = Camera()\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"image_widget = ipywidgets.Image(width=300, height=500)\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\\n\",\r\n    \"\\n\",\r\n    \"display(image_widget)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"We'll also create our robot instance which we'll need to drive the motors.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from jetbot import Robot\\n\",\r\n    \"\\n\",\r\n    \"robot = Robot()\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Now, we will define sliders to control JetBot\\n\",\r\n    \"> Note: We have initialize the slider values for best known configurations, however these might not work for your dataset, therefore please increase or decrease the sliders according to your setup and environment\\n\",\r\n    \"\\n\",\r\n    \"1. Speed Control (speed_gain_slider): To start your JetBot increase ``speed_gain_slider`` \\n\",\r\n    \"2. Steering Gain Control (steering_gain_slider): If you see JetBot is wobbling, you need to reduce ``steering_gain_slider`` till it is smooth\\n\",\r\n    \"3. Steering Bias control (steering_bias_slider): If you see JetBot is biased towards extreme right or extreme left side of the track, you should control this slider till JetBot start following line or track in the center.  This accounts for motor biases as well as camera offsets\\n\",\r\n    \"\\n\",\r\n    \"> Note: You should play around above mentioned sliders with lower speed to get smooth JetBot road following behavior.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1, step=0.001, value=0.2, description='speed gain', readout_format='.3f')\\n\",\r\n    \"steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.0001, value=0.08, description='steering gain', readout_format='.3f')\\n\",\r\n    \"steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=2, step=0.001, value=0.82, description='steering kd', readout_format='.3f')\\n\",\r\n    \"steering_bias_slider = ipywidgets.FloatSlider(min=-0.1, max=0.1, step=0.001, value=-0.01, description='steering bias', readout_format='.3f')\\n\",\r\n    \"\\n\",\r\n    \"display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Next, let's display some sliders that will let us see what JetBot is thinking.  The x and y sliders will display the predicted x, y values.\\n\",\r\n    \"\\n\",\r\n    \"The steering slider will display our estimated steering value.  Please remember, this value isn't the actual angle of the target, but simply a value that is\\n\",\r\n    \"nearly proportional.  When the actual angle is ``0``, this will be zero, and it will increase / decrease with the actual angle.  \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\\n\",\r\n    \"y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\\n\",\r\n    \"steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\\n\",\r\n    \"speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\\n\",\r\n    \"\\n\",\r\n    \"display(ipywidgets.HBox([y_slider, speed_slider]))\\n\",\r\n    \"display(x_slider, steering_slider)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Next, we'll create a function that will get called whenever the camera's value changes. This function will do the following steps\\n\",\r\n    \"\\n\",\r\n    \"1. Pre-process the camera image\\n\",\r\n    \"2. Execute the neural network\\n\",\r\n    \"3. Compute the approximate steering value\\n\",\r\n    \"4. Control the motors using proportional / derivative control (PD)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"import time\\n\",\r\n    \"angle = 0.0\\n\",\r\n    \"angle_last = 0.0\\n\",\r\n    \"et=[]\\n\",\r\n    \"\\n\",\r\n    \"def execute(change):\\n\",\r\n    \"    start_time = time.process_time()\\n\",\r\n    \"    global angle, angle_last\\n\",\r\n    \"    image = change['new']\\n\",\r\n    \"    xy = model_trt(preprocess(image)).detach().float().cpu().numpy().flatten()\\n\",\r\n    \"    x = xy[0]\\n\",\r\n    \"    y = (0.5 - xy[1]) / 2.0\\n\",\r\n    \"    \\n\",\r\n    \"    x_slider.value = x\\n\",\r\n    \"    y_slider.value = y\\n\",\r\n    \"    \\n\",\r\n    \"    speed_slider.value = speed_gain_slider.value\\n\",\r\n    \"    \\n\",\r\n    \"    angle = np.arctan2(x, y)\\n\",\r\n    \"    pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\\n\",\r\n    \"    angle_last = angle\\n\",\r\n    \"    \\n\",\r\n    \"    steering_slider.value = pid + steering_bias_slider.value\\n\",\r\n    \"    \\n\",\r\n    \"    robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0)\\n\",\r\n    \"    robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0)\\n\",\r\n    \"    \\n\",\r\n    \"    end_time = time.process_time()\\n\",\r\n    \"    et.append(end_time - start_time)\\n\",\r\n    \"    # et.append(end_time - start_time + camera.cap_time)\\n\",\r\n    \"    \\n\",\r\n    \"execute({'new': camera.value})\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing.\\n\",\r\n    \"\\n\",\r\n    \"We accomplish that with the observe function.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \">WARNING: This code will move the robot!! Please make sure your robot has clearance and it is on Lego or Track you have collected data on. The road follower should work, but the neural network is only as good as the data it's trained on!\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"camera.observe(execute, names='value')\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame. \\n\",\r\n    \"\\n\",\r\n    \"You can now place JetBot on  Lego or Track you have collected data on and see whether it can follow track.\\n\",\r\n    \"\\n\",\r\n    \"If you want to stop this behavior, you can unattach this callback by executing the code below.\\n\",\r\n    \"\\n\",\r\n    \"And, let's close the camera conneciton properly so that we can use the camera in other notebooks.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"import time\\n\",\r\n    \"import numpy as np, matplotlib.pyplot as plt\\n\",\r\n    \"import os\\n\",\r\n    \"\\n\",\r\n    \"out = ipywidgets.Output()\\n\",\r\n    \"button_stop = ipywidgets.Button(description='Stop', tooltip='Click to stop running',icon='fa-circle-stop')\\n\",\r\n    \"display(button_stop, out)\\n\",\r\n    \"\\n\",\r\n    \"def stop_run(b):\\n\",\r\n    \"    camera.unobserve(execute, names='value')\\n\",\r\n    \"    time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\\n\",\r\n    \"    robot.stop()\\n\",\r\n    \"    camera.stop()\\n\",\r\n    \"    \\n\",\r\n    \"    execute_time = np.array(et[1:])\\n\",\r\n    \"    mean_execute_time = np.mean(execute_time)\\n\",\r\n    \"    max_execute_time = np.amax(execute_time)\\n\",\r\n    \"    min_execute_time = np.amin(execute_time)\\n\",\r\n    \"    \\n\",\r\n    \"    with out:\\n\",\r\n    \"        os.environ[\\\"DISPLAY\\\"] = \\\":10.0\\\"        \\n\",\r\n    \"        # print(\\\"The execution time of road cruise model --- \\\\nMean excution time : %f \\\\nMax excution time : %f \\\\nMin excution time : %f \\\" \\\\\\n\",\r\n    \"        #       % (mean_execute_time, max_execute_time, min_execute_time))\\n\",\r\n    \"        fig, ax = plt.subplots()\\n\",\r\n    \"        ax.hist(execute_time, bins=(0.005 * np.array(list(range(101)))).tolist())\\n\",\r\n    \"        ax.set_xlabel('processing time, sec.')\\n\",\r\n    \"        ax.set_ylabel('No. of detection processes')\\n\",\r\n    \"        ax.set_title('Histogram of detection processing time of road cruise mode: ')\\n\",\r\n    \"        props = dict(boxstyle='round', facecolor='wheat')\\n\",\r\n    \"        text_str = \\\" mean execution time : %.4f sec. \\\\n max execution time : %.4f sec. \\\\n min execution time : %.4f sec. \\\" % (mean_execute_time, max_execute_time, min_execute_time)\\n\",\r\n    \"        ax.text(0.5, 0.85, text_str, transform=ax.transAxes, fontsize=12, verticalalignment='top', bbox=props)\\n\",\r\n    \"        \\n\",\r\n    \"        plt.show()\\n\",\r\n    \"    \\n\",\r\n    \"button_stop.on_click(stop_run)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Again, let's close the camera conneciton properly so that we can use the camera in other notebooks.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Conclusion\\n\",\r\n    \"That's it for this live demo! Hopefully you had some fun seeing your JetBot moving smoothly on track following the road!!!\\n\",\r\n    \"\\n\",\r\n    \"If your JetBot wasn't following road very well, try to spot where it fails. The beauty is that we can collect more data for these failure scenarios and the JetBot should get even better :)\"\r\n   ]\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3 (ipykernel)\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.8.10\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 4\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/road_following/live_demo_trt.ipynb b/notebooks/road_following/live_demo_trt.ipynb
--- a/notebooks/road_following/live_demo_trt.ipynb	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/notebooks/road_following/live_demo_trt.ipynb	(date 1722754934000)
@@ -23,7 +23,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -40,24 +40,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "ename": "FileNotFoundError",
-     "evalue": "[Errno 2] No such file or directory: 'best_steering_model_xy_trt_resnet18.pth'",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
-      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m type_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# resnet, mobilenet\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_trt \u001b[38;5;241m=\u001b[39m TRTModule()\n\u001b[0;32m----> 8\u001b[0m model_trt\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_steering_model_xy_trt_resnet18.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# model_trt.load_state_dict(torch.load('best_steering_model_xy_trt_resnet34.pth'))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# model_trt.load_state_dict(torch.load('best_steering_model_xy_trt_mobilenet_v3_large.pth'))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model_trt.load_state_dict(torch.load('best_steering_model_xy_trt_inception_v3.pth'))\u001b[39;00m\n",
-      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
-      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
-      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
-      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_steering_model_xy_trt_resnet18.pth'"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "# import torch\n",
     "from torch2trt import TRTModule\n",
@@ -377,7 +362,7 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3 (ipykernel)",
+   "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
@@ -391,7 +376,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.10"
+   "version": "3.6.9"
   }
  },
  "nbformat": 4,
Index: notebooks/road_following/live_demo_light_trt.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# Road Following - Live demo (with TRT)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"In this notebook, we will use model we trained to move jetBot smoothly on track. \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Load Trained Model\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"We will assume that you have already downloaded ``best_steering_model_xy.pth`` to work station as instructed in \\\"train_model.ipynb\\\" notebook. Now, you should upload model file to JetBot in to this notebook's directory. Once that's finished there should be a file named ``best_steering_model_xy_model.pth`` in this notebook's directory.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"> Please make sure the file has uploaded fully before calling the next cell\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Execute the code below to initialize the PyTorch model. This should look very familiar from the training notebook.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# %matplotlib notebook\\n\",\r\n    \"\\n\",\r\n    \"import ipywidgets.widgets as widgets\\n\",\r\n    \"from ipywidgets.widgets import Box, HBox, VBox, Layout, Label\\n\",\r\n    \"import traitlets\\n\",\r\n    \"\\n\",\r\n    \"from jetbot.utils import model_selection\\n\",\r\n    \"from jetbot import RoadCruiserTRT\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"RC = RoadCruiserTRT(init_sensor_rc=True)\\n\",\r\n    \"\\n\",\r\n    \"trt_ms = model_selection(core_library = \\\"TensorRT\\\")\\n\",\r\n    \"trt_ms.model_function = \\\"classifier\\\"\\n\",\r\n    \"\\n\",\r\n    \"model_type_widget = widgets.Select(options=trt_ms.model_type_list, value=trt_ms.model_type_list[0],\\n\",\r\n    \"                                      description='Model Type:')\\n\",\r\n    \"traitlets.dlink((trt_ms, 'model_type_list'), (model_type_widget, 'options'))\\n\",\r\n    \"traitlets.dlink((model_type_widget, 'value'), (trt_ms, 'model_type'))\\n\",\r\n    \"traitlets.dlink((trt_ms, 'model_type'), (RC, 'type_cruiser_model'))\\n\",\r\n    \"\\n\",\r\n    \"model_path_widget = widgets.Select(options=trt_ms.model_path_list, description='Model Path:',\\n\",\r\n    \"                                      layout=Layout(width='50%'))\\n\",\r\n    \"traitlets.dlink((trt_ms, 'model_path_list'), (model_path_widget, 'options'))\\n\",\r\n    \"traitlets.dlink((model_path_widget, 'value'), (trt_ms, 'model_path'))\\n\",\r\n    \"traitlets.dlink((trt_ms, 'model_path'), (RC, 'cruiser_model'))\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Next, load the trained weights from the ``best_steering_model_xy_model.pth`` file that you uploaded.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"selected_button_layout = widgets.Layout(width='200px', height='40px', align_self='center')\\n\",\r\n    \"road_follower = widgets.Button(description='Select Road Follower', tooltip='Click to Select road_follower',\\n\",\r\n    \"                               layout=selected_button_layout)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"### Creating the Pre-Processing Function\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"We have now loaded our model, but there's a slight issue. The format that we trained our model doesn't exactly match the format of the camera. To do that, we need to do some preprocessing. This involves the following steps:\\n\",\r\n    \"\\n\",\r\n    \"1. Convert from HWC layout to CHW layout\\n\",\r\n    \"2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\\n\",\r\n    \"3. Transfer the data from CPU memory to GPU memory\\n\",\r\n    \"4. Add a batch dimension\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Awesome! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\\n\",\r\n    \"\\n\",\r\n    \"Now, let's start and display our camera. You should be pretty familiar with this by now. \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from IPython.display import display\\n\",\r\n    \"import ipywidgets\\n\",\r\n    \"from jetbot import bgr8_to_jpeg\\n\",\r\n    \"\\n\",\r\n    \"image_widget = ipywidgets.Image(width=300, height=300)\\n\",\r\n    \"# fps_widget = ipywidgets.FloatText(description='Capture rate')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((RC.capturer, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\\n\",\r\n    \"# traitlets.dlink((RC.camera, 'cap_time'), (fps_widget, 'value'))\\n\",\r\n    \"                \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Now, we will define sliders to control JetBot\\n\",\r\n    \"> Note: We have initialize the slider values for best known configurations, however these might not work for your dataset, therefore please increase or decrease the sliders according to your setup and environment\\n\",\r\n    \"\\n\",\r\n    \"1. Speed Control (speed_gain_slider): To start your JetBot increase ``speed_gain_slider`` \\n\",\r\n    \"2. Steering Gain Control (steering_gain_slider): If you see JetBot is wobbling, you need to reduce ``steering_gain_slider`` till it is smooth\\n\",\r\n    \"3. Steering Bias control (steering_bias_slider): If you see JetBot is biased towards extreme right or extreme left side of the track, you should control this slider till JetBot start following line or track in the center.  This accounts for motor biases as well as camera offsets\\n\",\r\n    \"\\n\",\r\n    \"> Note: You should play around above mentioned sliders with lower speed to get smooth JetBot road following behavior.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"speed_gain_slider = ipywidgets.FloatSlider(min=0, max=1, step=0.001, value=0.2, description='speed gain', readout_format='.3f')\\n\",\r\n    \"steering_gain_slider = ipywidgets.FloatSlider(min=0, max=0.5, step=0.001, value=0.08, description='steering gain', readout_format='.3f')\\n\",\r\n    \"steering_dgain_slider = ipywidgets.FloatSlider(min=0, max=2.0, step=0.001, value=0.82, description='steering kd', readout_format='.3f')\\n\",\r\n    \"steering_bias_slider = ipywidgets.FloatSlider(min=-0.1, max=0.1, step=0.001, value=-0.01, description='steering bias', readout_format='.3f')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((speed_gain_slider, 'value'), (RC, 'speed_gain_rc'))\\n\",\r\n    \"traitlets.dlink((steering_gain_slider, 'value'), (RC, 'steering_gain_rc'))\\n\",\r\n    \"traitlets.dlink((steering_dgain_slider, 'value'), (RC, 'steering_dgain_rc'))\\n\",\r\n    \"traitlets.dlink((steering_bias_slider, 'value'), (RC, 'steering_bias_rc'))\\n\",\r\n    \"\\n\",\r\n    \"# VBox_image = ipywidgets.VBox([image_widget, fps_widget], layout=ipywidgets.Layout(align_self='center'))\\n\",\r\n    \"VBox_image = ipywidgets.VBox([image_widget], layout=ipywidgets.Layout(align_self='center'))\\n\",\r\n    \"VBox_control = ipywidgets.VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider], layout=ipywidgets.Layout(align_self='center'))\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Next, let's display some sliders that will let us see what JetBot is thinking.  The x and y sliders will display the predicted x, y values.\\n\",\r\n    \"\\n\",\r\n    \"The steering slider will display our estimated steering value.  Please remember, this value isn't the actual angle of the target, but simply a value that is\\n\",\r\n    \"nearly proportional.  When the actual angle is ``0``, this will be zero, and it will increase / decrease with the actual angle.  \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\\n\",\r\n    \"y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\\n\",\r\n    \"steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\\n\",\r\n    \"speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((RC, 'x_slider'), (x_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((RC, 'y_slider'), (y_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((RC, 'steering_rc'), (steering_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((RC, 'speed_rc'), (speed_slider, 'value'))\\n\",\r\n    \"\\n\",\r\n    \"Box_y_state = ipywidgets.HBox([y_slider, speed_slider])\\n\",\r\n    \"Box_x_state = ipywidgets.VBox([x_slider, steering_slider])\\n\",\r\n    \"Box_state = ipywidgets.VBox([Box_y_state, Box_x_state])\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Next, we'll create a function that will get called whenever the camera's value changes. This function will do the following steps\\n\",\r\n    \"\\n\",\r\n    \"1. Pre-process the camera image\\n\",\r\n    \"2. Execute the neural network\\n\",\r\n    \"3. Compute the approximate steering value\\n\",\r\n    \"4. Control the motors using proportional / derivative control (PD)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing.\\n\",\r\n    \"\\n\",\r\n    \"We accomplish that with the observe function.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \">WARNING: This code will move the robot!! Please make sure your robot has clearance and it is on Lego or Track you have collected data on. The road follower should work, but the neural network is only as good as the data it's trained on!\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame. \\n\",\r\n    \"\\n\",\r\n    \"You can now place JetBot on  Lego or Track you have collected data on and see whether it can follow track.\\n\",\r\n    \"\\n\",\r\n    \"If you want to stop this behavior, you can unattach this callback by executing the code below.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"display(HBox([model_type_widget, model_path_widget]))\\n\",\r\n    \"\\n\",\r\n    \"display(ipywidgets.HBox([Box_state, VBox_image, VBox_control]))\\n\",\r\n    \"\\n\",\r\n    \"button_start = ipywidgets.Button(description='Start', tooltip='Click to start running', icon='solid play')\\n\",\r\n    \"button_start.style.button_color='lightBlue'\\n\",\r\n    \"button_start.on_click(RC.start_rc)\\n\",\r\n    \"\\n\",\r\n    \"button_stop = ipywidgets.Button(description='Stop', tooltip='Click to stop running', icon=\\\"solid stop\\\")\\n\",\r\n    \"button_stop.style.button_color='Red'\\n\",\r\n    \"button_stop.on_click(RC.stop_rc)\\n\",\r\n    \"display(HBox([button_start, button_stop]))\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Again, let's close the camera conneciton properly so that we can use the camera in other notebooks.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"### Conclusion\\n\",\r\n    \"That's it for this live demo! Hopefully you had some fun seeing your JetBot moving smoothly on track following the road!!!\\n\",\r\n    \"\\n\",\r\n    \"If your JetBot wasn't following road very well, try to spot where it fails. The beauty is that we can collect more data for these failure scenarios and the JetBot should get even better :)\"\r\n   ]\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3 (ipykernel)\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.8.10\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 4\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/road_following/live_demo_light_trt.ipynb b/notebooks/road_following/live_demo_light_trt.ipynb
--- a/notebooks/road_following/live_demo_light_trt.ipynb	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/notebooks/road_following/live_demo_light_trt.ipynb	(date 1722846609819)
@@ -76,7 +76,7 @@
     "traitlets.dlink((trt_ms, 'model_type'), (RC, 'type_cruiser_model'))\n",
     "\n",
     "model_path_widget = widgets.Select(options=trt_ms.model_path_list, description='Model Path:',\n",
-    "                                      layout=Layout(width='50%'))\n",
+    "                                      layout=Layout(width='60%'))\n",
     "traitlets.dlink((trt_ms, 'model_path_list'), (model_path_widget, 'options'))\n",
     "traitlets.dlink((model_path_widget, 'value'), (trt_ms, 'model_path'))\n",
     "traitlets.dlink((trt_ms, 'model_path'), (RC, 'cruiser_model'))\n"
@@ -272,11 +272,11 @@
     "\n",
     "display(ipywidgets.HBox([Box_state, VBox_image, VBox_control]))\n",
     "\n",
-    "button_start = ipywidgets.Button(description='Start', tooltip='Click to start running', icon='solid play')\n",
+    "button_start = ipywidgets.Button(description='Start', tooltip='Click to start running', icon='play')\n",
     "button_start.style.button_color='lightBlue'\n",
     "button_start.on_click(RC.start_rc)\n",
     "\n",
-    "button_stop = ipywidgets.Button(description='Stop', tooltip='Click to stop running', icon=\"solid stop\")\n",
+    "button_stop = ipywidgets.Button(description='Stop', tooltip='Click to stop running', icon=\"stop\")\n",
     "button_stop.style.button_color='Red'\n",
     "button_stop.on_click(RC.stop_rc)\n",
     "display(HBox([button_start, button_stop]))"
@@ -304,7 +304,7 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3 (ipykernel)",
+   "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
@@ -318,7 +318,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.10"
+   "version": "3.6.9"
   }
  },
  "nbformat": 4,
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"AutoImportSettings\">\r\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\r\n  </component>\r\n  <component name=\"ChangeListManager\">\r\n    <list default=\"true\" id=\"26d62f21-4216-452a-9253-5dc75bd4e437\" name=\"Changes\" comment=\"update\">\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/workspace.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\r\n    </list>\r\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\r\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\r\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\r\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\r\n  </component>\r\n  <component name=\"FileTemplateManagerImpl\">\r\n    <option name=\"RECENT_TEMPLATES\">\r\n      <list>\r\n        <option value=\"Python Script\" />\r\n      </list>\r\n    </option>\r\n  </component>\r\n  <component name=\"Git.Settings\">\r\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\r\n      <map>\r\n        <entry key=\"$PROJECT_DIR$\" value=\"v1.0\" />\r\n      </map>\r\n    </option>\r\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\r\n  </component>\r\n  <component name=\"GitHubPullRequestSearchHistory\">{\r\n  &quot;lastFilter&quot;: {\r\n    &quot;state&quot;: &quot;OPEN&quot;,\r\n    &quot;assignee&quot;: &quot;cuter9&quot;\r\n  }\r\n}</component>\r\n  <component name=\"GithubPullRequestsUISettings\">{\r\n  &quot;selectedUrlAndAccountId&quot;: {\r\n    &quot;url&quot;: &quot;https://github.com/cuter9/Cuterbot_2004.git&quot;,\r\n    &quot;accountId&quot;: &quot;56351cb7-3005-4fad-b1f1-687ebf3ba033&quot;\r\n  }\r\n}</component>\r\n  <component name=\"MarkdownSettingsMigration\">\r\n    <option name=\"stateVersion\" value=\"1\" />\r\n  </component>\r\n  <component name=\"ProjectColorInfo\">{\r\n  &quot;associatedIndex&quot;: 8\r\n}</component>\r\n  <component name=\"ProjectId\" id=\"2fRROPpql6uBOIZXFgmlEwTa5pH\" />\r\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\r\n  <component name=\"ProjectViewState\">\r\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\r\n    <option name=\"showLibraryContents\" value=\"true\" />\r\n  </component>\r\n  <component name=\"PropertiesComponent\">{\r\n  &quot;keyToString&quot;: {\r\n    &quot;Python tests.Python tests in test_inheritance.py.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.jetbot_states.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.model_selection.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.stats.executor&quot;: &quot;Debug&quot;,\r\n    &quot;Python.test_fm_function.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.test_fpn.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.test_inheritance.executor&quot;: &quot;Debug&quot;,\r\n    &quot;Python.test_yolo_v7.executor&quot;: &quot;Run&quot;,\r\n    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,\r\n    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,\r\n    &quot;SHARE_PROJECT_CONFIGURATION_FILES&quot;: &quot;true&quot;,\r\n    &quot;WebServerToolWindowPanel.toolwindow.highlight.mappings&quot;: &quot;true&quot;,\r\n    &quot;WebServerToolWindowPanel.toolwindow.highlight.symlinks&quot;: &quot;true&quot;,\r\n    &quot;WebServerToolWindowPanel.toolwindow.show.date&quot;: &quot;false&quot;,\r\n    &quot;WebServerToolWindowPanel.toolwindow.show.permissions&quot;: &quot;false&quot;,\r\n    &quot;WebServerToolWindowPanel.toolwindow.show.size&quot;: &quot;false&quot;,\r\n    &quot;git-widget-placeholder&quot;: &quot;master&quot;,\r\n    &quot;last_opened_file_path&quot;: &quot;F:/Courses/Course_Demos/NTUC/AI_Lecture_Demos/Cuterbot_2004/test&quot;,\r\n    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,\r\n    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,\r\n    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,\r\n    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,\r\n    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,\r\n    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;,\r\n    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;\r\n  }\r\n}</component>\r\n  <component name=\"RecentsManager\">\r\n    <key name=\"CopyFile.RECENT_KEYS\">\r\n      <recent name=\"F:\\Courses\\Course_Demos\\NTUC\\AI_Lecture_Demos\\Cuterbot_2004\\test\" />\r\n      <recent name=\"F:\\Courses\\Course_Demos\\NTUC\\AI_Lecture_Demos\\Cuterbot_2004\\jetbot\" />\r\n      <recent name=\"F:\\Courses\\Course_Demos\\NTUC\\AI_Lecture_Demos\\Cuterbot_2004\\notebooks\\object_following\" />\r\n      <recent name=\"F:\\Courses\\Course_Demos\\NTUC\\AI_Lecture_Demos\\Cuterbot_2004\\docker\\nv_docker_settings\" />\r\n    </key>\r\n  </component>\r\n  <component name=\"RunManager\" selected=\"Python.test_inheritance\">\r\n    <configuration name=\"jetbot_states\" type=\"PythonConfigurationType\" factoryName=\"Python\" nameIsGenerated=\"true\">\r\n      <module name=\"Cuterbot_WS\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"false\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/jetbot/apps/jetbot_states.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"test_fm_function\" type=\"PythonConfigurationType\" factoryName=\"Python\" nameIsGenerated=\"true\">\r\n      <module name=\"Cuterbot_2004\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n        <env name=\"DISPLAY\" value=\"localhost:10.0\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"SDK_NAME\" value=\"Remote Python 3.8.10 (sftp://cuterbot@192.168.1.113:22/usr/bin/python3.8)\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"false\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/test/test_fm_function.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"test_fpn\" type=\"PythonConfigurationType\" factoryName=\"Python\" nameIsGenerated=\"true\">\r\n      <module name=\"Cuterbot_2004\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n        <env name=\"DISPLAY\" value=\"localhost:10.0\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"SDK_NAME\" value=\"Remote Python 3.8.10 (sftp://cuterbot@192.168.1.113:22/usr/bin/python3.8)\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"false\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/test/test_fpn.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"test_inheritance\" type=\"PythonConfigurationType\" factoryName=\"Python\" nameIsGenerated=\"true\">\r\n      <module name=\"Cuterbot_2004\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n        <env name=\"DISPLAY\" value=\"localhost:10.0\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"SDK_NAME\" value=\"Remote Python 3.8.10 (sftp://cuterbot@192.168.1.113:22/usr/bin/python3.8)\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"false\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/test/test_inheritance.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"test_yolo_v7\" type=\"PythonConfigurationType\" factoryName=\"Python\" nameIsGenerated=\"true\">\r\n      <module name=\"Cuterbot_2004\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n        <env name=\"DISPLAY\" value=\"localhost:10.0\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"SDK_NAME\" value=\"Remote Python 3.8.10 (sftp://cuterbot@192.168.1.113:22/usr/bin/python3.8)\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"false\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/test/test_yolo_v7.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <list>\r\n      <item itemvalue=\"Python.jetbot_states\" />\r\n      <item itemvalue=\"Python.test_fm_function\" />\r\n      <item itemvalue=\"Python.test_fpn\" />\r\n      <item itemvalue=\"Python.test_inheritance\" />\r\n      <item itemvalue=\"Python.test_yolo_v7\" />\r\n    </list>\r\n  </component>\r\n  <component name=\"SharedIndexes\">\r\n    <attachedChunks>\r\n      <set>\r\n        <option value=\"bundled-js-predefined-1d06a55b98c1-0b3e54e931b4-JavaScript-PY-241.17890.14\" />\r\n        <option value=\"bundled-python-sdk-5b207ade9991-7e9c3bbb6e34-com.jetbrains.pycharm.pro.sharedIndexes.bundled-PY-241.17890.14\" />\r\n      </set>\r\n    </attachedChunks>\r\n  </component>\r\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\r\n  <component name=\"TaskManager\">\r\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\r\n      <changelist id=\"26d62f21-4216-452a-9253-5dc75bd4e437\" name=\"Changes\" comment=\"\" />\r\n      <created>1713761873921</created>\r\n      <option name=\"number\" value=\"Default\" />\r\n      <option name=\"presentableId\" value=\"Default\" />\r\n      <updated>1713761873921</updated>\r\n      <workItem from=\"1713761875448\" duration=\"97000\" />\r\n      <workItem from=\"1713761999974\" duration=\"1236000\" />\r\n      <workItem from=\"1713773202156\" duration=\"268000\" />\r\n      <workItem from=\"1713773800053\" duration=\"110000\" />\r\n      <workItem from=\"1713773943551\" duration=\"9000\" />\r\n      <workItem from=\"1713775446653\" duration=\"119000\" />\r\n      <workItem from=\"1713775600865\" duration=\"511000\" />\r\n      <workItem from=\"1713776118681\" duration=\"4000\" />\r\n      <workItem from=\"1713776230874\" duration=\"63000\" />\r\n      <workItem from=\"1713776297255\" duration=\"1290000\" />\r\n      <workItem from=\"1713847025061\" duration=\"274000\" />\r\n      <workItem from=\"1713850054680\" duration=\"3778000\" />\r\n      <workItem from=\"1713854622375\" duration=\"5260000\" />\r\n      <workItem from=\"1713863658158\" duration=\"1212000\" />\r\n      <workItem from=\"1713864879066\" duration=\"1266000\" />\r\n      <workItem from=\"1714194795260\" duration=\"829000\" />\r\n      <workItem from=\"1714205363779\" duration=\"967000\" />\r\n      <workItem from=\"1714209676282\" duration=\"2120000\" />\r\n      <workItem from=\"1714227719502\" duration=\"642000\" />\r\n      <workItem from=\"1714300299531\" duration=\"443000\" />\r\n      <workItem from=\"1714300758297\" duration=\"2322000\" />\r\n      <workItem from=\"1714361645825\" duration=\"953000\" />\r\n      <workItem from=\"1714364614984\" duration=\"1190000\" />\r\n      <workItem from=\"1714366437819\" duration=\"1404000\" />\r\n      <workItem from=\"1714368231013\" duration=\"1393000\" />\r\n      <workItem from=\"1714386179991\" duration=\"1484000\" />\r\n      <workItem from=\"1714457068771\" duration=\"1517000\" />\r\n      <workItem from=\"1714458630193\" duration=\"626000\" />\r\n      <workItem from=\"1714460832439\" duration=\"29000\" />\r\n      <workItem from=\"1714465715616\" duration=\"41000\" />\r\n      <workItem from=\"1714474922171\" duration=\"2808000\" />\r\n      <workItem from=\"1714477746932\" duration=\"836000\" />\r\n      <workItem from=\"1714478591643\" duration=\"16348000\" />\r\n      <workItem from=\"1714498089256\" duration=\"4643000\" />\r\n      <workItem from=\"1714629840092\" duration=\"318000\" />\r\n      <workItem from=\"1714761513736\" duration=\"15000\" />\r\n      <workItem from=\"1717061318898\" duration=\"2603000\" />\r\n      <workItem from=\"1719126115513\" duration=\"7017000\" />\r\n      <workItem from=\"1719135938093\" duration=\"1818000\" />\r\n      <workItem from=\"1719139386941\" duration=\"2875000\" />\r\n      <workItem from=\"1719142632275\" duration=\"19864000\" />\r\n      <workItem from=\"1719163271876\" duration=\"3768000\" />\r\n      <workItem from=\"1719167202825\" duration=\"5899000\" />\r\n      <workItem from=\"1719174000483\" duration=\"20315000\" />\r\n      <workItem from=\"1719213970203\" duration=\"2861000\" />\r\n      <workItem from=\"1719511537266\" duration=\"5506000\" />\r\n      <workItem from=\"1719540032708\" duration=\"243000\" />\r\n      <workItem from=\"1719541232194\" duration=\"26000\" />\r\n      <workItem from=\"1719541873667\" duration=\"2407000\" />\r\n      <workItem from=\"1719547435177\" duration=\"5223000\" />\r\n      <workItem from=\"1719556223429\" duration=\"24000\" />\r\n      <workItem from=\"1719557234172\" duration=\"159000\" />\r\n      <workItem from=\"1719887615381\" duration=\"698000\" />\r\n      <workItem from=\"1720026912985\" duration=\"159000\" />\r\n      <workItem from=\"1720291358402\" duration=\"1319000\" />\r\n      <workItem from=\"1720295360721\" duration=\"67000\" />\r\n      <workItem from=\"1720333047482\" duration=\"1186000\" />\r\n      <workItem from=\"1720334525720\" duration=\"1305000\" />\r\n      <workItem from=\"1720335858953\" duration=\"109000\" />\r\n      <workItem from=\"1720340200949\" duration=\"4026000\" />\r\n      <workItem from=\"1720356778274\" duration=\"628000\" />\r\n      <workItem from=\"1720357936454\" duration=\"9548000\" />\r\n      <workItem from=\"1720375467084\" duration=\"42000\" />\r\n      <workItem from=\"1720523501914\" duration=\"88000\" />\r\n      <workItem from=\"1720572334092\" duration=\"2000\" />\r\n      <workItem from=\"1720579797167\" duration=\"1401000\" />\r\n      <workItem from=\"1720598266538\" duration=\"45000\" />\r\n      <workItem from=\"1720621343974\" duration=\"38000\" />\r\n      <workItem from=\"1720791306206\" duration=\"189000\" />\r\n      <workItem from=\"1720791667133\" duration=\"70000\" />\r\n      <workItem from=\"1720798413891\" duration=\"3000\" />\r\n      <workItem from=\"1720845754176\" duration=\"50000\" />\r\n      <workItem from=\"1720845814599\" duration=\"139000\" />\r\n      <workItem from=\"1720856947477\" duration=\"235000\" />\r\n      <workItem from=\"1720857746557\" duration=\"826000\" />\r\n      <workItem from=\"1720860236515\" duration=\"15000\" />\r\n      <workItem from=\"1720861276907\" duration=\"49000\" />\r\n      <workItem from=\"1720861927018\" duration=\"34000\" />\r\n      <workItem from=\"1720862738308\" duration=\"2977000\" />\r\n      <workItem from=\"1720873969649\" duration=\"47000\" />\r\n      <workItem from=\"1720874881285\" duration=\"50000\" />\r\n      <workItem from=\"1720883448514\" duration=\"643000\" />\r\n      <workItem from=\"1720889552929\" duration=\"1145000\" />\r\n      <workItem from=\"1720891661708\" duration=\"360000\" />\r\n      <workItem from=\"1720918634258\" duration=\"1963000\" />\r\n      <workItem from=\"1720921135765\" duration=\"126000\" />\r\n      <workItem from=\"1720922757032\" duration=\"274000\" />\r\n      <workItem from=\"1721027852333\" duration=\"36501000\" />\r\n      <workItem from=\"1721067100460\" duration=\"14205000\" />\r\n      <workItem from=\"1721105014291\" duration=\"4443000\" />\r\n      <workItem from=\"1721214240754\" duration=\"33455000\" />\r\n      <workItem from=\"1721288238715\" duration=\"17977000\" />\r\n      <workItem from=\"1721340925139\" duration=\"2437000\" />\r\n      <workItem from=\"1721343409037\" duration=\"47353000\" />\r\n      <workItem from=\"1721469208776\" duration=\"301000\" />\r\n      <workItem from=\"1721469626385\" duration=\"6041000\" />\r\n      <workItem from=\"1721572681974\" duration=\"2824000\" />\r\n      <workItem from=\"1721575640965\" duration=\"197000\" />\r\n      <workItem from=\"1721789968099\" duration=\"1066000\" />\r\n      <workItem from=\"1721791046018\" duration=\"636000\" />\r\n      <workItem from=\"1721791688214\" duration=\"159000\" />\r\n      <workItem from=\"1721791851685\" duration=\"196000\" />\r\n      <workItem from=\"1721792556645\" duration=\"44804000\" />\r\n      <workItem from=\"1721872706081\" duration=\"6688000\" />\r\n      <workItem from=\"1721902411276\" duration=\"142000\" />\r\n      <workItem from=\"1721902606509\" duration=\"1521000\" />\r\n      <workItem from=\"1721904166327\" duration=\"18075000\" />\r\n      <workItem from=\"1721926051466\" duration=\"17801000\" />\r\n      <workItem from=\"1721974899788\" duration=\"239000\" />\r\n      <workItem from=\"1722018699890\" duration=\"56000\" />\r\n      <workItem from=\"1722047121881\" duration=\"101000\" />\r\n      <workItem from=\"1722077444316\" duration=\"147000\" />\r\n      <workItem from=\"1722846279102\" duration=\"16000\" />\r\n    </task>\r\n    <task id=\"LOCAL-00129\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721471798963</created>\r\n      <option name=\"number\" value=\"00129\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00129\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721471798963</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00130\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721471858967</created>\r\n      <option name=\"number\" value=\"00130\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00130\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721471858967</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00131\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721471979597</created>\r\n      <option name=\"number\" value=\"00131\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00131\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721471979597</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00132\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721472179383</created>\r\n      <option name=\"number\" value=\"00132\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00132\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721472179383</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00133\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721475930409</created>\r\n      <option name=\"number\" value=\"00133\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00133\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721475930409</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00134\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721476057983</created>\r\n      <option name=\"number\" value=\"00134\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00134\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721476057983</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00135\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721573580016</created>\r\n      <option name=\"number\" value=\"00135\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00135\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721573580018</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00136\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721574166838</created>\r\n      <option name=\"number\" value=\"00136\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00136\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721574166838</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00137\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721575553639</created>\r\n      <option name=\"number\" value=\"00137\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00137\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721575553639</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00138\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721575823951</created>\r\n      <option name=\"number\" value=\"00138\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00138\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721575823951</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00139\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721818735059</created>\r\n      <option name=\"number\" value=\"00139\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00139\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721818735062</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00140\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721818762627</created>\r\n      <option name=\"number\" value=\"00140\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00140\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721818762627</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00141\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721818808175</created>\r\n      <option name=\"number\" value=\"00141\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00141\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721818808176</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00142\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721820365614</created>\r\n      <option name=\"number\" value=\"00142\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00142\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721820365614</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00143\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721822004424</created>\r\n      <option name=\"number\" value=\"00143\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00143\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721822004424</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00144\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721825608114</created>\r\n      <option name=\"number\" value=\"00144\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00144\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721825608114</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00145\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721825782453</created>\r\n      <option name=\"number\" value=\"00145\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00145\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721825782453</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00146\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721831047341</created>\r\n      <option name=\"number\" value=\"00146\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00146\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721831047341</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00147\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721831116754</created>\r\n      <option name=\"number\" value=\"00147\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00147\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721831116755</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00148\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721856468562</created>\r\n      <option name=\"number\" value=\"00148\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00148\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721856468566</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00149\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721856565773</created>\r\n      <option name=\"number\" value=\"00149\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00149\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721856565773</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00150\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721856584333</created>\r\n      <option name=\"number\" value=\"00150\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00150\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721856584333</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00151\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721878773276</created>\r\n      <option name=\"number\" value=\"00151\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00151\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721878773277</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00152\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721879552508</created>\r\n      <option name=\"number\" value=\"00152\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00152\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721879552509</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00153\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721902470806</created>\r\n      <option name=\"number\" value=\"00153\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00153\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721902470807</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00154\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721904536863</created>\r\n      <option name=\"number\" value=\"00154\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00154\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721904536864</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00155\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721910059387</created>\r\n      <option name=\"number\" value=\"00155\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00155\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721910059387</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00156\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721911336373</created>\r\n      <option name=\"number\" value=\"00156\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00156\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721911336378</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00157\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721911583132</created>\r\n      <option name=\"number\" value=\"00157\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00157\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721911583132</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00158\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721918950444</created>\r\n      <option name=\"number\" value=\"00158\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00158\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721918950444</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00159\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721918993374</created>\r\n      <option name=\"number\" value=\"00159\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00159\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721918993375</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00160\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721921042236</created>\r\n      <option name=\"number\" value=\"00160\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00160\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721921042236</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00161\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721925086803</created>\r\n      <option name=\"number\" value=\"00161\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00161\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721925086804</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00162\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721925373633</created>\r\n      <option name=\"number\" value=\"00162\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00162\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721925373633</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00163\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721925396015</created>\r\n      <option name=\"number\" value=\"00163\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00163\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721925396015</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00164\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721926027267</created>\r\n      <option name=\"number\" value=\"00164\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00164\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721926027267</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00165\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721929449087</created>\r\n      <option name=\"number\" value=\"00165\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00165\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721929449087</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00166\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721957490631</created>\r\n      <option name=\"number\" value=\"00166\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00166\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721957490631</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00167\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721960583962</created>\r\n      <option name=\"number\" value=\"00167\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00167\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721960583962</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00168\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721960606489</created>\r\n      <option name=\"number\" value=\"00168\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00168\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721960606489</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00169\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721963961780</created>\r\n      <option name=\"number\" value=\"00169\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00169\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721963961780</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00170\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721966518271</created>\r\n      <option name=\"number\" value=\"00170\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00170\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721966518272</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00171\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721968581132</created>\r\n      <option name=\"number\" value=\"00171\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00171\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721968581132</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00172\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721968636441</created>\r\n      <option name=\"number\" value=\"00172\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00172\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721968636441</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00173\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721969449956</created>\r\n      <option name=\"number\" value=\"00173\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00173\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721969449956</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00174\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721970283504</created>\r\n      <option name=\"number\" value=\"00174\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00174\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721970283504</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00175\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721971543368</created>\r\n      <option name=\"number\" value=\"00175\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00175\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721971543368</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00176\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1721975085442</created>\r\n      <option name=\"number\" value=\"00176\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00176\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1721975085443</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00177\" summary=\"update\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1722077557559</created>\r\n      <option name=\"number\" value=\"00177\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00177\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1722077557559</updated>\r\n    </task>\r\n    <option name=\"localTasksCounter\" value=\"178\" />\r\n    <servers />\r\n  </component>\r\n  <component name=\"TypeScriptGeneratedFilesManager\">\r\n    <option name=\"version\" value=\"3\" />\r\n  </component>\r\n  <component name=\"Vcs.Log.Tabs.Properties\">\r\n    <option name=\"TAB_STATES\">\r\n      <map>\r\n        <entry key=\"MAIN\">\r\n          <value>\r\n            <State>\r\n              <option name=\"FILTERS\">\r\n                <map>\r\n                  <entry key=\"branch\">\r\n                    <value>\r\n                      <list>\r\n                        <option value=\"master\" />\r\n                      </list>\r\n                    </value>\r\n                  </entry>\r\n                </map>\r\n              </option>\r\n            </State>\r\n          </value>\r\n        </entry>\r\n      </map>\r\n    </option>\r\n  </component>\r\n  <component name=\"VcsManagerConfiguration\">\r\n    <ignored-roots>\r\n      <path value=\"$PROJECT_DIR$/jetbot/apps/INA3221-Python-Library\" />\r\n      <path value=\"$PROJECT_DIR$\" />\r\n    </ignored-roots>\r\n    <MESSAGE value=\"delete model files\" />\r\n    <MESSAGE value=\"modify np.int to int in according to the release of numpy version &gt; 20\" />\r\n    <MESSAGE value=\"add to modify the nv container run time to mount py 3.8 tensorrt\" />\r\n    <MESSAGE value=\"typo coding\" />\r\n    <MESSAGE value=\"add jetbot_stats class for monitor jetbot states\" />\r\n    <MESSAGE value=\"coding revise\" />\r\n    <MESSAGE value=\"Dockerfile modified\" />\r\n    <MESSAGE value=\"typo\" />\r\n    <MESSAGE value=\"NV container setting\" />\r\n    <MESSAGE value=\"change Dockerfile name of base images\" />\r\n    <MESSAGE value=\"Add pwr states function using jtop\" />\r\n    <MESSAGE value=\"update\" />\r\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"update\" />\r\n  </component>\r\n  <component name=\"XDebuggerManager\">\r\n    <breakpoint-manager>\r\n      <breakpoints>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/test/test_inheritance.py</url>\r\n          <line>3</line>\r\n          <option name=\"timeStamp\" value=\"2\" />\r\n        </line-breakpoint>\r\n      </breakpoints>\r\n    </breakpoint-manager>\r\n  </component>\r\n  <component name=\"com.intellij.coverage.CoverageDataManagerImpl\">\r\n    <SUITE FILE_PATH=\"coverage/Cuterbot_2004$test_fpn.coverage\" NAME=\"test_fpn Coverage Results\" MODIFIED=\"1721803031703\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"false\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"\" />\r\n    <SUITE FILE_PATH=\"coverage/Cuterbot_2004$stats.coverage\" NAME=\"stats Coverage Results\" MODIFIED=\"1713851089294\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/jetbot/apps\" />\r\n    <SUITE FILE_PATH=\"coverage/Cuterbot_2004$jetbot_states.coverage\" NAME=\"jetbot_states Coverage Results\" MODIFIED=\"1714499009096\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"\" />\r\n    <SUITE FILE_PATH=\"coverage/Cuterbot_2004$test_yolo_v7.coverage\" NAME=\"test_yolo_v7 Coverage Results\" MODIFIED=\"1720342343965\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"false\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"\" />\r\n    <SUITE FILE_PATH=\"coverage/Cuterbot_2004$.coverage\" NAME=\" Coverage Results\" MODIFIED=\"1721791715416\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"false\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/test\" />\r\n    <SUITE FILE_PATH=\"coverage/Cuterbot_2004$test_fm_function.coverage\" NAME=\"test_fm_function Coverage Results\" MODIFIED=\"1720364050161\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"false\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"\" />\r\n    <SUITE FILE_PATH=\"coverage/Cuterbot_2004$test_inheritance.coverage\" NAME=\"test_inheritance Coverage Results\" MODIFIED=\"1721814926708\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"false\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"\" />\r\n    <SUITE FILE_PATH=\"coverage/Cuterbot_2004$model_selection.coverage\" NAME=\"model_selection Coverage Results\" MODIFIED=\"1721229101527\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"false\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/jetbot/utils\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/.idea/workspace.xml	(date 1722850329460)
@@ -5,7 +5,41 @@
   </component>
   <component name="ChangeListManager">
     <list default="true" id="26d62f21-4216-452a-9253-5dc75bd4e437" name="Changes" comment="update">
+      <change afterPath="$PROJECT_DIR$/notebooks/road_following/live_demo_build_trt_v0.ipynb" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/notebooks/road_following/live_demo_build_trt_v1.ipynb" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/notebooks/road_following/train_model_PC_o.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/Cuterbot_2004.iml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/Cuterbot_2004.iml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/deployment.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/deployment.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/misc.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/misc.xml" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/docker/display/build.sh" beforeDir="false" afterPath="$PROJECT_DIR$/docker/display/build.sh" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/docker/jupyter/enable.sh" beforeDir="false" afterPath="$PROJECT_DIR$/docker/jupyter/enable.sh" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/apps/stats.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/apps/stats.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/fleet_manager_trt.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/fleet_manager_trt.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/fleet_manager_trt_v0.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/fleet_manager_trt_v0.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/fleet_manager_trt_v1.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/fleet_manager_trt_v1.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/object_detection.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/object_detection.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/object_follower.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/object_follower.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/object_follower_v0.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/object_follower_v0.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/road_cruiser.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/road_cruiser.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/road_cruiser_trt.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/road_cruiser_trt.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/road_cruiser_v0.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/road_cruiser_v0.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/tensorrt_model.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/tensorrt_model.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/tensorrt_model_o.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/tensorrt_model_o.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/utils/model_selection.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/utils/model_selection.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/utils/plot_perf.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/utils/plot_perf.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/jetbot/utils/training_profile.py" beforeDir="false" afterPath="$PROJECT_DIR$/jetbot/utils/training_profile.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/fleet_management/live_demo.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/fleet_management/live_demo.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/fleet_management/live_demo_trt.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/fleet_management/live_demo_trt.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/object_following/live_demo.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/object_following/live_demo.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/object_following/live_demo_o.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/object_following/live_demo_o.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/road_following/data_collection_gamepad.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/road_following/data_collection_gamepad.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/road_following/live_demo_build_trt.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/road_following/live_demo_build_trt.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/road_following/live_demo_light.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/road_following/live_demo_light.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/road_following/live_demo_light_trt.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/road_following/live_demo_light_trt.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/road_following/live_demo_trt.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/road_following/live_demo_trt.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/road_following/train_model.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/road_following/train_model.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/notebooks/road_following/train_model_PC.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/notebooks/road_following/train_model_PC.ipynb" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -51,41 +85,42 @@
     <option name="hideEmptyMiddlePackages" value="true" />
     <option name="showLibraryContents" value="true" />
   </component>
-  <component name="PropertiesComponent">{
-  &quot;keyToString&quot;: {
-    &quot;Python tests.Python tests in test_inheritance.py.executor&quot;: &quot;Run&quot;,
-    &quot;Python.jetbot_states.executor&quot;: &quot;Run&quot;,
-    &quot;Python.model_selection.executor&quot;: &quot;Run&quot;,
-    &quot;Python.stats.executor&quot;: &quot;Debug&quot;,
-    &quot;Python.test_fm_function.executor&quot;: &quot;Run&quot;,
-    &quot;Python.test_fpn.executor&quot;: &quot;Run&quot;,
-    &quot;Python.test_inheritance.executor&quot;: &quot;Debug&quot;,
-    &quot;Python.test_yolo_v7.executor&quot;: &quot;Run&quot;,
-    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,
-    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
-    &quot;SHARE_PROJECT_CONFIGURATION_FILES&quot;: &quot;true&quot;,
-    &quot;WebServerToolWindowPanel.toolwindow.highlight.mappings&quot;: &quot;true&quot;,
-    &quot;WebServerToolWindowPanel.toolwindow.highlight.symlinks&quot;: &quot;true&quot;,
-    &quot;WebServerToolWindowPanel.toolwindow.show.date&quot;: &quot;false&quot;,
-    &quot;WebServerToolWindowPanel.toolwindow.show.permissions&quot;: &quot;false&quot;,
-    &quot;WebServerToolWindowPanel.toolwindow.show.size&quot;: &quot;false&quot;,
-    &quot;git-widget-placeholder&quot;: &quot;master&quot;,
-    &quot;last_opened_file_path&quot;: &quot;F:/Courses/Course_Demos/NTUC/AI_Lecture_Demos/Cuterbot_2004/test&quot;,
-    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,
-    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,
-    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,
-    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,
-    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,
-    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;,
-    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;
+  <component name="PropertiesComponent"><![CDATA[{
+  "keyToString": {
+    "Python tests.Python tests in test_inheritance.py.executor": "Run",
+    "Python.jetbot_states.executor": "Run",
+    "Python.model_selection.executor": "Run",
+    "Python.stats.executor": "Debug",
+    "Python.test_fm_function.executor": "Run",
+    "Python.test_fpn.executor": "Run",
+    "Python.test_inheritance.executor": "Debug",
+    "Python.test_yolo_v7.executor": "Run",
+    "RunOnceActivity.OpenProjectViewOnStart": "true",
+    "RunOnceActivity.ShowReadmeOnStart": "true",
+    "SHARE_PROJECT_CONFIGURATION_FILES": "true",
+    "WebServerToolWindowPanel.toolwindow.highlight.mappings": "true",
+    "WebServerToolWindowPanel.toolwindow.highlight.symlinks": "true",
+    "WebServerToolWindowPanel.toolwindow.show.date": "false",
+    "WebServerToolWindowPanel.toolwindow.show.permissions": "false",
+    "WebServerToolWindowPanel.toolwindow.show.size": "false",
+    "git-widget-placeholder": "master",
+    "last_opened_file_path": "F:/Courses/Course_Demos/NTUC/AI_Lecture_Demos/Cuterbot_2004/jetbot/apps",
+    "node.js.detected.package.eslint": "true",
+    "node.js.detected.package.tslint": "true",
+    "node.js.selected.package.eslint": "(autodetect)",
+    "node.js.selected.package.tslint": "(autodetect)",
+    "nodejs_package_manager_path": "npm",
+    "settings.editor.selected.configurable": "com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable",
+    "vue.rearranger.settings.migration": "true"
   }
-}</component>
+}]]></component>
   <component name="RecentsManager">
     <key name="CopyFile.RECENT_KEYS">
-      <recent name="F:\Courses\Course_Demos\NTUC\AI_Lecture_Demos\Cuterbot_2004\test" />
-      <recent name="F:\Courses\Course_Demos\NTUC\AI_Lecture_Demos\Cuterbot_2004\jetbot" />
+      <recent name="F:\Courses\Course_Demos\NTUC\AI_Lecture_Demos\Cuterbot_2004\jetbot\apps" />
+      <recent name="F:\Courses\Course_Demos\NTUC\AI_Lecture_Demos\Cuterbot_2004\jetbot\utils" />
+      <recent name="F:\Courses\Course_Demos\NTUC\AI_Lecture_Demos\Cuterbot_2004\notebooks\road_following" />
       <recent name="F:\Courses\Course_Demos\NTUC\AI_Lecture_Demos\Cuterbot_2004\notebooks\object_following" />
-      <recent name="F:\Courses\Course_Demos\NTUC\AI_Lecture_Demos\Cuterbot_2004\docker\nv_docker_settings" />
+      <recent name="F:\Courses\Course_Demos\NTUC\AI_Lecture_Demos\Cuterbot_2004\notebooks\fleet_management" />
     </key>
   </component>
   <component name="RunManager" selected="Python.test_inheritance">
@@ -349,14 +384,8 @@
       <workItem from="1722047121881" duration="101000" />
       <workItem from="1722077444316" duration="147000" />
       <workItem from="1722846279102" duration="16000" />
-    </task>
-    <task id="LOCAL-00129" summary="update">
-      <option name="closed" value="true" />
-      <created>1721471798963</created>
-      <option name="number" value="00129" />
-      <option name="presentableId" value="LOCAL-00129" />
-      <option name="project" value="LOCAL" />
-      <updated>1721471798963</updated>
+      <workItem from="1722846308216" duration="1072000" />
+      <workItem from="1722847591984" duration="2299000" />
     </task>
     <task id="LOCAL-00130" summary="update">
       <option name="closed" value="true" />
@@ -742,7 +771,15 @@
       <option name="project" value="LOCAL" />
       <updated>1722077557559</updated>
     </task>
-    <option name="localTasksCounter" value="178" />
+    <task id="LOCAL-00178" summary="update">
+      <option name="closed" value="true" />
+      <created>1722846623711</created>
+      <option name="number" value="00178" />
+      <option name="presentableId" value="LOCAL-00178" />
+      <option name="project" value="LOCAL" />
+      <updated>1722846623711</updated>
+    </task>
+    <option name="localTasksCounter" value="179" />
     <servers />
   </component>
   <component name="TypeScriptGeneratedFilesManager">
Index: jetbot/utils/plot_perf.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nimport matplotlib\r\n\r\nmatplotlib.use(\"TkAgg\")\r\nimport matplotlib.pyplot as plt\r\nimport os\r\n\r\n\r\ndef plot_exec_time(execution_time, model_name, model_str):\r\n    # os.environ['DISPLAY'] = ':10.0'\r\n    execute_time = np.array(execution_time)\r\n    mean_execute_time = np.mean(execute_time)\r\n    max_execute_time = np.amax(execute_time)\r\n    min_execute_time = np.amin(execute_time)\r\n\r\n    # fps = np.array(fps)\r\n    mean_fps = 1/mean_execute_time\r\n    max_fps = 1/min_execute_time\r\n    min_fps = 1/max_execute_time\r\n\r\n    print(\r\n        \"The execution time statistics of %s  ----- \\n     Mean execution time of : %.4f sec.\\n     Max execution time : %.4f sec.\\n     Min execution time of : %.4f sec. \" \\\r\n        % (model_name, mean_execute_time, max_execute_time, min_execute_time))\r\n\r\n    # fig = plt.figure()\r\n    fig, ax = plt.subplots()\r\n    # ax = fig.add_subplot()\r\n    nbin = 150\r\n    sbin = (max_execute_time*1.2 - min_execute_time * 0.8) / nbin\r\n    ax.hist(execute_time, bins=(np.arange(min_execute_time * 0.8, max_execute_time * 1.2, sbin)).tolist())\r\n    # ax.hist(execute_time, bins=(0.003 * np.array(list(range(151)))).tolist())\r\n    ax.set_xlabel('processing time, sec.')\r\n    ax.set_ylabel('No. of processes')\r\n    ax.set_title('Histogram of processing time of  ' + model_name  + \"\\n\"+ model_str)\r\n    props = dict(boxstyle='round', facecolor='wheat')\r\n    text_str = \" mean execution time : %.4f sec. (%.1f FPS)\\n max execution time : %.4f sec. (%.1f FPS)\\n min execution time : %.4f sec. (%.1f FPS)\" \\\r\n        % (mean_execute_time, mean_fps, max_execute_time, min_fps, min_execute_time, max_fps)\r\n    ax.text(0.5, 0.85, text_str, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=props)\r\n    # plt.show(block=False)\r\n    
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/jetbot/utils/plot_perf.py b/jetbot/utils/plot_perf.py
--- a/jetbot/utils/plot_perf.py	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/jetbot/utils/plot_perf.py	(date 1722088175949)
@@ -3,7 +3,9 @@
 
 matplotlib.use("TkAgg")
 import matplotlib.pyplot as plt
-import os
+
+font = {'fontweight': 'normal', 'fontsize': 12}
+font_title = {'fontweight': 'medium', 'fontsize': 18}
 
 
 def plot_exec_time(execution_time, model_name, model_str):
@@ -14,27 +16,30 @@
     min_execute_time = np.amin(execute_time)
 
     # fps = np.array(fps)
-    mean_fps = 1/mean_execute_time
-    max_fps = 1/min_execute_time
-    min_fps = 1/max_execute_time
+    mean_fps = 1 / mean_execute_time
+    max_fps = 1 / min_execute_time
+    min_fps = 1 / max_execute_time
 
     print(
         "The execution time statistics of %s  ----- \n     Mean execution time of : %.4f sec.\n     Max execution time : %.4f sec.\n     Min execution time of : %.4f sec. " \
-        % (model_name, mean_execute_time, max_execute_time, min_execute_time))
+        % (model_name, float(mean_execute_time), float(max_execute_time), float(min_execute_time)))
 
     # fig = plt.figure()
-    fig, ax = plt.subplots()
+    fig, ax = plt.subplots(figsize=(12, 6))
     # ax = fig.add_subplot()
     nbin = 150
-    sbin = (max_execute_time*1.2 - min_execute_time * 0.8) / nbin
+    sbin = (max_execute_time * 1.2 - min_execute_time * 0.8) / nbin
     ax.hist(execute_time, bins=(np.arange(min_execute_time * 0.8, max_execute_time * 1.2, sbin)).tolist())
     # ax.hist(execute_time, bins=(0.003 * np.array(list(range(151)))).tolist())
-    ax.set_xlabel('processing time, sec.')
-    ax.set_ylabel('No. of processes')
-    ax.set_title('Histogram of processing time of  ' + model_name  + "\n"+ model_str)
+    ax.set_xlabel('processing time, sec.', fontdict=font)
+    ax.set_ylabel('No. of processes', fontdict=font)
+    ax.set_title('Histogram of processing time of  ' + model_name + "\n" + model_str, fontdict=font_title)
     props = dict(boxstyle='round', facecolor='wheat')
     text_str = " mean execution time : %.4f sec. (%.1f FPS)\n max execution time : %.4f sec. (%.1f FPS)\n min execution time : %.4f sec. (%.1f FPS)" \
-        % (mean_execute_time, mean_fps, max_execute_time, min_fps, min_execute_time, max_fps)
-    ax.text(0.5, 0.85, text_str, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=props)
-    # plt.show(block=False)
-    
\ No newline at end of file
+               % (float(mean_execute_time), float(mean_fps), float(max_execute_time), float(min_fps),
+                  float(min_execute_time), float(max_fps))
+    ax.text(0.55, 0.85, text_str, transform=ax.transAxes, fontsize=12, verticalalignment='top', bbox=props)
+
+    fig.canvas.draw()
+    fig.canvas.flush_events()
+    plt.show(block=False)
Index: .idea/deployment.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"PublishConfigData\" autoUpload=\"Always\" serverName=\"cuterbot@192.168.1.113:22 password\" remoteFilesAllowedToDisappearOnAutoupload=\"false\" showEqualInSync=\"true\" showDifferentInSync=\"false\">\r\n    <serverData>\r\n      <paths name=\"cuterbot@192.168.1.113:22 password\">\r\n        <serverdata>\r\n          <mappings>\r\n            <mapping deploy=\"/home/cuterbot/Cuterbot_2004\" local=\"$PROJECT_DIR$\" />\r\n          </mappings>\r\n        </serverdata>\r\n      </paths>\r\n    </serverData>\r\n    <option name=\"myAutoUpload\" value=\"ALWAYS\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/deployment.xml b/.idea/deployment.xml
--- a/.idea/deployment.xml	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/.idea/deployment.xml	(date 1722847470881)
@@ -1,7 +1,28 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
-  <component name="PublishConfigData" autoUpload="Always" serverName="cuterbot@192.168.1.113:22 password" remoteFilesAllowedToDisappearOnAutoupload="false" showEqualInSync="true" showDifferentInSync="false">
+  <component name="PublishConfigData" autoUpload="Always" serverName="cuterbot@192.168.1.102:22 password" remoteFilesAllowedToDisappearOnAutoupload="false" showEqualInSync="true" showDifferentInSync="false">
     <serverData>
+      <paths name="cuterbot@192.168.1.102:22 password">
+        <serverdata>
+          <mappings>
+            <mapping deploy="/home/cuterbot/Cuterbot_2004" local="$PROJECT_DIR$" />
+          </mappings>
+        </serverdata>
+      </paths>
+      <paths name="cuterbot@192.168.1.110:22 password">
+        <serverdata>
+          <mappings>
+            <mapping local="$PROJECT_DIR$" web="/" />
+          </mappings>
+        </serverdata>
+      </paths>
+      <paths name="cuterbot@192.168.1.111:22 password">
+        <serverdata>
+          <mappings>
+            <mapping local="$PROJECT_DIR$" web="/" />
+          </mappings>
+        </serverdata>
+      </paths>
       <paths name="cuterbot@192.168.1.113:22 password">
         <serverdata>
           <mappings>
Index: jetbot/utils/training_profile.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># %matplotlib inline\r\n# from IPython.display import clear_output\r\n\r\nimport numpy as np\r\nimport os\r\nimport matplotlib\r\nimport matplotlib.pyplot as plt\r\n\r\nmatplotlib.use(\"TkAgg\")\r\n\r\n# dir_training_records = os.path.join(dir_depo, 'training records', TRAIN_MODEL)\r\n# os.makedirs(dir_training_records, exist_ok=True)\r\n\r\nfig_1, ax_1 = plt.subplots(figsize=(16, 8))\r\n\r\nfont = {'fontweight': 'normal', 'fontsize': 18}\r\nfont_title = {'fontweight': 'medium', 'fontsize': 24}\r\n\r\n\r\n# plot the training convergence profile\r\ndef plot_loss(loss_data, best_loss, no_epoch, dir_training_records, train_model, train_method):\r\n    plt.cla()\r\n    plt.tick_params(axis='both', labelsize='large')\r\n    epochs = range(len(loss_data))\r\n    ld_train = [ld[0] for ld in loss_data]\r\n    ld_test = [ld[1] for ld in loss_data]\r\n    ax_1.semilogy(epochs, ld_train, \"r-\", linewidth=2.0, label=\"Training Loss: {:.4E}\".format(ld_train[-1]))\r\n    ax_1.semilogy(epochs, ld_test, 'bs--', linewidth=2.0, label=\"Test Loss: {:.4E}\".format(ld_test[-1]))\r\n    ax_1.set_xlim(0, int(epochs[-1] * 1.1) + 1)\r\n    xlim = epochs[-1] + 2\r\n    ax_1.set_xlim(0, xlim)\r\n\r\n    plt.legend(fontsize='x-large')\r\n    plt.title(\"Training convergence plot -- {:s} \\n current best test loss : {:.4f}\".format(train_model, best_loss),\r\n              fontdict=font_title)\r\n    plt.xlabel('epoch', fontdict=font)\r\n    plt.ylabel('loss', fontdict=font)\r\n\r\n    fig_1.canvas.draw()\r\n    fig_1.canvas.flush_events()\r\n    plt.show(block=False)\r\n    if len(loss_data) >= no_epoch:\r\n        profile_plot = os.path.join(dir_training_records,\r\n                                    \"Training_convergence_plot_Model_{:s}_Training_Method_{:s})\".\r\n                                    format(train_model, train_method))\r\n        fig_1.savefig(profile_plot)\r\n    # plt.clf()\r\n\r\n\r\n# plot the statistical histogram of learning time in terms of epoch and sample\r\ndef lt_plot(lt_epoch, lt_sample, dir_training_records, train_model, train_method):\r\n    # ----- training time statistics in terms of epoch\r\n    learning_time_epoch = np.array(lt_epoch)\r\n    mean_lt_epoch = np.mean(learning_time_epoch)\r\n    max_lt_epoch = np.amax(learning_time_epoch)\r\n    min_lt_epoch = np.amax(learning_time_epoch)\r\n    print(\r\n        \"mean learning time per epoch: {:.3f} s, maximum epoch learning time: {:.3f} s, minimum epoch learning time: {:.3f} s\".\r\n        format(mean_lt_epoch, max_lt_epoch, min_lt_epoch))\r\n\r\n    # ----- training time statistics in terms of sample\r\n    learning_time_sample = np.array(lt_sample)\r\n    mean_lt_sample = np.mean(learning_time_sample)\r\n    max_lt_sample = np.amax(learning_time_sample)\r\n    min_lt_sample = np.amax(learning_time_sample)\r\n    print(\r\n        \"mean learning time per sample: {:.3f} s, maximum sample learning time: {:.3f} s, minimum sample learning time: {:.3f} s\".\r\n        format(mean_lt_sample, max_lt_sample, min_lt_sample))\r\n\r\n    fig_2, axh = plt.subplots(1, 2, figsize=(16, 8))\r\n    fig_2.suptitle(\"Training Time Statistics -- {:s}\".format(train_model), fontsize=24, fontweight='medium')\r\n    axh[0].set_ylabel('no. of epoch', fontdict=font)\r\n    axh[0].set_xlabel('time of training in an epoch , sec.', fontdict=font)\r\n    cf = np.floor(0.9 * min_lt_epoch)\r\n    cc = np.ceil(1.1 * max_lt_epoch)\r\n    bins_epochs_time = np.arange(cf, cc, np.ceil((cc-cf)/50))\r\n    axh[0].hist(learning_time_epoch, bins=bins_epochs_time.tolist())\r\n    axh[0].tick_params(axis='both', labelsize='large')\r\n\r\n    axh[1].set_ylabel('no. of sample', fontdict=font)\r\n    axh[1].set_xlabel('time for training a sample , sec.', fontdict=font)\r\n    sf = np.floor(0.9 * min_lt_sample)\r\n    sc = np.ceil(1.1 * max_lt_sample)\r\n    bins_samples_time = np.arange(sf, sc, np.ceil((sc-sf)/50))\r\n    axh[1].hist(learning_time_sample, bins=bins_samples_time.tolist())\r\n    # axh[1].hist(learning_time_sample, bins=(0.01 * np.array(list(range(101)))).tolist())\r\n    axh[1].tick_params(axis='both', labelsize='large')\r\n\r\n    fig_2.canvas.draw()\r\n    fig_2.canvas.flush_events()\r\n    plt.show(block=False)\r\n    training_time_file = os.path.join(dir_training_records,\r\n                                      \"Training_time_Model_{:s}_Training_Method_{:s})\".\r\n                                      format(train_model, train_method))\r\n    fig_2.savefig(training_time_file)\r\n    # plt.clf()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/jetbot/utils/training_profile.py b/jetbot/utils/training_profile.py
--- a/jetbot/utils/training_profile.py	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/jetbot/utils/training_profile.py	(date 1722561725281)
@@ -4,21 +4,23 @@
 import numpy as np
 import os
 import matplotlib
-import matplotlib.pyplot as plt
 
 matplotlib.use("TkAgg")
+import matplotlib.pyplot as plt
 
 # dir_training_records = os.path.join(dir_depo, 'training records', TRAIN_MODEL)
 # os.makedirs(dir_training_records, exist_ok=True)
 
-fig_1, ax_1 = plt.subplots(figsize=(16, 8))
+fig_1, ax_1 = plt.subplots(figsize=(14, 6))
 
-font = {'fontweight': 'normal', 'fontsize': 18}
-font_title = {'fontweight': 'medium', 'fontsize': 24}
+font = {'fontweight': 'normal', 'fontsize': 16}
+font_title = {'fontweight': 'medium', 'fontsize': 20}
 
 
 # plot the training convergence profile
-def plot_loss(loss_data, best_loss, no_epoch, dir_training_records, train_model, train_method):
+def plot_loss(loss_data, best_loss, no_epoch,
+              dir_training_records, train_model, train_method, processor,
+              show_training_plot=False):
     plt.cla()
     plt.tick_params(axis='both', labelsize='large')
     epochs = range(len(loss_data))
@@ -30,15 +32,17 @@
     xlim = epochs[-1] + 2
     ax_1.set_xlim(0, xlim)
 
-    plt.legend(fontsize='x-large')
-    plt.title("Training convergence plot -- {:s} \n current best test loss : {:.4f}".format(train_model, best_loss),
+    plt.title("Training convergence ({:s} with {:s}) -- {:s} \n current best test loss : {:.4f}".
+              format(processor, train_method, train_model, best_loss),
               fontdict=font_title)
     plt.xlabel('epoch', fontdict=font)
     plt.ylabel('loss', fontdict=font)
+    plt.legend(fontsize='x-large')
 
     fig_1.canvas.draw()
     fig_1.canvas.flush_events()
-    plt.show(block=False)
+    if show_training_plot:
+        plt.show(block=False)
     if len(loss_data) >= no_epoch:
         profile_plot = os.path.join(dir_training_records,
                                     "Training_convergence_plot_Model_{:s}_Training_Method_{:s})".
@@ -48,43 +52,59 @@
 
 
 # plot the statistical histogram of learning time in terms of epoch and sample
-def lt_plot(lt_epoch, lt_sample, dir_training_records, train_model, train_method):
+def lt_plot(lt_epoch, lt_sample, overall_time, dir_training_records, train_model, train_method, processor):
+    from math import ceil, floor
+    import time
     # ----- training time statistics in terms of epoch
+    # lt_epoch[0] -= lt_sample[0]
     learning_time_epoch = np.array(lt_epoch)
     mean_lt_epoch = np.mean(learning_time_epoch)
     max_lt_epoch = np.amax(learning_time_epoch)
-    min_lt_epoch = np.amax(learning_time_epoch)
+    min_lt_epoch = np.amin(learning_time_epoch)
     print(
         "mean learning time per epoch: {:.3f} s, maximum epoch learning time: {:.3f} s, minimum epoch learning time: {:.3f} s".
         format(mean_lt_epoch, max_lt_epoch, min_lt_epoch))
 
     # ----- training time statistics in terms of sample
-    learning_time_sample = np.array(lt_sample)
+    lt_sample.sort(reverse=True)
+    nex = ceil(0.001 * len(lt_sample))
+    learning_time_sample = np.array(lt_sample[nex: -nex])
     mean_lt_sample = np.mean(learning_time_sample)
     max_lt_sample = np.amax(learning_time_sample)
-    min_lt_sample = np.amax(learning_time_sample)
+    min_lt_sample = np.amin(learning_time_sample)
     print(
         "mean learning time per sample: {:.3f} s, maximum sample learning time: {:.3f} s, minimum sample learning time: {:.3f} s".
         format(mean_lt_sample, max_lt_sample, min_lt_sample))
 
-    fig_2, axh = plt.subplots(1, 2, figsize=(16, 8))
-    fig_2.suptitle("Training Time Statistics -- {:s}".format(train_model), fontsize=24, fontweight='medium')
+    fig_2, axh = plt.subplots(1, 2, figsize=(14, 6))
+    fig_2.suptitle("Training Time Statistics ({:s} with {:s}) -- {:s} \n Overall training time : {:s} ({:.2f} sec.)".
+                   format(processor, train_method, train_model,
+                          time.strftime("%H:%M:%S", time.gmtime(ceil(overall_time))), overall_time),
+                   fontsize=20, fontweight='medium')
     axh[0].set_ylabel('no. of epoch', fontdict=font)
-    axh[0].set_xlabel('time of training in an epoch , sec.', fontdict=font)
-    cf = np.floor(0.9 * min_lt_epoch)
-    cc = np.ceil(1.1 * max_lt_epoch)
-    bins_epochs_time = np.arange(cf, cc, np.ceil((cc-cf)/50))
+    axh[0].set_xlabel('time of training in an epoch, sec.', fontdict=font)
+    cf = 0.9 * min_lt_epoch
+    cc = 1.1 * max_lt_epoch
+    bins_epochs_time = np.arange(cf, cc, (cc - cf) / 30)
     axh[0].hist(learning_time_epoch, bins=bins_epochs_time.tolist())
     axh[0].tick_params(axis='both', labelsize='large')
+    props = dict(boxstyle='round', facecolor='wheat')
+    text_str_0 = (" mean time: %.4f sec. \n max time: %.4f sec. \n min time: %.4f sec. "
+                  % (float(mean_lt_epoch), float(max_lt_epoch), float(min_lt_epoch)))
+    axh[0].text(0.55, 0.85, text_str_0, transform=axh[0].transAxes, fontsize=10, verticalalignment='top', bbox=props)
 
     axh[1].set_ylabel('no. of sample', fontdict=font)
-    axh[1].set_xlabel('time for training a sample , sec.', fontdict=font)
-    sf = np.floor(0.9 * min_lt_sample)
-    sc = np.ceil(1.1 * max_lt_sample)
-    bins_samples_time = np.arange(sf, sc, np.ceil((sc-sf)/50))
+    axh[1].set_xlabel('time for training a batch of samples , sec.', fontdict=font)
+    sf = 0.9 * min_lt_sample
+    sc = 1.1 * max_lt_sample
+    bins_samples_time = np.arange(sf, sc, (sc - sf) / 30)
     axh[1].hist(learning_time_sample, bins=bins_samples_time.tolist())
     # axh[1].hist(learning_time_sample, bins=(0.01 * np.array(list(range(101)))).tolist())
     axh[1].tick_params(axis='both', labelsize='large')
+    props = dict(boxstyle='round', facecolor='wheat')
+    text_str_1 = (" mean time: %.4f sec. \n max time: %.4f sec. \n min time: %.4f sec. "
+                  % (float(mean_lt_sample), float(max_lt_sample), float(min_lt_sample)))
+    axh[1].text(0.55, 0.85, text_str_1, transform=axh[1].transAxes, fontsize=10, verticalalignment='top', bbox=props)
 
     fig_2.canvas.draw()
     fig_2.canvas.flush_events()
Index: .idea/Cuterbot_2004.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<module type=\"PYTHON_MODULE\" version=\"4\">\r\n  <component name=\"NewModuleRootManager\">\r\n    <content url=\"file://$MODULE_DIR$\" />\r\n    <orderEntry type=\"jdk\" jdkName=\"Remote Python 3.8.10 (sftp://cuterbot@192.168.1.113:22/usr/bin/python3.8)\" jdkType=\"Python SDK\" />\r\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\r\n  </component>\r\n  <component name=\"PyDocumentationSettings\">\r\n    <option name=\"format\" value=\"PLAIN\" />\r\n    <option name=\"myDocStringFormat\" value=\"Plain\" />\r\n  </component>\r\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/Cuterbot_2004.iml b/.idea/Cuterbot_2004.iml
--- a/.idea/Cuterbot_2004.iml	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/.idea/Cuterbot_2004.iml	(date 1722847470765)
@@ -2,7 +2,7 @@
 <module type="PYTHON_MODULE" version="4">
   <component name="NewModuleRootManager">
     <content url="file://$MODULE_DIR$" />
-    <orderEntry type="jdk" jdkName="Remote Python 3.8.10 (sftp://cuterbot@192.168.1.113:22/usr/bin/python3.8)" jdkType="Python SDK" />
+    <orderEntry type="jdk" jdkName="Remote Python 3.6.9 (sftp://cuterbot@192.168.1.102:22/usr/bin/python3.6)" jdkType="Python SDK" />
     <orderEntry type="sourceFolder" forTests="false" />
   </component>
   <component name="PyDocumentationSettings">
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"Black\">\r\n    <option name=\"sdkName\" value=\"Remote Python 3.8.10 (sftp://cuterbot@192.168.1.113:22/usr/bin/python3.8)\" />\r\n  </component>\r\n  <component name=\"ProjectRootManager\" version=\"2\" project-jdk-name=\"Remote Python 3.8.10 (sftp://cuterbot@192.168.1.113:22/usr/bin/python3.8)\" project-jdk-type=\"Python SDK\" />\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
--- a/.idea/misc.xml	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/.idea/misc.xml	(date 1722847470866)
@@ -3,5 +3,5 @@
   <component name="Black">
     <option name="sdkName" value="Remote Python 3.8.10 (sftp://cuterbot@192.168.1.113:22/usr/bin/python3.8)" />
   </component>
-  <component name="ProjectRootManager" version="2" project-jdk-name="Remote Python 3.8.10 (sftp://cuterbot@192.168.1.113:22/usr/bin/python3.8)" project-jdk-type="Python SDK" />
+  <component name="ProjectRootManager" version="2" project-jdk-name="Remote Python 3.6.9 (sftp://cuterbot@192.168.1.102:22/usr/bin/python3.6)" project-jdk-type="Python SDK" />
 </project>
\ No newline at end of file
Index: jetbot/road_cruiser_trt.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\r\nimport time\r\n\r\nimport PIL.Image\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torchvision\r\nimport torchvision.transforms as transforms\r\nimport traitlets\r\nfrom traitlets import HasTraits, Unicode, Float\r\n# import torchvision.models as models\r\nfrom torch2trt import TRTModule\r\n\r\nfrom jetbot import Camera\r\nfrom jetbot import Robot\r\n\r\n\r\nclass RoadCruiserTRT(HasTraits):\r\n    cruiser_model = Unicode(default_value='').tag(config=True)\r\n    type_cruiser_model = Unicode(default_value='').tag(config=True)\r\n    speed_rc = Float(default_value=0).tag(config=True)\r\n    speed_gain_rc = Float(default_value=0.15).tag(config=True)\r\n    steering_gain_rc = Float(default_value=0.08).tag(config=True)\r\n    steering_dgain_rc = Float(default_value=1.5).tag(config=True)\r\n    steering_bias_rc = Float(default_value=0.0).tag(config=True)\r\n    steering_rc = Float(default_value=0.0).tag(config=True)\r\n    x_slider = Float(default_value=0).tag(config=True)\r\n    y_slider = Float(default_value=0).tag(config=True)\r\n\r\n    def __init__(self, init_sensor_rc=False):\r\n        super().__init__()\r\n\r\n        self.trt_model_rc = TRTModule()\r\n\r\n        self.robot = None\r\n        self.capturer = None\r\n        if init_sensor_rc:\r\n            self.capturer = Camera()\r\n            self.robot = Robot.instance()\r\n\r\n        self.angle = 0.0\r\n        self.angle_last = 0.0\r\n        self.execution_time = []\r\n        # self.fps = []\r\n        self.x_slider = 0\r\n        self.y_slider = 0\r\n        self.speed_rc = self.speed_gain_rc\r\n\r\n        self.device = torch.device('cuda')\r\n        self.execution_time_rc = []\r\n\r\n    # ---- Creating the Pre-Processing Function\r\n    # 1. Convert from HWC layout to CHW layout\r\n    # 2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\r\n    # 3. Transfer the data from CPU memory to GPU memory\r\n    # 4. Add a batch dimension\r\n    def load_road_cruiser(self, change):\r\n\r\n        \"\"\"\r\n        self.cruiser_model = cruiser_model\r\n        self.type_cruiser_model = type_cruiser_model\r\n        \"\"\"\r\n        # self.road_cruiser = None\r\n        print('path of cruiser model: %s' % self.cruiser_model)\r\n\r\n        if \"workspace\" in self.cruiser_model:\r\n            self.trt_model_rc.load_state_dict(torch.load(self.cruiser_model))\r\n        else:\r\n            self.trt_model_rc.load_state_dict(torch.load('best_steering_model_xy_trt_' + self.cruiser_model + '.pth'))\r\n\r\n    def preprocess_rc(self, image):\r\n        mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\r\n        std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\r\n        # mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\r\n        # std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\r\n        image = PIL.Image.fromarray(image)\r\n        # resize the cam captured image to (224, 224) for optimal resnet model inference\r\n        if self.type_cruiser_model == 'InceptionNet':\r\n            image = image.resize((299, 299))\r\n        elif self.type_cruiser_model == 'ResNet':\r\n            image = image.resize((224, 224))\r\n        image = transforms.functional.to_tensor(image).to(self.device).half()\r\n        # image = transforms.functional.to_tensor(image).to(self.device)\r\n        image.sub_(mean[:, None, None]).div_(std[:, None, None])\r\n        return image[None, ...]\r\n\r\n    def execute_rc(self, change):\r\n        start_time = time.process_time()\r\n        # global angle, angle_last\r\n        image = change['new']\r\n        xy = self.trt_model_rc(self.preprocess_rc(image)).detach().float().cpu().numpy().flatten()\r\n        x = xy[0]\r\n        # y = (0.5 - xy[1]) / 2.0\r\n        y = (1 + xy[1])\r\n\r\n        self.x_slider = x.item()\r\n        self.y_slider = y.item()\r\n\r\n        self.speed_rc = self.speed_gain_rc\r\n\r\n        # angle = np.sqrt(xy)*np.arctan2(x, y)\r\n        angle_1 = np.arctan2(x, y)\r\n        self.angle = 0.5 * np.pi * np.tanh(0.5 * angle_1)\r\n        pid = self.angle * self.steering_gain_rc + (self.angle - self.angle_last) * self.steering_dgain_rc\r\n        self.angle_last = self.angle\r\n\r\n        self.steering_rc = pid + self.steering_bias_rc\r\n\r\n        self.robot.left_motor.value = max(min(self.speed_gain_rc + self.steering_rc, 1.0), 0.0)\r\n        self.robot.right_motor.value = max(min(self.speed_gain_rc - self.steering_rc, 1.0), 0.0)\r\n\r\n        end_time = time.process_time()\r\n        # self.execution_time.append(end_time - start_time + self.camera.cap_time)\r\n        self.execution_time_rc.append(end_time - start_time)\r\n        # self.fps.append(1/(end_time - start_time))\r\n\r\n    # We accomplish that with the observe function.\r\n    def start_rc(self, change):\r\n        # self.execute({'new': self.camera.value})\r\n        self.load_road_cruiser(change)\r\n        self.capturer.observe(self.execute_rc, names='value')\r\n\r\n    def stop_rc(self, change):\r\n        import matplotlib.pyplot as plt\r\n        from jetbot.utils import plot_exec_time\r\n        # self.camera.unobserve(self.execute, names='value')\r\n        self.capturer.unobserve_all()\r\n        time.sleep(1.0)\r\n        self.robot.stop()\r\n        self.capturer.stop()\r\n\r\n        # plot execution time of road cruiser model processing\r\n        model_name = 'road cruiser model'\r\n        cruiser_model_name = self.cruiser_model.split(\"/\")[-1].split('.')[0]\r\n        plot_exec_time(self.execution_time_rc[1:], model_name, cruiser_model_name)\r\n        # plot_exec_time(self.execution_time[1:], self.fps[1:], model_name, self.cruiser_model_str)\r\n        plt.show()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/jetbot/road_cruiser_trt.py b/jetbot/road_cruiser_trt.py
--- a/jetbot/road_cruiser_trt.py	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/jetbot/road_cruiser_trt.py	(date 1722846158466)
@@ -1,15 +1,10 @@
-import os
 import time
-
 import PIL.Image
 
 import numpy as np
 import torch
-import torchvision
 import torchvision.transforms as transforms
-import traitlets
 from traitlets import HasTraits, Unicode, Float
-# import torchvision.models as models
 from torch2trt import TRTModule
 
 from jetbot import Camera
@@ -86,8 +81,8 @@
         return image[None, ...]
 
     def execute_rc(self, change):
-        start_time = time.process_time()
-        # global angle, angle_last
+        start_time = time.time()
+
         image = change['new']
         xy = self.trt_model_rc(self.preprocess_rc(image)).detach().float().cpu().numpy().flatten()
         x = xy[0]
@@ -110,7 +105,7 @@
         self.robot.left_motor.value = max(min(self.speed_gain_rc + self.steering_rc, 1.0), 0.0)
         self.robot.right_motor.value = max(min(self.speed_gain_rc - self.steering_rc, 1.0), 0.0)
 
-        end_time = time.process_time()
+        end_time = time.time()
         # self.execution_time.append(end_time - start_time + self.camera.cap_time)
         self.execution_time_rc.append(end_time - start_time)
         # self.fps.append(1/(end_time - start_time))
@@ -122,9 +117,7 @@
         self.capturer.observe(self.execute_rc, names='value')
 
     def stop_rc(self, change):
-        import matplotlib.pyplot as plt
         from jetbot.utils import plot_exec_time
-        # self.camera.unobserve(self.execute, names='value')
         self.capturer.unobserve_all()
         time.sleep(1.0)
         self.robot.stop()
@@ -134,5 +127,4 @@
         model_name = 'road cruiser model'
         cruiser_model_name = self.cruiser_model.split("/")[-1].split('.')[0]
         plot_exec_time(self.execution_time_rc[1:], model_name, cruiser_model_name)
-        # plot_exec_time(self.execution_time[1:], self.fps[1:], model_name, self.cruiser_model_str)
-        plt.show()
+        # plt.show()
Index: jetbot/road_cruiser.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import time\r\n\r\nimport PIL.Image\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torchvision\r\nimport torchvision.transforms as transforms\r\nimport traitlets\r\nfrom traitlets import HasTraits, Float, Unicode\r\nimport torchvision.models as pth_models\r\n\r\nfrom jetbot import Camera\r\nfrom jetbot import Robot\r\n\r\n\r\ndef load_tune_pth_model(pth_model_name=\"resnet18\", pretrained=True):\r\n    if pretrained:\r\n        model = getattr(pth_models, pth_model_name)()\r\n    else:\r\n        model = getattr(pth_models, pth_model_name)(weights=False)\r\n    # ----- modify last layer for classification, and the model used in notebook should be modified too.\r\n\r\n    if pth_model_name == 'mobilenet_v3_large':  # MobileNet\r\n        model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features,\r\n                                              2)  # for mobilenet_v3 model. must add the block expansion factor 4\r\n\r\n    elif pth_model_name == 'mobilenet_v2':\r\n        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features,\r\n                                              2)  # for mobilenet_v2 model. must add the block expansion factor 4\r\n\r\n    elif pth_model_name == 'vgg11':  # VGGNet\r\n        model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features,\r\n                                              2)  # for VGG model. must add the block expansion factor 4\r\n\r\n    elif 'resnet' in pth_model_name:  # ResNet\r\n        model.fc = torch.nn.Linear(model.fc.in_features,\r\n                                   2)  # for resnet model must add the block expansion factor 4\r\n        # model.fc = torch.nn.Linear(512, 2)\r\n\r\n    elif pth_model_name == 'inception_v3':  # Inception_v3\r\n        model.fc = torch.nn.Linear(model.fc.in_features, 2)\r\n        if model.aux_logits:\r\n            model.AuxLogits.fc = torch.nn.Linear(model.AuxLogits.fc.in_features, 2)\r\n\r\n    return model\r\n\r\n\r\nclass RoadCruiser(HasTraits):\r\n    cruiser_model = Unicode(default_value='').tag(config=True)\r\n    type_cruiser_model = Unicode(default_value='').tag(config=True)\r\n    speed_gain = Float(default_value=0.15).tag(config=True)\r\n    steering_gain = Float(default_value=0.08).tag(config=True)\r\n    steering_dgain = Float(default_value=1.5).tag(config=True)\r\n    steering_bias = Float(default_value=0.0).tag(config=True)\r\n    steering = Float(default_value=0.0).tag(config=True)\r\n    x_slider = Float(default_value=0).tag(config=True)\r\n    y_slider = Float(default_value=0).tag(config=True)\r\n    speed = Float(default_value=0).tag(config=True)\r\n    use_gpu = Unicode(default_value='gpu').tag(config=True)\r\n\r\n    def __init__(self, init_sensor_rc=False):\r\n        super().__init__()\r\n\r\n        self.cruiser_model_pth = None\r\n\r\n        if init_sensor_rc:\r\n            self.capturer = Camera()\r\n            self.robot = Robot.instance()\r\n        # self.robot = Robot()\r\n        self.angle = 0.0\r\n        self.angle_last = 0.0\r\n        # self.fps = []\r\n        self.x_slider = 0\r\n        self.y_slider = 0\r\n\r\n        self.execution_time_rc = []\r\n        self.observe(self.select_gpu, names=['use_gpu'])\r\n        self.device = None\r\n\r\n    def load_road_cruiser(self, change):\r\n        # The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\r\n        self.cruiser_model_pth = None\r\n        pth_model_name = self.cruiser_model.split('/')[-1].split('.')[0].split('_', 4)[-1].split('-')[0]\r\n        print('pytorch model name: %s' % pth_model_name)\r\n        self.cruiser_model_pth = load_tune_pth_model(pth_model_name=pth_model_name, pretrained=False)\r\n\r\n        print('path of cruiser model: %s' % self.cruiser_model)\r\n        print('use %s' % self.use_gpu)\r\n        # self.cruiser_model.load_state_dict(torch.load('best_steering_model_xy_' + cruiser_model + '.pth'))\r\n        self.cruiser_model_pth.load_state_dict(torch.load(self.cruiser_model))\r\n\r\n        if self.use_gpu == 'gpu':\r\n            print(\"torch cuda version : \", torch.version.cuda)\r\n            print(\"cuda is available for pytorch: \", torch.cuda.is_available())\r\n            self.device = torch.device('cuda')\r\n            self.cruiser_model_pth.to(self.device)\r\n            self.cruiser_model_pth.eval().half()\r\n\r\n        elif self.use_gpu == 'cpu':\r\n            self.device = torch.device('cpu')\r\n            self.cruiser_model_pth.to(self.device)\r\n            self.cruiser_model_pth.eval()\r\n\r\n        # self.cruiser_model = self.cruiser_model.float()\r\n        # self.cruiser_model = self.cruiser_model.to(self.device, dtype=torch.float)\r\n        # self.cruiser_model = self.cruiser_model.eval()\r\n\r\n    def select_gpu(self, change):\r\n        self.use_gpu = change['new']\r\n\r\n    # ---- Creating the Pre-Processing Function\r\n    # 1. Convert from HWC layout to CHW layout\r\n    # 2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\r\n    # 3. Transfer the data from CPU memory to GPU memory\r\n    # 4. Add a batch dimension\r\n\r\n    def preprocess_rc(self, image):\r\n        mean = None\r\n        std = None\r\n        if self.use_gpu == 'gpu':\r\n            mean = torch.Tensor([0.485, 0.456, 0.406]).to(self.device).half()\r\n            std = torch.Tensor([0.229, 0.224, 0.225]).to(self.device).half()\r\n        elif self.use_gpu == 'cpu':\r\n            mean = torch.Tensor([0.485, 0.456, 0.406]).to(self.device)\r\n            std = torch.Tensor([0.229, 0.224, 0.225]).to(self.device)\r\n        # mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\r\n        # std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\r\n        image = PIL.Image.fromarray(image)\r\n        # resize the cam captured image to (224, 224) for optimal resnet model inference\r\n        if self.type_cruiser_model == 'inception':\r\n            image = image.resize((299, 299))\r\n        else:\r\n            image = image.resize((224, 224))\r\n\r\n        if self.use_gpu == 'gpu':\r\n            image = transforms.functional.to_tensor(image).to(self.device).half()\r\n        elif self.use_gpu == 'cpu':\r\n            image = transforms.functional.to_tensor(image).to(self.device)\r\n\r\n        image.sub_(mean[:, None, None]).div_(std[:, None, None])\r\n        return image[None, ...]\r\n\r\n    def execute_rc(self, change):\r\n        start_time = time.process_time()\r\n        # global angle, angle_last\r\n        image = change['new']\r\n        xy = self.cruiser_model_pth(self.preprocess_rc(image)).detach().float().cpu().numpy().flatten()\r\n        x = xy[0]\r\n        # y = (0.5 - xy[1]) / 2.0\r\n        y = (1 + xy[1])\r\n\r\n        self.x_slider = x.item()\r\n        self.y_slider = y.item()\r\n\r\n        self.speed = self.speed_gain\r\n\r\n        # angle = np.sqrt(xy)*np.arctan2(x, y)\r\n        angle_1 = np.arctan2(x, y)\r\n        self.angle = 0.5 * np.pi * np.tanh(0.5 * angle_1)\r\n        pid = self.angle * self.steering_gain + (self.angle - self.angle_last) * self.steering_dgain\r\n        self.angle_last = self.angle\r\n\r\n        self.steering = pid + self.steering_bias\r\n\r\n        self.robot.left_motor.value = max(min(self.speed_gain + self.steering, 1.0), 0.0)\r\n        self.robot.right_motor.value = max(min(self.speed_gain - self.steering, 1.0), 0.0)\r\n\r\n        end_time = time.process_time()\r\n        # self.execution_time.append(end_time - start_time + self.camera.cap_time)\r\n        self.execution_time_rc.append(end_time - start_time)\r\n        # self.fps.append(1/(end_time - start_time))\r\n\r\n    # We accomplish that with the observe function.\r\n    def start_rc(self, change):\r\n        # self.execute({'new': self.camera.value})\r\n        self.load_road_cruiser(change)\r\n        self.capturer.observe(self.execute_rc, names='value')\r\n\r\n    def stop_rc(self, change):\r\n        import matplotlib.pyplot as plt\r\n        from jetbot.utils import plot_exec_time\r\n        # self.camera.unobserve(self.execute, names='value')\r\n        self.capturer.unobserve_all()\r\n        time.sleep(1.0)\r\n        self.robot.stop()\r\n        self.capturer.stop()\r\n\r\n        # plot execution time of road cruiser model processing\r\n        model_name = \"road cruiser model\"\r\n        cruiser_model_str = self.cruiser_model.split(\"/\")[-1].split('.')[0]\r\n        plot_exec_time(self.execution_time_rc[1:], model_name, cruiser_model_str)\r\n        # plot_exec_time(self.execution_time[1:], self.fps[1:], model_name, self.cruiser_model_str)\r\n        plt.show()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/jetbot/road_cruiser.py b/jetbot/road_cruiser.py
--- a/jetbot/road_cruiser.py	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/jetbot/road_cruiser.py	(date 1722782695360)
@@ -4,64 +4,31 @@
 
 import numpy as np
 import torch
-import torchvision
 import torchvision.transforms as transforms
-import traitlets
 from traitlets import HasTraits, Float, Unicode
-import torchvision.models as pth_models
 
 from jetbot import Camera
 from jetbot import Robot
-
-
-def load_tune_pth_model(pth_model_name="resnet18", pretrained=True):
-    if pretrained:
-        model = getattr(pth_models, pth_model_name)()
-    else:
-        model = getattr(pth_models, pth_model_name)(weights=False)
-    # ----- modify last layer for classification, and the model used in notebook should be modified too.
-
-    if pth_model_name == 'mobilenet_v3_large':  # MobileNet
-        model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features,
-                                              2)  # for mobilenet_v3 model. must add the block expansion factor 4
-
-    elif pth_model_name == 'mobilenet_v2':
-        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features,
-                                              2)  # for mobilenet_v2 model. must add the block expansion factor 4
-
-    elif pth_model_name == 'vgg11':  # VGGNet
-        model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features,
-                                              2)  # for VGG model. must add the block expansion factor 4
-
-    elif 'resnet' in pth_model_name:  # ResNet
-        model.fc = torch.nn.Linear(model.fc.in_features,
-                                   2)  # for resnet model must add the block expansion factor 4
-        # model.fc = torch.nn.Linear(512, 2)
-
-    elif pth_model_name == 'inception_v3':  # Inception_v3
-        model.fc = torch.nn.Linear(model.fc.in_features, 2)
-        if model.aux_logits:
-            model.AuxLogits.fc = torch.nn.Linear(model.AuxLogits.fc.in_features, 2)
-
-    return model
+from jetbot.utils.model_selection import load_tune_pth_model
 
 
 class RoadCruiser(HasTraits):
     cruiser_model = Unicode(default_value='').tag(config=True)
     type_cruiser_model = Unicode(default_value='').tag(config=True)
-    speed_gain = Float(default_value=0.15).tag(config=True)
-    steering_gain = Float(default_value=0.08).tag(config=True)
-    steering_dgain = Float(default_value=1.5).tag(config=True)
-    steering_bias = Float(default_value=0.0).tag(config=True)
-    steering = Float(default_value=0.0).tag(config=True)
+    speed_rc = Float(default_value=0).tag(config=True)
+    speed_gain_rc = Float(default_value=0.15).tag(config=True)
+    steering_gain_rc = Float(default_value=0.08).tag(config=True)
+    steering_dgain_rc = Float(default_value=1.5).tag(config=True)
+    steering_bias_rc = Float(default_value=0.0).tag(config=True)
+    steering_rc = Float(default_value=0.0).tag(config=True)
     x_slider = Float(default_value=0).tag(config=True)
     y_slider = Float(default_value=0).tag(config=True)
-    speed = Float(default_value=0).tag(config=True)
     use_gpu = Unicode(default_value='gpu').tag(config=True)
 
     def __init__(self, init_sensor_rc=False):
         super().__init__()
 
+        self.cruiser_model_type_pth = None
         self.cruiser_model_pth = None
 
         if init_sensor_rc:
@@ -81,9 +48,11 @@
     def load_road_cruiser(self, change):
         # The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
         self.cruiser_model_pth = None
+        self.cruiser_model_type_pth = None
+
         pth_model_name = self.cruiser_model.split('/')[-1].split('.')[0].split('_', 4)[-1].split('-')[0]
         print('pytorch model name: %s' % pth_model_name)
-        self.cruiser_model_pth = load_tune_pth_model(pth_model_name=pth_model_name, pretrained=False)
+        self.cruiser_model_pth, self.cruiser_model_type_pth = load_tune_pth_model(pth_model_name=pth_model_name, pretrained=False)
 
         print('path of cruiser model: %s' % self.cruiser_model)
         print('use %s' % self.use_gpu)
@@ -142,7 +111,7 @@
         return image[None, ...]
 
     def execute_rc(self, change):
-        start_time = time.process_time()
+        start_time = time.time()
         # global angle, angle_last
         image = change['new']
         xy = self.cruiser_model_pth(self.preprocess_rc(image)).detach().float().cpu().numpy().flatten()
@@ -153,21 +122,20 @@
         self.x_slider = x.item()
         self.y_slider = y.item()
 
-        self.speed = self.speed_gain
+        self.speed_rc = self.speed_gain_rc
 
         # angle = np.sqrt(xy)*np.arctan2(x, y)
         angle_1 = np.arctan2(x, y)
         self.angle = 0.5 * np.pi * np.tanh(0.5 * angle_1)
-        pid = self.angle * self.steering_gain + (self.angle - self.angle_last) * self.steering_dgain
+        pid = self.angle * self.steering_gain_rc + (self.angle - self.angle_last) * self.steering_dgain_rc
         self.angle_last = self.angle
 
-        self.steering = pid + self.steering_bias
+        self.steering = pid + self.steering_bias_rc
 
-        self.robot.left_motor.value = max(min(self.speed_gain + self.steering, 1.0), 0.0)
-        self.robot.right_motor.value = max(min(self.speed_gain - self.steering, 1.0), 0.0)
+        self.robot.left_motor.value = max(min(self.speed_gain_rc + self.steering, 1.0), 0.0)
+        self.robot.right_motor.value = max(min(self.speed_gain_rc - self.steering, 1.0), 0.0)
 
-        end_time = time.process_time()
-        # self.execution_time.append(end_time - start_time + self.camera.cap_time)
+        end_time = time.time()
         self.execution_time_rc.append(end_time - start_time)
         # self.fps.append(1/(end_time - start_time))
 
@@ -178,7 +146,6 @@
         self.capturer.observe(self.execute_rc, names='value')
 
     def stop_rc(self, change):
-        import matplotlib.pyplot as plt
         from jetbot.utils import plot_exec_time
         # self.camera.unobserve(self.execute, names='value')
         self.capturer.unobserve_all()
@@ -190,5 +157,4 @@
         model_name = "road cruiser model"
         cruiser_model_str = self.cruiser_model.split("/")[-1].split('.')[0]
         plot_exec_time(self.execution_time_rc[1:], model_name, cruiser_model_str)
-        # plot_exec_time(self.execution_time[1:], self.fps[1:], model_name, self.cruiser_model_str)
-        plt.show()
+        # plt.show()
Index: jetbot/object_follower.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# # Object Following - Live Demo\r\n# \r\n# In this notebook we'll show how you can follow an object with JetBot!  We'll use a pre-trained neural network\r\n# that was trained on the [COCO dataset](http://cocodataset.org) to detect 90 different common objects.  These include\r\n# \r\n# * Person (index 0)\r\n# * Cup (index 47)\r\n# \r\n# and many others (you can check [this file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) for a full list of class indices).  The model is sourced from the [TensorFlow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection),\r\n# which provides utilities for training object detectors for custom tasks also!  Once the model is trained, we optimize it using NVIDIA TensorRT on the Jetson Nano.\r\n# \r\n# This makes the network very fast, capable of real-time execution on Jetson Nano!  We won't run through all of the training and optimization steps in this notebook though.\r\n# \r\n# Anyways, let's get started.  First, we'll want to import the ``ObjectDetector`` class which takes our pre-trained SSD engine.\r\n\r\n# ### Compute detections on single camera image\r\n\r\n# In[ ]:\r\n\r\nfrom queue import Empty\r\nimport torch\r\nimport torchvision\r\nimport torch.nn.functional as F\r\nimport cv2\r\nimport numpy as np\r\nimport traitlets\r\nimport time\r\n\r\n# from jetbot import ObjectDetector\r\n# from jetbot.object_detection_yolo import ObjectDetector_YOLO\r\nfrom jetbot import Camera\r\nfrom jetbot import Robot\r\nfrom jetbot import bgr8_to_jpeg\r\nfrom jetbot.utils import get_cls_dict_yolo, get_cls_dict_ssd\r\nimport time\r\nfrom jetbot import ObjectDetector\r\n\r\n\r\n# model = ObjectDetector('ssd_mobilenet_v2_coco_onnx.engine')\r\n# model = ObjectDetector_YOLO('yolov4-288.engine')\r\n\r\n\r\n# class ObjectFollower(traitlets.HasTraits):\r\ndef norm(vec):\r\n    \"\"\"Computes the length of the 2D vector\"\"\"\r\n    return np.sqrt(vec[0] ** 2 + vec[1] ** 2)\r\n\r\n\r\ndef object_center_detection(det):\r\n    \"\"\"Computes the center x, y coordinates of the object\"\"\"\r\n    # print(self.matching_detections)\r\n    bbox = det['bbox']\r\n    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\r\n    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\r\n    object_center = (center_x, center_y)\r\n    return object_center\r\n\r\n\r\nclass ObjectFollower(ObjectDetector):\r\n    follower_model = traitlets.Unicode(default_value='').tag(config=True)\r\n    type_follower_model = traitlets.Unicode(default_value='').tag(config=True)\r\n    conf_th = traitlets.Float(default_value=0.5).tag(config=True)\r\n    cap_image = traitlets.Any()\r\n    label = traitlets.Integer(default_value=1).tag(config=True)\r\n    label_text = traitlets.Unicode(default_value='').tag(config=True)\r\n    speed_of = traitlets.Float(default_value=0).tag(config=True)\r\n    speed_gain_of = traitlets.Float(default_value=0.15).tag(config=True)\r\n    turn_gain_of = traitlets.Float(default_value=0.3).tag(config=True)\r\n    steering_bias_of = traitlets.Float(default_value=0.0).tag(config=True)\r\n    blocked = traitlets.Float(default_value=0).tag(config=True)\r\n    is_detecting = traitlets.Bool(default_value=True).tag(config=True)\r\n\r\n    def __init__(self, init_sensor_of=False):\r\n\r\n        super().__init__()\r\n        self.detections = None\r\n        self.matching_detections = None\r\n        self.object_center = None\r\n        self.closest_object = None\r\n        self.speed_of = self.speed_gain_of\r\n        # self.is_detecting = True\r\n\r\n        self.robot = None\r\n        self.capturer = None\r\n        if init_sensor_of:\r\n            self.robot = Robot.instance()\r\n            # Camera instance would be better to put after all models instantiation\r\n            self.capturer = Camera()\r\n            self.img_width = self.capturer.width\r\n            self.img_height = self.capturer.height\r\n            self.cap_image = np.empty(shape=(self.img_height, self.img_width, 3), dtype=np.uint8).tobytes()\r\n            self.current_image = np.empty((self.img_height, self.img_width, 3))\r\n\r\n        self.execution_time_of = []\r\n        # self.fps = []\r\n\r\n    def load_object_detector(self, change):\r\n\r\n        # self.object_detector = None\r\n        self.engine_path = self.follower_model  # set the engine_path in object detection module\r\n        self.type_model_od = self.type_follower_model  # set the type_model in object detection module\r\n\r\n        # avoider_model='../collision_avoidance/best_model.pth'\r\n        # self.obstacle_detector = Avoider(model_params=self.avoider_model)\r\n        print('path of object detector model: %s' % self.follower_model)\r\n        self.load_od_engine()  # load object detection engine function in object detection module\r\n\r\n    def run_objects_detection(self):\r\n        # self.image = self.capturer.value\r\n        # print(self.image[1][1], np.shape(self.image))\r\n        self.detections = self.execute_od(self.current_image)  # function in object detection for executing trt engine\r\n        self.matching_detections = [d for d in self.detections[0] if d['label'] == int(self.label)]\r\n        if int(self.label) >= 0:\r\n            if self.type_follower_model == \"SSD\" or self.type_follower_model == \"SSD_FPN\":\r\n                self.label_text = get_cls_dict_ssd('coco')[int(self.label)]\r\n            elif self.type_follower_model == \"YOLO\" or self.type_follower_model == \"YOLO_v7\":\r\n                self.label_text = get_cls_dict_yolo('coco')[int(self.label)]\r\n        else:\r\n            self.label_text = \" Not defined !\"\r\n\r\n        # print(int(self.label), \"\\n\", self.matching_detections)\r\n\r\n    def closest_object_detection(self):\r\n        \"\"\"Finds the detection closest to the image center\"\"\"\r\n        closest_detection = None\r\n        if len(self.matching_detections) != 0:\r\n            for det in self.matching_detections:\r\n                if closest_detection is None:\r\n                    closest_detection = det\r\n                elif norm(object_center_detection(det)) < norm(\r\n                        object_center_detection(closest_detection)):\r\n                    closest_detection = det\r\n\r\n        self.closest_object = closest_detection\r\n\r\n    def start_of(self, change):\r\n        self.load_object_detector(change)\r\n        self.capturer.unobserve_all()\r\n        print(\"start running\")\r\n        self.capturer.observe(self.execute_of, names='value')\r\n\r\n    def execute_of(self, change):\r\n        # print(\"start execution !\")\r\n        start_time = time.process_time()\r\n\r\n        self.current_image = change['new']\r\n        # width = self.img_width\r\n        # height = self.img_height\r\n\r\n        # print(image)\r\n        # ** execute collision model to determine if blocked\r\n        # self.obstacle_detector.detect(self.current_image)\r\n        # self.blocked = self.obstacle_detector.prob_blocked\r\n        # turn left if blocked\r\n        if self.blocked > 0.5:\r\n            #      # robot.left(0.3)\r\n            self.robot.left(0.05)\r\n            self.cap_image = bgr8_to_jpeg(self.current_image)\r\n            return\r\n\r\n        # compute all detected objects\r\n        self.run_objects_detection()\r\n        self.closest_object_detection()\r\n        # detections = self.object_detector(image)\r\n        # print(self.detections)\r\n\r\n        self.speed_of = self.speed_gain_of\r\n\r\n        # draw all detections on image\r\n        for det in self.detections[0]:\r\n            bbox = det['bbox']\r\n            cv2.rectangle(self.current_image, (int(self.img_width * bbox[0]), int(self.img_height * bbox[1])),\r\n                          (int(self.img_width * bbox[2]), int(self.img_height * bbox[3])), (255, 0, 0), 2)\r\n\r\n        # select detections that match selected class label\r\n\r\n        # get detection closest to center of field of view and draw it\r\n        cls_obj = self.closest_object\r\n        if cls_obj is not None:\r\n            bbox = cls_obj['bbox']\r\n            cv2.rectangle(self.current_image, (int(self.img_width * bbox[0]), int(self.img_height * bbox[1])),\r\n                          (int(self.img_width * bbox[2]), int(self.img_height * bbox[3])), (0, 255, 0), 5)\r\n\r\n        # otherwise go forward if no target detected\r\n        if cls_obj is None:\r\n            self.robot.forward(float(self.speed_gain_of))\r\n\r\n        # otherwise steer towards target\r\n        else:\r\n            # move robot forward and steer proportional target's x-distance from center\r\n            center = object_center_detection(cls_obj)\r\n            self.robot.set_motors(\r\n                float(self.speed_gain_of + self.turn_gain_of * center[0] + self.steering_bias_of),\r\n                float(self.speed_gain_of - self.turn_gain_of * center[0] + self.steering_bias_of)\r\n            )\r\n\r\n        end_time = time.process_time()\r\n        # self.execution_time.append(end_time - start_time + self.capturer.cap_time)\r\n        self.execution_time_of.append(end_time - start_time)\r\n        # self.fps.append(1/(end_time - start_time))\r\n\r\n        # update image widget\r\n        # image_widget.value = bgr8_to_jpeg(image)\r\n        self.cap_image = bgr8_to_jpeg(self.current_image)\r\n        # print(\"ok!\")\r\n        # return self.cap_image\r\n\r\n    def stop_of(self, change):\r\n        import matplotlib.pyplot as plt\r\n        from jetbot.utils import plot_exec_time\r\n        print(\"stop running!\")\r\n        self.capturer.unobserve_all()\r\n        time.sleep(1.0)\r\n        self.robot.stop()\r\n        self.capturer.stop()\r\n\r\n        # plot execution time of object follower model processing\r\n        model_name = \"object follower model\"\r\n        model_name_str = self.follower_model.split('/')[-1].split('.')[0]\r\n        plot_exec_time(self.execution_time_of[1:], model_name, model_name_str)\r\n        # plot_exec_time(self.execution_time[1:], self.fps[1:], model_name, self.follower_model.split('.')[0])\r\n        plt.show()\r\n\r\n\r\nclass Avoider(object):\r\n\r\n    def __init__(self, model_params='../collision_avoidance/best_model.pth'):\r\n        self.model_params = model_params\r\n        self.collision_model = torchvision.models.alexnet(pretrained=False)\r\n        self.collision_model.classifier[6] = torch.nn.Linear(self.collision_model.classifier[6].in_features, 2)\r\n        self.collision_model.load_state_dict(torch.load(self.model_params))\r\n        # collision_model.load_state_dict(torch.load('../collision_avoidance/best_model.pth'))\r\n        self.device = torch.device('cuda')\r\n        self.collision_model = self.collision_model.to(self.device)\r\n        self.prob_blocked = 0\r\n\r\n    def detect(self, image):\r\n        collision_output = self.collision_model(self.preprocess(image)).detach().cpu()\r\n        self.prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\r\n        # blocked_widget.value = prob_blocked\r\n\r\n    def preprocess(self, camera_value):\r\n        # global device\r\n        mean = 255.0 * np.array([0.485, 0.456, 0.406])\r\n        stdev = 255.0 * np.array([0.229, 0.224, 0.225])\r\n        normalize = torchvision.transforms.Normalize(mean, stdev)\r\n        x = camera_value\r\n        x = cv2.resize(x, (224, 224))\r\n        x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\r\n        x = x.transpose((2, 0, 1))\r\n        x = torch.from_numpy(x).float()\r\n        x = normalize(x)\r\n        x = x.to(self.device)\r\n        x = x[None, ...]\r\n        return x\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/jetbot/object_follower.py b/jetbot/object_follower.py
--- a/jetbot/object_follower.py	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/jetbot/object_follower.py	(date 1722533435648)
@@ -27,10 +27,7 @@
 import cv2
 import numpy as np
 import traitlets
-import time
 
-# from jetbot import ObjectDetector
-# from jetbot.object_detection_yolo import ObjectDetector_YOLO
 from jetbot import Camera
 from jetbot import Robot
 from jetbot import bgr8_to_jpeg
@@ -144,7 +141,7 @@
 
     def execute_of(self, change):
         # print("start execution !")
-        start_time = time.process_time()
+        start_time = time.time()
 
         self.current_image = change['new']
         # width = self.img_width
@@ -197,7 +194,7 @@
                 float(self.speed_gain_of - self.turn_gain_of * center[0] + self.steering_bias_of)
             )
 
-        end_time = time.process_time()
+        end_time = time.time()
         # self.execution_time.append(end_time - start_time + self.capturer.cap_time)
         self.execution_time_of.append(end_time - start_time)
         # self.fps.append(1/(end_time - start_time))
@@ -209,7 +206,6 @@
         # return self.cap_image
 
     def stop_of(self, change):
-        import matplotlib.pyplot as plt
         from jetbot.utils import plot_exec_time
         print("stop running!")
         self.capturer.unobserve_all()
@@ -222,7 +218,7 @@
         model_name_str = self.follower_model.split('/')[-1].split('.')[0]
         plot_exec_time(self.execution_time_of[1:], model_name, model_name_str)
         # plot_exec_time(self.execution_time[1:], self.fps[1:], model_name, self.follower_model.split('.')[0])
-        plt.show()
+        # plt.show()
 
 
 class Avoider(object):
Index: jetbot/fleet_manager_trt.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# # Object Following - Live Demo\r\n# \r\n# In this notebook we'll show how you can follow an object with JetBot!  We'll use a pre-trained neural network\r\n# that was trained on the [COCO dataset](http://cocodataset.org) to detect 90 different common objects.  These include\r\n# \r\n# * Person (index 0)\r\n# * Cup (index 47)\r\n# \r\n# and many others (you can check [this file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) for a full list of class indices).  The model is sourced from the [TensorFlow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection),\r\n# which provides utilities for training object detectors for custom tasks also!  Once the model is trained, we optimize it using NVIDIA TensorRT on the Jetson Nano.\r\n# \r\n# This makes the network very fast, capable of real-time execution on Jetson Nano!  We won't run through all of the training and optimization steps in this notebook though.\r\n# \r\n# Anyways, let's get started.  First, we'll want to import the ``ObjectDetector`` class which takes our pre-trained SSD engine.\r\n\r\n# ### Compute detections on single camera image\r\n\r\n# In[ ]:\r\n\r\nfrom queue import Empty\r\nimport torch.nn.functional as F\r\nimport cv2\r\nimport numpy as np\r\nimport traitlets\r\nfrom traitlets import HasTraits, Unicode, Float, Integer, Bool, Any\r\nimport os\r\nimport time\r\n\r\n# from jetbot import ObjectDetector\r\n# from jetbot.object_detection_yolo import ObjectDetector_YOLO\r\nfrom jetbot import Camera\r\nfrom jetbot import Robot\r\nfrom jetbot import bgr8_to_jpeg\r\nfrom jetbot import ObjectFollower\r\nfrom jetbot import RoadCruiserTRT\r\nfrom jetbot.utils import get_cls_dict_yolo, get_cls_dict_ssd\r\n\r\nimport time\r\n\r\n\r\ndef norm(vec):\r\n    \"\"\"Computes the length of the 2D vector\"\"\"\r\n    return np.sqrt(vec[0] ** 2 + vec[1] ** 2)\r\n\r\n\r\ndef object_center_detection(det):\r\n    \"\"\"Computes the center x, y coordinates of the object\"\"\"\r\n    # print(self.matching_detections)\r\n    bbox = det['bbox']\r\n    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\r\n    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\r\n    object_center = (center_x, center_y)\r\n    return object_center\r\n\r\n\r\nclass FleeterTRT(ObjectFollower, RoadCruiserTRT):\r\n    cap_image = Any()\r\n    # model parameters\r\n    # follower_model = Unicode(default_value='').tag(config=True)\r\n    # type_follower_model = Unicode(default_value='').tag(config=True)\r\n    # cruiser_model = Unicode(default_value='').tag(config=True)\r\n    # type_cruiser_model = Unicode(default_value='').tag(config=True)\r\n    conf_th = Float(default_value=0.5).tag(config=True)\r\n    # label = Integer(default_value=1).tag(config=True)\r\n    # label_text = Unicode(default_value='').tag(config=True)\r\n    # control parameters\r\n    # speed_rc = Float(default_value=0).tag(config=True)\r\n    speed_fm = Float(default_value=0.10).tag(config=True)\r\n    speed_gain_fm = Float(default_value=0.01).tag(config=True)\r\n    speed_dev_fm = Float(default_value=0.5).tag(config=True)\r\n    turn_gain_fm = Float(default_value=0.3).tag(config=True)\r\n    steering_bias_fm = Float(default_value=0.0).tag(config=True)\r\n    # blocked = Float(default_value=0).tag(config=True)\r\n    target_view = Float(default_value=0.6).tag(config=True)\r\n    mean_view = Float(default_value=0).tag(config=True)\r\n    e_view = Float(default_value=0).tag(config=True)\r\n    # is_detecting = Bool(default_value=True).tag(config=True)\r\n    is_detected = Bool(default_value=False).tag(config=True)\r\n\r\n    def __init__(self, init_sensor_fm=False):\r\n\r\n        # the parent classes (ObjectFollower, RoadCruiserTRT) may revisit during initialization,\r\n        # causing the  re-instantiation error of camera and robot motor,\r\n        # which should be avoided when design the parent classes\r\n        ObjectFollower.__init__(self, init_sensor_of=False)\r\n        RoadCruiserTRT.__init__(self, init_sensor_rc=False)\r\n\r\n        self.detections = None\r\n        self.matching_detections = None\r\n        self.object_center = None\r\n        self.closest_object = None\r\n        self.is_detecting = True\r\n        self.is_detected = False\r\n        self.is_loaded = False\r\n\r\n        self.robot = None\r\n        self.capturer = None\r\n        if init_sensor_fm:\r\n            self.robot = Robot.instance()\r\n            self.capturer = Camera()\r\n            self.img_width = self.capturer.width\r\n            self.img_height = self.capturer.height\r\n            self.cap_image = np.empty(shape=(self.img_height, self.img_width, 3), dtype=np.uint8).tobytes()\r\n            self.current_image = np.empty((self.img_height, self.img_width, 3))\r\n\r\n        self.default_speed = self.speed_fm\r\n        self.detect_duration_max = 10\r\n        self.no_detect = 0\r\n        self.target_view = 0.5\r\n        self.mean_view = 0\r\n        self.mean_view_prev = 0\r\n        self.e_view = 0\r\n        self.e_view_prev = 0\r\n\r\n        self.execution_time_fm = []\r\n        # self.fps = []\r\n\r\n    def execute_fm(self, change):\r\n\r\n        # do object following\r\n        start_time = time.process_time()\r\n        self.execute(change)\r\n        end_time = time.process_time()\r\n        # self.execution_time.append(end_time - start_time + self.capturer.cap_time)\r\n        self.execution_time_fm.append(end_time - start_time)\r\n        # self.fps.append(1/(end_time - start_time))\r\n\r\n        # if closest object is not detected and followed, do road cruising\r\n        if not self.is_detected:\r\n            self.execute_rc(change)\r\n            self.speed_fm = self.speed_rc  # set fleet mge speed to road cruising speed (self.speed)\r\n\r\n    def start_fm(self, change):\r\n        self.load_object_detector(change)  # load object detector function in object follower module\r\n        self.load_road_cruiser(change)  # load_road_cruiser function in road_cruiser_trt module\r\n        self.capturer.unobserve_all()\r\n        print(\"start running\")\r\n        self.capturer.observe(self.execute_fm, names='value')\r\n\r\n    def execute(self, change):\r\n        # print(\"start execution !\")\r\n        self.current_image = change['new']\r\n\r\n        # compute all detected objects\r\n        self.run_objects_detection()\r\n        self.closest_object_detection()\r\n        # detections = self.object_detector(image)\r\n        # print(self.detections)\r\n\r\n        # draw all detections on image\r\n        for det in self.detections[0]:\r\n            bbox = det['bbox']\r\n            cv2.rectangle(self.current_image, (int(self.img_width * bbox[0]), int(self.img_height * bbox[1])),\r\n                          (int(self.img_width * bbox[2]), int(self.img_height * bbox[3])), (255, 0, 0), 2)\r\n\r\n        # select detections that match selected class label\r\n        # get detection closest to center of field of view and draw it\r\n        cls_obj = self.closest_object\r\n        if cls_obj is not None:\r\n            self.is_detected = True\r\n            self.no_detect = self.detect_duration_max  # set max detection no to prevent temporary loss of object detection\r\n            bbox = cls_obj['bbox']\r\n            cv2.rectangle(self.current_image, (int(self.img_width * bbox[0]), int(self.img_height * bbox[1])),\r\n                          (int(self.img_width * bbox[2]), int(self.img_height * bbox[3])), (0, 255, 0), 5)\r\n\r\n            self.mean_view = 0.8 * (bbox[2] - bbox[0]) + 0.2 * self.mean_view_prev\r\n            self.e_view = self.target_view - self.mean_view\r\n            if np.abs(self.e_view / self.target_view) > 0.1:\r\n                self.speed_fm = self.speed_fm + self.speed_gain_fm * self.e_view + self.speed_dev_fm * (\r\n                        self.e_view - self.e_view_prev)\r\n            # self.speed = self.speed_fm\r\n\r\n            self.mean_view_prev = self.mean_view\r\n            self.e_view_prev = self.e_view\r\n\r\n        # otherwise go forward if no target detected\r\n        if cls_obj is None:\r\n            if self.no_detect <= 0:  # if object is not detected for a duration, road cruising\r\n                self.mean_view = 0.0\r\n                self.mean_view_prev = 0.0\r\n                self.is_detected = False\r\n                self.cap_image = bgr8_to_jpeg(self.current_image)\r\n                return\r\n            else:\r\n                self.no_detect -= 1  # observe for a duration for the miss of object detection\r\n            # self.robot.forward(float(self.speed))\r\n\r\n        # otherwise steer towards target\r\n        else:\r\n            # move robot forward and steer proportional target's x-distance from center\r\n            center = object_center_detection(cls_obj)\r\n            self.robot.set_motors(\r\n                float(self.speed_fm + self.turn_gain_fm * center[0] + self.steering_bias_fm),\r\n                float(self.speed_fm - self.turn_gain_fm * center[0] + self.steering_bias_fm)\r\n            )\r\n\r\n        # update image widget\r\n        self.cap_image = bgr8_to_jpeg(self.current_image)\r\n        # print(\"ok!\")\r\n        # return self.cap_image\r\n\r\n    def stop_fm(self, change):\r\n        import matplotlib.pyplot as plt\r\n        from jetbot.utils import plot_exec_time\r\n        print(\"start stopping!\")\r\n\r\n        self.capturer.unobserve_all()\r\n        time.sleep(1.0)\r\n        self.robot.stop()\r\n        self.capturer.stop()\r\n\r\n        # self.road_cruiser.stop_cruising(change)\r\n        # plot execution time of road cruiser model processing\r\n        cruiser_model_name = \"road cruiser model\"\r\n        cruiser_model_str = self.cruiser_model.split(\"/\")[-1].split('.')[0]\r\n        plot_exec_time(self.execution_time_rc[1:], cruiser_model_name, cruiser_model_str)\r\n\r\n        # plot execution time of fleet controller model processing\r\n        follower_model_name = \"fleet controller model\"\r\n        follower_model_str = self.follower_model.split(\".\")[0]\r\n        plot_exec_time(self.execution_time_fm[1:], follower_model_name, follower_model_str)\r\n        # plot_exec_time(self.execution_time[1:], self.fps[1:], model_name, self.follower_model.split(\".\")[0])\r\n        plt.show()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/jetbot/fleet_manager_trt.py b/jetbot/fleet_manager_trt.py
--- a/jetbot/fleet_manager_trt.py	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/jetbot/fleet_manager_trt.py	(date 1722533250421)
@@ -21,16 +21,11 @@
 # In[ ]:
 
 from queue import Empty
-import torch.nn.functional as F
 import cv2
 import numpy as np
 import traitlets
-from traitlets import HasTraits, Unicode, Float, Integer, Bool, Any
-import os
-import time
+from traitlets import Float, Bool, Any
 
-# from jetbot import ObjectDetector
-# from jetbot.object_detection_yolo import ObjectDetector_YOLO
 from jetbot import Camera
 from jetbot import Robot
 from jetbot import bgr8_to_jpeg
@@ -121,9 +116,9 @@
     def execute_fm(self, change):
 
         # do object following
-        start_time = time.process_time()
+        start_time = time.time()
         self.execute(change)
-        end_time = time.process_time()
+        end_time = time.time()
         # self.execution_time.append(end_time - start_time + self.capturer.cap_time)
         self.execution_time_fm.append(end_time - start_time)
         # self.fps.append(1/(end_time - start_time))
@@ -137,6 +132,7 @@
         self.load_object_detector(change)  # load object detector function in object follower module
         self.load_road_cruiser(change)  # load_road_cruiser function in road_cruiser_trt module
         self.capturer.unobserve_all()
+
         print("start running")
         self.capturer.observe(self.execute_fm, names='value')
 
@@ -203,7 +199,6 @@
         # return self.cap_image
 
     def stop_fm(self, change):
-        import matplotlib.pyplot as plt
         from jetbot.utils import plot_exec_time
         print("start stopping!")
 
@@ -220,7 +215,7 @@
 
         # plot execution time of fleet controller model processing
         follower_model_name = "fleet controller model"
-        follower_model_str = self.follower_model.split(".")[0]
+        follower_model_str = self.follower_model.split("/")[-1].split(".")[0]
         plot_exec_time(self.execution_time_fm[1:], follower_model_name, follower_model_str)
         # plot_exec_time(self.execution_time[1:], self.fps[1:], model_name, self.follower_model.split(".")[0])
-        plt.show()
+        # plt.show()
Index: notebooks/fleet_management/live_demo_trt.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"9efeacbd-ea0c-4d9a-8fe3-5a96f4316613\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"# Fleet Management - Live Demo (with TRT)\\n\",\r\n    \"\\n\",\r\n    \"In this notebook we'll show how you can follow an object with JetBot!  We'll use a pre-trained neural network\\n\",\r\n    \"that was trained on the [COCO dataset](http://cocodataset.org) to detect 90 different common objects.  These include\\n\",\r\n    \"\\n\",\r\n    \"* Person (index 0)\\n\",\r\n    \"* Cup (index 47)\\n\",\r\n    \"\\n\",\r\n    \"and many others (you can check [this file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) for a full list of class indices).  The model is sourced from the [TensorFlow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)\\n\",\r\n    \"which provides utilities for training object detectors for custom tasks also!  Once the model is trained, we optimize it using NVIDIA TensorRT on the Jetson Nano.\\n\",\r\n    \"\\n\",\r\n    \"This makes the network very fast, capable of real-time execution on Jetson Nano!  We won't run through all of the training and optimization steps in this notebook though.\\n\",\r\n    \"\\n\",\r\n    \"Anyways, let's get started.  First, we'll want to import the ``Object_Follower`` class which takes our pre-trained SSD engine or yolo engine.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"5f53c9fa-0817-4c33-a2b4-91b10dd1c0b6\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from IPython.display import display\\n\",\r\n    \"import ipywidgets.widgets as widgets\\n\",\r\n    \"from ipywidgets.widgets import Box, HBox, VBox, Layout, Label\\n\",\r\n    \"from IPython.display import clear_output\\n\",\r\n    \"import traitlets\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"b1c8856f10648d6f\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# %pip install pandas\\n\",\r\n    \"from jetbot.utils import model_selection\\n\",\r\n    \"from jetbot import FleeterTRT\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"d0a9485c1d715293\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"FL = FleeterTRT(init_sensor_fm=True)\\n\",\r\n    \"FL.conf_th = 0.5\\n\",\r\n    \"\\n\",\r\n    \"od_trt_ms = model_selection(core_library = \\\"TensorRT\\\")\\n\",\r\n    \"od_trt_ms.model_function = \\\"object detection\\\"\\n\",\r\n    \"\\n\",\r\n    \"od_model_type_widget = widgets.Select(options=od_trt_ms.model_type_list, value=od_trt_ms.model_type_list[0], description='Object Detection Model Type:')\\n\",\r\n    \"traitlets.dlink((od_trt_ms, 'model_type_list'), (od_model_type_widget, 'options'))\\n\",\r\n    \"traitlets.dlink((od_model_type_widget, 'value'), (od_trt_ms, 'model_type'))\\n\",\r\n    \"traitlets.dlink((od_trt_ms, 'model_type'), (FL, 'type_follower_model'))\\n\",\r\n    \"\\n\",\r\n    \"od_model_path_widget = widgets.Select(options=od_trt_ms.model_path_list, description='Model Path:', layout=Layout(width='60%'))\\n\",\r\n    \"traitlets.dlink((od_trt_ms, 'model_path_list'), (od_model_path_widget, 'options'))\\n\",\r\n    \"traitlets.dlink((od_model_path_widget, 'value'), (od_trt_ms, 'model_path'))\\n\",\r\n    \"traitlets.dlink((od_trt_ms, 'model_path'), (FL, 'follower_model'))\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"e489d4340a56e4de\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"rd_trt_ms = model_selection()\\n\",\r\n    \"rd_trt_ms.model_function = \\\"classifier\\\"\\n\",\r\n    \"\\n\",\r\n    \"rd_model_type_widget = widgets.Select(options=rd_trt_ms.model_type_list, value=rd_trt_ms.model_type_list[0], description='Classifier Model Type:')\\n\",\r\n    \"traitlets.dlink((rd_trt_ms, 'model_type_list'), (rd_model_type_widget, 'options'))\\n\",\r\n    \"traitlets.dlink((rd_model_type_widget, 'value'), (rd_trt_ms, 'model_type'))\\n\",\r\n    \"traitlets.dlink((rd_trt_ms, 'model_type'), (FL, 'type_cruiser_model'))\\n\",\r\n    \"\\n\",\r\n    \"rd_model_path_widget = widgets.Select(options=rd_trt_ms.model_path_list, description='Model Path:', layout=Layout(width='60%'))\\n\",\r\n    \"traitlets.dlink((rd_trt_ms, 'model_path_list'), (rd_model_path_widget, 'options'))\\n\",\r\n    \"traitlets.dlink((rd_model_path_widget, 'value'), (rd_trt_ms, 'model_path'))\\n\",\r\n    \"traitlets.dlink((rd_trt_ms, 'model_path'), (FL, 'cruiser_model'))\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"5b9261761df11435\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"The followings is to construct the control widgets for fleet control.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"f38029e13e04a95d\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"out = widgets.Output()\\n\",\r\n    \"\\n\",\r\n    \"# image_widget = widgets.Image(format='jpeg', width=OF.img_width, height=OF.img_height)\\n\",\r\n    \"image_widget = widgets.Image(format='jpeg', width=300, height=300, layout = Layout(align_self='center'))\\n\",\r\n    \"\\n\",\r\n    \"# display(image_widget)\\n\",\r\n    \"traitlets.dlink((FL, 'cap_image'), (image_widget, 'value'))\\n\",\r\n    \"\\n\",\r\n    \"# display buttons for start and stop running\\n\",\r\n    \"button_layout = widgets.Layout(width='150px', height='40px', align_self='center')\\n\",\r\n    \"stop_button = widgets.Button(description='Stop', tooltip='Click to stop running', icon='solid stop', layout=button_layout)\\n\",\r\n    \"stop_button.style.button_color='Red'\\n\",\r\n    \"\\n\",\r\n    \"start_button = widgets.Button(description='Start', tooltip='Click to start running', icon='solid play', layout=button_layout)\\n\",\r\n    \"start_button.style.button_color='lightBlue'\\n\",\r\n    \"\\n\",\r\n    \"button_box = widgets.HBox([start_button, stop_button], layout=widgets.Layout(justify_content='space-around', width='30%'))\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"411c644f9d8f8dab\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# infos and params of fleeting control\\n\",\r\n    \"w_layout = Layout(flex='1 1 auto', width='auto')\\n\",\r\n    \"blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked', layout=w_layout)\\n\",\r\n    \"object_view_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='object view', layout=w_layout)\\n\",\r\n    \"label_widget = widgets.BoundedIntText(value=1, min=0, max=100, step=1, description='tracked label', layout=w_layout)  # target to be tracked\\n\",\r\n    \"label_text_widget = widgets.Text(value='', description='label name', layout=w_layout)  # target name to be tracked\\n\",\r\n    \"speed_widget = widgets.FloatSlider(min=0, max=1.0, description='speed', readout_format='.3f', layout=w_layout)\\n\",\r\n    \"speed_gain_widget = widgets.FloatSlider(value=0.1, min=0.0, max=0.5, step=0.01, description='speed_gain', readout_format='.3f', layout=w_layout)\\n\",\r\n    \"speed_dev_widget = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01, description='speed_dev', readout_format='.3f', layout=w_layout)\\n\",\r\n    \"turn_gain_widget = widgets.FloatSlider(value=0.25, min=0.05, max=0.5, step=0.001, description='turn gain', readout_format='.3f', layout=w_layout)\\n\",\r\n    \"steering_bias_widget = widgets.FloatSlider(value=0.0, min=-0.1, max=0.1, step=0.001, description='steering bias', readout_format='.3f', layout=w_layout)\\n\",\r\n    \"view_target_widget = widgets.FloatSlider(value=0.3, min=0.001, max=1.0, step=0.001, description='view_target', readout_format='.3f', layout=w_layout)\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((FL, 'blocked'), (blocked_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((FL, 'mean_view'), (object_view_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((label_widget, 'value'), (FL, 'label'))\\n\",\r\n    \"traitlets.dlink((FL, 'label_text'), (label_text_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((turn_gain_widget, 'value'), (FL, 'turn_gain_fm'))\\n\",\r\n    \"traitlets.dlink((speed_gain_widget, 'value'), (FL, 'speed_gain_fm'))\\n\",\r\n    \"traitlets.dlink((speed_dev_widget, 'value'), (FL, 'speed_dev_fm'))\\n\",\r\n    \"traitlets.dlink((FL, 'speed_fm'), (speed_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((steering_bias_widget, 'value'), (FL, 'steering_bias_fm'))\\n\",\r\n    \"traitlets.dlink((view_target_widget, 'value'), (FL, 'target_view'))\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"437460c981770071\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# control params of road cruising control\\n\",\r\n    \"speed_gain_slider = widgets.FloatSlider(min=0, max=1, step=0.001, value=0.2, description='speed gain', readout_format='.3f')\\n\",\r\n    \"steering_gain_slider = widgets.FloatSlider(min=0, max=0.5, step=0.001, value=0.08, description='steering gain', readout_format='.3f')\\n\",\r\n    \"steering_dgain_slider = widgets.FloatSlider(min=0, max=2.0, step=0.001, value=0.82, description='steering kd', readout_format='.3f')\\n\",\r\n    \"steering_bias_slider = widgets.FloatSlider(min=-0.1, max=0.1, step=0.001, value=-0.01, description='steering bias', readout_format='.3f')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((speed_gain_slider, 'value'), (FL, 'speed_gain_rc'))\\n\",\r\n    \"traitlets.dlink((steering_gain_slider, 'value'), (FL, 'steering_gain_rc'))\\n\",\r\n    \"traitlets.dlink((steering_dgain_slider, 'value'), (FL, 'steering_dgain_rc'))\\n\",\r\n    \"traitlets.dlink((steering_bias_slider, 'value'), (FL, 'steering_bias_rc'))\\n\",\r\n    \"\\n\",\r\n    \"VBox_control = VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider], layout=Layout(align_self='center'))\\n\",\r\n    \"\\n\",\r\n    \"# stat information of road cruising\\n\",\r\n    \"x_slider = widgets.FloatSlider(min=-1.0, max=1.0, description='x')\\n\",\r\n    \"y_slider = widgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\\n\",\r\n    \"steering_slider = widgets.FloatSlider(min=-1.0, max=1.0, description='steering')\\n\",\r\n    \"speed_slider = widgets.FloatSlider(min=0.0, max=1.0, orientation='vertical', description='speed')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((FL, 'x_slider'), (x_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((FL, 'y_slider'), (y_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((FL, 'steering_rc'), (steering_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((FL, 'speed_rc'), (speed_slider, 'value'))\\n\",\r\n    \"\\n\",\r\n    \"Box_y_state = HBox([y_slider, speed_slider])\\n\",\r\n    \"Box_x_state = VBox([x_slider, steering_slider])\\n\",\r\n    \"Box_state = VBox([Box_y_state, Box_x_state])\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"aaae18b54ef351eb\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"def start(change):\\n\",\r\n    \"    FL.start_fm(change)\\n\",\r\n    \"\\n\",\r\n    \"def stop(change):\\n\",\r\n    \"    FL.stop_fm(change)\\n\",\r\n    \"    %reset -f\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"cca5a080c047808b\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"RC_box_layout = Layout(display='flex', flex_flow='column', align_items='stretch', border='solid 2px', width='60%')\\n\",\r\n    \"FL_box_layout = Layout(display='flex', flex_flow='column', align_items='stretch', border='solid 2px', width='60%')\\n\",\r\n    \"H_box_layout = Layout(display='flex', flex_flow='row', align_items='stretch')\\n\",\r\n    \"label_layout = Layout(display='flex', flex_flow='row', align_items='stretch', align_self='center')\\n\",\r\n    \"\\n\",\r\n    \"# Fleet control widgets layout\\n\",\r\n    \"Fleet_Control_items = [HBox([Label(value='--- Fleet Control ---')], layout=Layout(align_self='center')),\\n\",\r\n    \"                       HBox([blocked_widget, object_view_widget], layout = H_box_layout),\\n\",\r\n    \"                       HBox([speed_widget, view_target_widget], layout = H_box_layout),\\n\",\r\n    \"                       HBox([speed_gain_widget, speed_dev_widget], layout = H_box_layout),\\n\",\r\n    \"                       HBox([turn_gain_widget, steering_bias_widget], layout = H_box_layout)]\\n\",\r\n    \"\\n\",\r\n    \"# Road cruising control widgets layout\\n\",\r\n    \"RC_control_items = [HBox([Label(value='--- Cruising Control ---')], layout=Layout(align_self='center')),\\n\",\r\n    \"                    HBox([Box_state, VBox_control], layout = H_box_layout)]\\n\",\r\n    \"\\n\",\r\n    \"image_items = [image_widget, VBox([label_text_widget, label_widget])]\\n\",\r\n    \"\\n\",\r\n    \"display(HBox([od_model_type_widget, od_model_path_widget]))\\n\",\r\n    \"display(HBox([rd_model_type_widget, rd_model_path_widget]))\\n\",\r\n    \"\\n\",\r\n    \"display(HBox([VBox(children = image_items, layout = Layout(align_self='center')),\\n\",\r\n    \"              Box(children = RC_control_items, layout = RC_box_layout)], layout = Layout(justify_content='space-between')),\\n\",\r\n    \"        HBox([button_box, Box(children = Fleet_Control_items, layout = FL_box_layout)], layout = Layout(align_items='stretch', justify_content='space-between')))\\n\",\r\n    \"\\n\",\r\n    \"start_button.on_click(start)\\n\",\r\n    \"stop_button.on_click(stop)\"\r\n   ]\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3 (ipykernel)\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.8.10\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 5\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/fleet_management/live_demo_trt.ipynb b/notebooks/fleet_management/live_demo_trt.ipynb
--- a/notebooks/fleet_management/live_demo_trt.ipynb	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/notebooks/fleet_management/live_demo_trt.ipynb	(date 1722743081603)
@@ -80,7 +80,7 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "rd_trt_ms = model_selection()\n",
+    "rd_trt_ms = model_selection(core_library = \"TensorRT\")\n",
     "rd_trt_ms.model_function = \"classifier\"\n",
     "\n",
     "rd_model_type_widget = widgets.Select(options=rd_trt_ms.model_type_list, value=rd_trt_ms.model_type_list[0], description='Classifier Model Type:')\n",
@@ -119,10 +119,10 @@
     "\n",
     "# display buttons for start and stop running\n",
     "button_layout = widgets.Layout(width='150px', height='40px', align_self='center')\n",
-    "stop_button = widgets.Button(description='Stop', tooltip='Click to stop running', icon='solid stop', layout=button_layout)\n",
+    "stop_button = widgets.Button(description='Stop', tooltip='Click to stop running', icon='stop', layout=button_layout)\n",
     "stop_button.style.button_color='Red'\n",
     "\n",
-    "start_button = widgets.Button(description='Start', tooltip='Click to start running', icon='solid play', layout=button_layout)\n",
+    "start_button = widgets.Button(description='Start', tooltip='Click to start running', icon='play', layout=button_layout)\n",
     "start_button.style.button_color='lightBlue'\n",
     "\n",
     "button_box = widgets.HBox([start_button, stop_button], layout=widgets.Layout(justify_content='space-around', width='30%'))\n"
@@ -246,11 +246,19 @@
     "start_button.on_click(start)\n",
     "stop_button.on_click(stop)"
    ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "ad2f7069-9e3c-47dd-a1d0-252016d2c978",
+   "metadata": {},
+   "outputs": [],
+   "source": []
   }
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3 (ipykernel)",
+   "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
@@ -264,7 +272,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.10"
+   "version": "3.6.9"
   }
  },
  "nbformat": 4,
Index: notebooks/object_following/live_demo.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"9efeacbd-ea0c-4d9a-8fe3-5a96f4316613\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"# Object Following - Live Demo\\n\",\r\n    \"\\n\",\r\n    \"In this notebook we'll show how you can follow an object with JetBot!  We'll use a pre-trained neural network\\n\",\r\n    \"that was trained on the [COCO dataset](http://cocodataset.org) to detect 90 different common objects.  These include\\n\",\r\n    \"\\n\",\r\n    \"* Person (index 0)\\n\",\r\n    \"* Cup (index 47)\\n\",\r\n    \"\\n\",\r\n    \"and many others (you can check [this file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) for a full list of class indices).  The model is sourced from the [TensorFlow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)\\n\",\r\n    \"which provides utilities for training object detectors for custom tasks also!  Once the model is trained, we optimize it using NVIDIA TensorRT on the Jetson Nano.\\n\",\r\n    \"\\n\",\r\n    \"This makes the network very fast, capable of real-time execution on Jetson Nano!  We won't run through all of the training and optimization steps in this notebook though.\\n\",\r\n    \"\\n\",\r\n    \"Anyways, let's get started.  First, we'll want to import the ``Object_Follower`` class which takes our pre-trained SSD engine or yolo engine.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"5f53c9fa-0817-4c33-a2b4-91b10dd1c0b6\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from IPython.display import display\\n\",\r\n    \"import ipywidgets.widgets as widgets\\n\",\r\n    \"from IPython.display import clear_output\\n\",\r\n    \"\\n\",\r\n    \"from ipywidgets.widgets import HBox, Layout\\n\",\r\n    \"import traitlets\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"f68101715a389a64\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# %pip install pandas\\n\",\r\n    \"from jetbot.utils import model_selection\\n\",\r\n    \"from jetbot import ObjectFollower\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"5709ce555ac1de28\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# avoider_model='../collision_avoidance/best_model.pth'\\n\",\r\n    \"OF = ObjectFollower(init_sensor_of=True)\\n\",\r\n    \"OF.conf_th = 0.5\\n\",\r\n    \"\\n\",\r\n    \"trt_ms = model_selection(core_library = \\\"TensorRT\\\")\\n\",\r\n    \"trt_ms.model_function = \\\"object detection\\\"\\n\",\r\n    \"\\n\",\r\n    \"model_function_widget = widgets.Select(options=trt_ms.model_function_list, value=\\\"object detection\\\",\\n\",\r\n    \"                                       description='Model Function:')\\n\",\r\n    \"\\n\",\r\n    \"model_type_widget = widgets.Select(options=trt_ms.model_type_list, value=\\\"SSD\\\", description='Model Type:')\\n\",\r\n    \"traitlets.dlink((trt_ms, 'model_type_list'), (model_type_widget, 'options'))\\n\",\r\n    \"traitlets.dlink((model_type_widget, 'value'), (trt_ms, 'model_type'))\\n\",\r\n    \"traitlets.dlink((trt_ms, 'model_type'), (OF, 'type_follower_model'))\\n\",\r\n    \"\\n\",\r\n    \"model_path_widget = widgets.Select(options=trt_ms.model_path_list, description='Model Path:',\\n\",\r\n    \"                                   layout=Layout(width='75%'))\\n\",\r\n    \"traitlets.dlink((trt_ms, 'model_path_list'), (model_path_widget, 'options'))\\n\",\r\n    \"traitlets.dlink((model_path_widget, 'value'), (trt_ms, 'model_path'))\\n\",\r\n    \"traitlets.dlink((trt_ms, 'model_path'), (OF, 'follower_model'))\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"5fa94be7-f298-48b8-9298-d0fa3fecab3f\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"out = widgets.Output()\\n\",\r\n    \"\\n\",\r\n    \"# image_widget = widgets.Image(format='jpeg', width=OF.img_width, height=OF.img_height)\\n\",\r\n    \"image_widget = widgets.Image(format='jpeg', width=350, height=350)\\n\",\r\n    \"\\n\",\r\n    \"# display(image_widget)\\n\",\r\n    \"traitlets.dlink((OF, 'cap_image'), (image_widget, 'value'))\\n\",\r\n    \"\\n\",\r\n    \"# display buttons\\n\",\r\n    \"button_layout = widgets.Layout(width='100px', height='40px', align_self='center')\\n\",\r\n    \"stop_button = widgets.Button(description='Stop', tooltip='Click to stop running', icon='solid stop', layout=button_layout)\\n\",\r\n    \"stop_button.style.button_color='Red'\\n\",\r\n    \"\\n\",\r\n    \"start_button = widgets.Button(description='Start', tooltip='Click to start running', icon='solid play', layout=button_layout)\\n\",\r\n    \"start_button.style.button_color='lightBlue'\\n\",\r\n    \"button_box = widgets.HBox([start_button, stop_button], layout=widgets.Layout(align_self='center'))\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"b6f72bd0-b6a1-411b-b0a9-d6553586c991\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\\n\",\r\n    \"label_widget = widgets.IntText(value=1, description='tracked label')  # target to be tracked\\n\",\r\n    \"label_text_widget = widgets.Text(value='', description='label name')  # target name to be tracked\\n\",\r\n    \"speed_widget = widgets.FloatSlider(value=0.18, min=0.05, max=0.5, step=0.001, description='speed', readout_format='.3f')\\n\",\r\n    \"speed_gain_widget = widgets.FloatSlider(value=0.18, min=0.05, max=0.5, step=0.001, description='speed gain', readout_format='.3f')\\n\",\r\n    \"turn_gain_widget = widgets.FloatSlider(value=0.25, min=0.05, max=0.5, step=0.001, description='turn gain', readout_format='.3f')\\n\",\r\n    \"steering_bias_widget = widgets.FloatSlider(value=0.02, min=-0.1, max=0.1, step=0.001, description='steering bias', readout_format='.3f')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((OF, 'blocked'), (blocked_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((label_widget, 'value'), (OF, 'label'))\\n\",\r\n    \"traitlets.dlink((OF, 'label_text'), (label_text_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((OF, 'speed_of'), (speed_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((turn_gain_widget, 'value'), (OF, 'turn_gain_of'))\\n\",\r\n    \"traitlets.dlink((speed_gain_widget, 'value'), (OF, 'speed_gain_of'))\\n\",\r\n    \"traitlets.dlink((steering_bias_widget, 'value'), (OF, 'steering_bias_of'))\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"d598ddb8-6d22-4105-a59a-d36496c71c3e\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"def start(change):\\n\",\r\n    \"    OF.start_of(change)\\n\",\r\n    \"\\n\",\r\n    \"def stop(change):\\n\",\r\n    \"    OF.stop_of(change)\\n\",\r\n    \"    %reset -f\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"54ffdec6-9a82-453a-9a02-eab9187c2c1e\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"display(HBox([model_type_widget, model_path_widget]))\\n\",\r\n    \"\\n\",\r\n    \"display(widgets.VBox([\\n\",\r\n    \"    widgets.HBox([image_widget, blocked_widget]),\\n\",\r\n    \"    widgets.HBox([label_widget, label_text_widget]),\\n\",\r\n    \"    widgets.HBox([speed_gain_widget, speed_widget]),\\n\",\r\n    \"    turn_gain_widget,\\n\",\r\n    \"    steering_bias_widget,\\n\",\r\n    \"    button_box\\n\",\r\n    \"]))\\n\",\r\n    \"\\n\",\r\n    \"start_button.on_click(start)\\n\",\r\n    \"stop_button.on_click(stop)\"\r\n   ]\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3 (ipykernel)\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.8.10\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 5\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/object_following/live_demo.ipynb b/notebooks/object_following/live_demo.ipynb
--- a/notebooks/object_following/live_demo.ipynb	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/notebooks/object_following/live_demo.ipynb	(date 1722534978169)
@@ -96,10 +96,10 @@
     "\n",
     "# display buttons\n",
     "button_layout = widgets.Layout(width='100px', height='40px', align_self='center')\n",
-    "stop_button = widgets.Button(description='Stop', tooltip='Click to stop running', icon='solid stop', layout=button_layout)\n",
+    "stop_button = widgets.Button(description='Stop', tooltip='Click to stop running', icon='stop', layout=button_layout)\n",
     "stop_button.style.button_color='Red'\n",
     "\n",
-    "start_button = widgets.Button(description='Start', tooltip='Click to start running', icon='solid play', layout=button_layout)\n",
+    "start_button = widgets.Button(description='Start', tooltip='Click to start running', icon='play', layout=button_layout)\n",
     "start_button.style.button_color='lightBlue'\n",
     "button_box = widgets.HBox([start_button, stop_button], layout=widgets.Layout(align_self='center'))\n"
    ]
@@ -168,7 +168,7 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3 (ipykernel)",
+   "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
@@ -182,7 +182,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.10"
+   "version": "3.6.9"
   }
  },
  "nbformat": 4,
Index: notebooks/fleet_management/live_demo.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"9efeacbd-ea0c-4d9a-8fe3-5a96f4316613\",\r\n   \"metadata\": {\r\n    \"tags\": []\r\n   },\r\n   \"source\": [\r\n    \"# Fleet Management - Live Demo\\n\",\r\n    \"\\n\",\r\n    \"In this notebook we'll show how you can follow an object with JetBot!  We'll use a pre-trained neural network\\n\",\r\n    \"that was trained on the [COCO dataset](http://cocodataset.org) to detect 90 different common objects.  These include\\n\",\r\n    \"\\n\",\r\n    \"* Person (index 0)\\n\",\r\n    \"* Cup (index 47)\\n\",\r\n    \"\\n\",\r\n    \"and many others (you can check [this file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) for a full list of class indices).  The model is sourced from the [TensorFlow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)\\n\",\r\n    \"which provides utilities for training object detectors for custom tasks also!  Once the model is trained, we optimize it using NVIDIA TensorRT on the Jetson Nano.\\n\",\r\n    \"\\n\",\r\n    \"This makes the network very fast, capable of real-time execution on Jetson Nano!  We won't run through all of the training and optimization steps in this notebook though.\\n\",\r\n    \"\\n\",\r\n    \"Anyways, let's get started.  First, we'll want to import the ``Object_Follower`` class which takes our pre-trained SSD engine or yolo engine.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"id\": \"5f53c9fa-0817-4c33-a2b4-91b10dd1c0b6\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"from IPython.display import display\\n\",\r\n    \"import ipywidgets.widgets as widgets\\n\",\r\n    \"from ipywidgets.widgets import Box, HBox, VBox, Layout, Label\\n\",\r\n    \"from IPython.display import clear_output\\n\",\r\n    \"import traitlets\\n\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"id\": \"db409a49-f088-4214-a5fb-6feee7e8ebbd\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# from main_scripts import Object_Follower\\n\",\r\n    \"from jetbot import Fleeter\\n\",\r\n    \"\\n\",\r\n    \"type_follower_model = \\\"YOLO\\\"  # \\\"SSD\\\", \\\"SSD_v7\\\", \\\"YOLO\\\", \\\"YOLO_v7\\\"\\n\",\r\n    \"# follower_model='ssd_mobilenet_v2_coco_onnx.engine'\\n\",\r\n    \"# follower_model='yolov3_tiny_288.engine'\\n\",\r\n    \"# follower_model='yolov4_tiny_288.engine'\\n\",\r\n    \"follower_model='yolov4_tiny_416.eigine'\\n\",\r\n    \"# follower_model='yolov4_288.engine'\\n\",\r\n    \"# follower_model='yolov4_416.engine'\\n\",\r\n    \"\\n\",\r\n    \"type_cruiser_model = \\\"resnet\\\"\\n\",\r\n    \"cruiser_model='resnet18'\\n\",\r\n    \"\\n\",\r\n    \"FL = Fleeter(follower_model=follower_model, type_follower_model=type_follower_model, \\n\",\r\n    \"             cruiser_model=cruiser_model, type_cruiser_model=type_cruiser_model, conf_th=0.3)\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"10169b16-aece-4a23-b00d-d84cdb14df3d\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"The followings is to construct the control widgets for fleet control.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"id\": \"5fa94be7-f298-48b8-9298-d0fa3fecab3f\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"out = widgets.Output()\\n\",\r\n    \"\\n\",\r\n    \"# image_widget = widgets.Image(format='jpeg', width=OF.img_width, height=OF.img_height)\\n\",\r\n    \"image_widget = widgets.Image(format='jpeg', width=300, height=300, layout = Layout(align_self='center'))\\n\",\r\n    \"\\n\",\r\n    \"# display(image_widget)\\n\",\r\n    \"traitlets.dlink((FL, 'cap_image'), (image_widget, 'value'))\\n\",\r\n    \"\\n\",\r\n    \"# display buttons\\n\",\r\n    \"button_layout = widgets.Layout(width='100px', height='40px', align_self='center')\\n\",\r\n    \"stop_button = widgets.Button(description='Stop', button_style='danger', tooltip='Click to stop running', icon='stop', layout=button_layout)\\n\",\r\n    \"start_button = widgets.Button(description='Start', tooltip='Click to start running', layout=button_layout)\\n\",\r\n    \"button_box = widgets.HBox([start_button, stop_button], layout=widgets.Layout(justify_content='space-around', width='30%'))\\n\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"id\": \"b6f72bd0-b6a1-411b-b0a9-d6553586c991\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# infos and params of fleeting\\n\",\r\n    \"blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\\n\",\r\n    \"object_view_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='object view')\\n\",\r\n    \"label_widget = widgets.IntText(value=1, description='tracked label')  # target to be tracked\\n\",\r\n    \"label_text_widget = widgets.Text(value='', description='label name')  # target name to be tracked\\n\",\r\n    \"speed_widget = widgets.FloatSlider(min=0.0, max=1.0, description='speed', readout_format='.3f')\\n\",\r\n    \"speed_gain_widget = widgets.FloatSlider(value=0.01, min=0.0, max=0.05, step=0.01, description='speed_gain', readout_format='.3f')\\n\",\r\n    \"speed_dev_widget = widgets.FloatSlider(value=0.5, min=0.05, max=1, step=0.01, description='speed_dev', readout_format='.3f')\\n\",\r\n    \"turn_gain_widget = widgets.FloatSlider(value=0.25, min=0.05, max=0.5, step=0.001, description='turn gain', readout_format='.3f')\\n\",\r\n    \"steering_bias_widget = widgets.FloatSlider(value=0.02, min=-0.1, max=0.1, step=0.001, description='steering bias', readout_format='.3f')\\n\",\r\n    \"view_target_widget = widgets.FloatSlider(value=0.3, min=0.001, max=1.0, step=0.001, description='view target', readout_format='.3f')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((FL, 'blocked'), (blocked_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((FL, 'mean_view'), (object_view_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((label_widget, 'value'), (FL, 'label'))\\n\",\r\n    \"traitlets.dlink((FL, 'label_text'), (label_text_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((turn_gain_widget, 'value'), (FL, 'turn_gain'))\\n\",\r\n    \"traitlets.dlink((speed_gain_widget, 'value'), (FL, 'speed_gain'))\\n\",\r\n    \"traitlets.dlink((speed_dev_widget, 'value'), (FL, 'speed_dev'))\\n\",\r\n    \"traitlets.dlink((FL, 'speed'), (speed_widget, 'value'))\\n\",\r\n    \"traitlets.dlink((steering_bias_widget, 'value'), (FL, 'steering_bias'))\\n\",\r\n    \"traitlets.dlink((view_target_widget, 'value'), (FL, 'target_view'))\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"id\": \"a67ec8b9-b17b-43d0-86d5-abc06c5977e6\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# control params of road cruising\\n\",\r\n    \"speed_gain_slider = widgets.FloatSlider(min=0, max=1, step=0.001, value=0.2, description='speed gain', readout_format='.3f')\\n\",\r\n    \"steering_gain_slider = widgets.FloatSlider(min=0, max=0.5, step=0.001, value=0.08, description='steering gain', readout_format='.3f')\\n\",\r\n    \"steering_dgain_slider = widgets.FloatSlider(min=0, max=2.0, step=0.001, value=0.82, description='steering kd', readout_format='.3f')\\n\",\r\n    \"steering_bias_slider = widgets.FloatSlider(min=-0.1, max=0.1, step=0.001, value=-0.01, description='steering bias', readout_format='.3f')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((speed_gain_slider, 'value'), (FL.road_cruiser, 'speed_gain'))\\n\",\r\n    \"traitlets.dlink((steering_gain_slider, 'value'), (FL.road_cruiser, 'steering_gain'))\\n\",\r\n    \"traitlets.dlink((steering_dgain_slider, 'value'), (FL.road_cruiser, 'steering_dgain'))\\n\",\r\n    \"traitlets.dlink((steering_bias_slider, 'value'), (FL.road_cruiser, 'steering_bias'))\\n\",\r\n    \"\\n\",\r\n    \"# VBox_image = VBox([image_widget], layout=Layout(align_self='center'))\\n\",\r\n    \"VBox_control = VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider], layout=Layout(align_self='center'))\\n\",\r\n    \"\\n\",\r\n    \"# stat information of road cruising\\n\",\r\n    \"x_slider = widgets.FloatSlider(min=-1.0, max=1.0, description='x')\\n\",\r\n    \"y_slider = widgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\\n\",\r\n    \"steering_slider = widgets.FloatSlider(min=-1.0, max=1.0, description='steering')\\n\",\r\n    \"speed_slider = widgets.FloatSlider(min=0.0, max=1.0, orientation='vertical', description='speed')\\n\",\r\n    \"\\n\",\r\n    \"traitlets.dlink((FL.road_cruiser, 'x_slider'), (x_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((FL.road_cruiser, 'y_slider'), (y_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((FL.road_cruiser, 'steering'), (steering_slider, 'value'))\\n\",\r\n    \"traitlets.dlink((FL.road_cruiser, 'speed'), (speed_slider, 'value'))\\n\",\r\n    \"\\n\",\r\n    \"Box_y_state = HBox([y_slider, speed_slider])\\n\",\r\n    \"Box_x_state = VBox([x_slider, steering_slider])\\n\",\r\n    \"Box_state = VBox([Box_y_state, Box_x_state])\\n\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"id\": \"d598ddb8-6d22-4105-a59a-d36496c71c3e\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"def start(change):\\n\",\r\n    \"    FL.start_run(change)\\n\",\r\n    \"\\n\",\r\n    \"def stop(change):\\n\",\r\n    \"    FL.stop_run(change)\\n\",\r\n    \"    %reset -f\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"id\": \"89e311f4-2762-4df0-8b4d-2f7f870c2a6e\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"RC_box_layout = Layout(display='flex', flex_flow='column', align_items='stretch', border='solid 2px', width='60%')\\n\",\r\n    \"FL_box_layout = Layout(display='flex', flex_flow='column', align_items='stretch', border='solid 2px', width='60%')\\n\",\r\n    \"H_box_layout = Layout(display='flex', flex_flow='row', align_items='stretch')\\n\",\r\n    \"label_layout = Layout(display='flex', flex_flow='row', align_items='stretch', align_self='center')\\n\",\r\n    \"\\n\",\r\n    \"# fleet control widgets \\n\",\r\n    \"Fleet_Control_items = [HBox([Label(value='--- Fleet Control ---')], layout=Layout(align_self='center')),\\n\",\r\n    \"                       HBox([blocked_widget, object_view_widget], layout = H_box_layout),\\n\",\r\n    \"                       HBox([speed_widget, view_target_widget], layout = H_box_layout),\\n\",\r\n    \"                       HBox([speed_gain_widget, speed_dev_widget], layout = H_box_layout),\\n\",\r\n    \"                       HBox([turn_gain_widget, steering_bias_widget], layout = H_box_layout)]\\n\",\r\n    \"\\n\",\r\n    \"RC_control_items = [HBox([Label(value='--- Cruising Control ---')], layout=Layout(align_self='center')),\\n\",\r\n    \"                    HBox([Box_state, VBox_control], layout = H_box_layout)]\\n\",\r\n    \"\\n\",\r\n    \"image_items = [image_widget, VBox([label_text_widget, label_widget])]\\n\",\r\n    \"\\n\",\r\n    \"display(HBox([VBox(children = image_items, layout = Layout(align_self='center')),\\n\",\r\n    \"             Box(children = RC_control_items, layout = RC_box_layout)], layout = Layout(justify_content='space-between')),\\n\",\r\n    \"        HBox([button_box, Box(children = Fleet_Control_items, layout = FL_box_layout)], layout = Layout(align_items='stretch', justify_content='space-between')))\\n\",\r\n    \"\\n\",\r\n    \"start_button.on_click(start)\\n\",\r\n    \"stop_button.on_click(stop)\"\r\n   ],\r\n   \"outputs\": [],\r\n   \"execution_count\": null\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3 (ipykernel)\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.8.10\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 5\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/fleet_management/live_demo.ipynb b/notebooks/fleet_management/live_demo.ipynb
--- a/notebooks/fleet_management/live_demo.ipynb	(revision 727661090452b626806c00b5ee95fd60c1a81ea5)
+++ b/notebooks/fleet_management/live_demo.ipynb	(date 1722743081637)
@@ -25,42 +25,79 @@
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "id": "5f53c9fa-0817-4c33-a2b4-91b10dd1c0b6",
    "metadata": {},
+   "outputs": [],
    "source": [
     "from IPython.display import display\n",
     "import ipywidgets.widgets as widgets\n",
     "from ipywidgets.widgets import Box, HBox, VBox, Layout, Label\n",
     "from IPython.display import clear_output\n",
     "import traitlets\n"
-   ],
-   "outputs": [],
-   "execution_count": null
+   ]
   },
   {
+   "metadata": {},
    "cell_type": "code",
-   "id": "db409a49-f088-4214-a5fb-6feee7e8ebbd",
-   "metadata": {},
+   "outputs": [],
+   "execution_count": null,
    "source": [
     "# from main_scripts import Object_Follower\n",
-    "from jetbot import Fleeter\n",
+    "from jetbot.utils import model_selection\n",
+    "from jetbot import Fleeter"
+   ],
+   "id": "680885bcd60fead1"
+  },
+  {
+   "metadata": {},
+   "cell_type": "code",
+   "outputs": [],
+   "execution_count": null,
+   "source": [
+    "\n",
+    "FL = Fleeter(init_sensor_fm=True)\n",
+    "FL.conf_th = 0.5\n",
     "\n",
-    "type_follower_model = \"YOLO\"  # \"SSD\", \"SSD_v7\", \"YOLO\", \"YOLO_v7\"\n",
-    "# follower_model='ssd_mobilenet_v2_coco_onnx.engine'\n",
-    "# follower_model='yolov3_tiny_288.engine'\n",
-    "# follower_model='yolov4_tiny_288.engine'\n",
-    "follower_model='yolov4_tiny_416.eigine'\n",
-    "# follower_model='yolov4_288.engine'\n",
-    "# follower_model='yolov4_416.engine'\n",
+    "od_trt_ms = model_selection(core_library=\"TensorRT\")\n",
+    "od_trt_ms.model_function = \"object detection\"\n",
     "\n",
-    "type_cruiser_model = \"resnet\"\n",
-    "cruiser_model='resnet18'\n",
+    "od_model_type_widget = widgets.Select(options=od_trt_ms.model_type_list, value=od_trt_ms.model_type_list[0],\n",
+    "                                      description='Object Detection Model Type:')\n",
+    "traitlets.dlink((od_trt_ms, 'model_type_list'), (od_model_type_widget, 'options'))\n",
+    "traitlets.dlink((od_model_type_widget, 'value'), (od_trt_ms, 'model_type'))\n",
+    "traitlets.dlink((od_trt_ms, 'model_type'), (FL, 'type_follower_model'))\n",
     "\n",
-    "FL = Fleeter(follower_model=follower_model, type_follower_model=type_follower_model, \n",
-    "             cruiser_model=cruiser_model, type_cruiser_model=type_cruiser_model, conf_th=0.3)"
+    "od_model_path_widget = widgets.Select(options=od_trt_ms.model_path_list, description='Model Path:',\n",
+    "                                      layout=Layout(width='60%'))\n",
+    "traitlets.dlink((od_trt_ms, 'model_path_list'), (od_model_path_widget, 'options'))\n",
+    "traitlets.dlink((od_model_path_widget, 'value'), (od_trt_ms, 'model_path'))\n",
+    "traitlets.dlink((od_trt_ms, 'model_path'), (FL, 'follower_model'))\n"
    ],
+   "id": "db409a49-f088-4214-a5fb-6feee7e8ebbd"
+  },
+  {
+   "metadata": {},
+   "cell_type": "code",
    "outputs": [],
-   "execution_count": null
+   "execution_count": null,
+   "source": [
+    "rd_trt_ms = model_selection(core_library=\"Pytorch\")\n",
+    "rd_trt_ms.model_function = \"classifier\"\n",
+    "\n",
+    "rd_model_type_widget = widgets.Select(options=rd_trt_ms.model_type_list, value=rd_trt_ms.model_type_list[0],\n",
+    "                                      description='Classifier Model Type:')\n",
+    "traitlets.dlink((rd_trt_ms, 'model_type_list'), (rd_model_type_widget, 'options'))\n",
+    "traitlets.dlink((rd_model_type_widget, 'value'), (rd_trt_ms, 'model_type'))\n",
+    "traitlets.dlink((rd_trt_ms, 'model_type'), (FL, 'type_cruiser_model'))\n",
+    "\n",
+    "rd_model_path_widget = widgets.Select(options=rd_trt_ms.model_path_list, description='Model Path:',\n",
+    "                                      layout=Layout(width='60%'))\n",
+    "traitlets.dlink((rd_trt_ms, 'model_path_list'), (rd_model_path_widget, 'options'))\n",
+    "traitlets.dlink((rd_model_path_widget, 'value'), (rd_trt_ms, 'model_path'))\n",
+    "traitlets.dlink((rd_trt_ms, 'model_path'), (FL, 'cruiser_model'))\n"
+   ],
+   "id": "75d8ffd3064ecaea"
   },
   {
    "cell_type": "markdown",
@@ -72,8 +109,10 @@
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "id": "5fa94be7-f298-48b8-9298-d0fa3fecab3f",
    "metadata": {},
+   "outputs": [],
    "source": [
     "out = widgets.Output()\n",
     "\n",
@@ -85,17 +124,17 @@
     "\n",
     "# display buttons\n",
     "button_layout = widgets.Layout(width='100px', height='40px', align_self='center')\n",
-    "stop_button = widgets.Button(description='Stop', button_style='danger', tooltip='Click to stop running', icon='stop', layout=button_layout)\n",
+    "stop_button = widgets.Button(description='Stop', button_style='danger', tooltip='Click to stop running', icon='fa-stop', layout=button_layout)\n",
     "start_button = widgets.Button(description='Start', tooltip='Click to start running', layout=button_layout)\n",
     "button_box = widgets.HBox([start_button, stop_button], layout=widgets.Layout(justify_content='space-around', width='30%'))\n"
-   ],
-   "outputs": [],
-   "execution_count": null
+   ]
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "id": "b6f72bd0-b6a1-411b-b0a9-d6553586c991",
    "metadata": {},
+   "outputs": [],
    "source": [
     "# infos and params of fleeting\n",
     "blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
@@ -113,20 +152,20 @@
     "traitlets.dlink((FL, 'mean_view'), (object_view_widget, 'value'))\n",
     "traitlets.dlink((label_widget, 'value'), (FL, 'label'))\n",
     "traitlets.dlink((FL, 'label_text'), (label_text_widget, 'value'))\n",
-    "traitlets.dlink((turn_gain_widget, 'value'), (FL, 'turn_gain'))\n",
-    "traitlets.dlink((speed_gain_widget, 'value'), (FL, 'speed_gain'))\n",
-    "traitlets.dlink((speed_dev_widget, 'value'), (FL, 'speed_dev'))\n",
-    "traitlets.dlink((FL, 'speed'), (speed_widget, 'value'))\n",
-    "traitlets.dlink((steering_bias_widget, 'value'), (FL, 'steering_bias'))\n",
+    "traitlets.dlink((turn_gain_widget, 'value'), (FL, 'turn_gain_fm'))\n",
+    "traitlets.dlink((speed_gain_widget, 'value'), (FL, 'speed_gain_fm'))\n",
+    "traitlets.dlink((speed_dev_widget, 'value'), (FL, 'speed_dev_fm'))\n",
+    "traitlets.dlink((FL, 'speed_fm'), (speed_widget, 'value'))\n",
+    "traitlets.dlink((steering_bias_widget, 'value'), (FL, 'steering_bias_fm'))\n",
     "traitlets.dlink((view_target_widget, 'value'), (FL, 'target_view'))"
-   ],
-   "outputs": [],
-   "execution_count": null
+   ]
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "id": "a67ec8b9-b17b-43d0-86d5-abc06c5977e6",
    "metadata": {},
+   "outputs": [],
    "source": [
     "# control params of road cruising\n",
     "speed_gain_slider = widgets.FloatSlider(min=0, max=1, step=0.001, value=0.2, description='speed gain', readout_format='.3f')\n",
@@ -134,10 +173,10 @@
     "steering_dgain_slider = widgets.FloatSlider(min=0, max=2.0, step=0.001, value=0.82, description='steering kd', readout_format='.3f')\n",
     "steering_bias_slider = widgets.FloatSlider(min=-0.1, max=0.1, step=0.001, value=-0.01, description='steering bias', readout_format='.3f')\n",
     "\n",
-    "traitlets.dlink((speed_gain_slider, 'value'), (FL.road_cruiser, 'speed_gain'))\n",
-    "traitlets.dlink((steering_gain_slider, 'value'), (FL.road_cruiser, 'steering_gain'))\n",
-    "traitlets.dlink((steering_dgain_slider, 'value'), (FL.road_cruiser, 'steering_dgain'))\n",
-    "traitlets.dlink((steering_bias_slider, 'value'), (FL.road_cruiser, 'steering_bias'))\n",
+    "traitlets.dlink((speed_gain_slider, 'value'), (FL, 'speed_gain_rc'))\n",
+    "traitlets.dlink((steering_gain_slider, 'value'), (FL, 'steering_gain_rc'))\n",
+    "traitlets.dlink((steering_dgain_slider, 'value'), (FL, 'steering_dgain_rc'))\n",
+    "traitlets.dlink((steering_bias_slider, 'value'), (FL, 'steering_bias_rc'))\n",
     "\n",
     "# VBox_image = VBox([image_widget], layout=Layout(align_self='center'))\n",
     "VBox_control = VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider], layout=Layout(align_self='center'))\n",
@@ -148,37 +187,37 @@
     "steering_slider = widgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
     "speed_slider = widgets.FloatSlider(min=0.0, max=1.0, orientation='vertical', description='speed')\n",
     "\n",
-    "traitlets.dlink((FL.road_cruiser, 'x_slider'), (x_slider, 'value'))\n",
-    "traitlets.dlink((FL.road_cruiser, 'y_slider'), (y_slider, 'value'))\n",
-    "traitlets.dlink((FL.road_cruiser, 'steering'), (steering_slider, 'value'))\n",
-    "traitlets.dlink((FL.road_cruiser, 'speed'), (speed_slider, 'value'))\n",
+    "traitlets.dlink((FL, 'x_slider'), (x_slider, 'value'))\n",
+    "traitlets.dlink((FL, 'y_slider'), (y_slider, 'value'))\n",
+    "traitlets.dlink((FL, 'steering_rc'), (steering_slider, 'value'))\n",
+    "traitlets.dlink((FL, 'speed_rc'), (speed_slider, 'value'))\n",
     "\n",
     "Box_y_state = HBox([y_slider, speed_slider])\n",
     "Box_x_state = VBox([x_slider, steering_slider])\n",
     "Box_state = VBox([Box_y_state, Box_x_state])\n"
-   ],
-   "outputs": [],
-   "execution_count": null
+   ]
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "id": "d598ddb8-6d22-4105-a59a-d36496c71c3e",
    "metadata": {},
+   "outputs": [],
    "source": [
     "def start(change):\n",
-    "    FL.start_run(change)\n",
+    "    FL.start_fm(change)\n",
     "\n",
     "def stop(change):\n",
-    "    FL.stop_run(change)\n",
+    "    FL.stop_fm(change)\n",
     "    %reset -f"
-   ],
-   "outputs": [],
-   "execution_count": null
+   ]
   },
   {
    "cell_type": "code",
+   "execution_count": null,
    "id": "89e311f4-2762-4df0-8b4d-2f7f870c2a6e",
    "metadata": {},
+   "outputs": [],
    "source": [
     "RC_box_layout = Layout(display='flex', flex_flow='column', align_items='stretch', border='solid 2px', width='60%')\n",
     "FL_box_layout = Layout(display='flex', flex_flow='column', align_items='stretch', border='solid 2px', width='60%')\n",
@@ -196,6 +235,9 @@
     "                    HBox([Box_state, VBox_control], layout = H_box_layout)]\n",
     "\n",
     "image_items = [image_widget, VBox([label_text_widget, label_widget])]\n",
+    "\n",
+    "display(HBox([od_model_type_widget, od_model_path_widget]))\n",
+    "display(HBox([rd_model_type_widget, rd_model_path_widget]))\n",
     "\n",
     "display(HBox([VBox(children = image_items, layout = Layout(align_self='center')),\n",
     "             Box(children = RC_control_items, layout = RC_box_layout)], layout = Layout(justify_content='space-between')),\n",
@@ -203,14 +245,20 @@
     "\n",
     "start_button.on_click(start)\n",
     "stop_button.on_click(stop)"
-   ],
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "5a0a9af5-8f32-45ac-ad0f-db6911b65c7d",
+   "metadata": {},
    "outputs": [],
-   "execution_count": null
+   "source": []
   }
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3 (ipykernel)",
+   "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
@@ -224,7 +272,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.10"
+   "version": "3.6.9"
   }
  },
  "nbformat": 4,
